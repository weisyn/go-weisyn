package kbucket

import (
	"container/list"
	"context"
	"encoding/json"
	"fmt"
	"strings"
	"sync"
	"time"

	lphost "github.com/libp2p/go-libp2p/core/host"
	libnetwork "github.com/libp2p/go-libp2p/core/network"
	"github.com/libp2p/go-libp2p/core/peer"
	"github.com/weisyn/v1/internal/config/node"
	"github.com/weisyn/v1/pkg/constants"
	"github.com/weisyn/v1/pkg/constants/events"
	"github.com/weisyn/v1/pkg/constants/protocols"
	"github.com/weisyn/v1/pkg/interfaces/config"
	"github.com/weisyn/v1/pkg/interfaces/infrastructure/event"
	"github.com/weisyn/v1/pkg/interfaces/infrastructure/kademlia"
	"github.com/weisyn/v1/pkg/interfaces/infrastructure/log"
	p2pi "github.com/weisyn/v1/pkg/interfaces/p2p"
	"github.com/weisyn/v1/pkg/types"
)

// RoutingTableManager å®ç°è·¯ç”±è¡¨ç®¡ç†å™¨
// åŸºäºdefs-back/kbucketçš„åŸå§‹ç®—æ³•ï¼Œç¡®ä¿Kademliaç®—æ³•çš„å‡†ç¡®æ€§
type RoutingTableManager struct {
	// é…ç½®å’Œä¾èµ–
	config         kademlia.KBucketConfig
	logger         log.Logger
	p2pService     p2pi.Service    // æ–°å¢ï¼šç”¨äºWESèŠ‚ç‚¹éªŒè¯å’Œè¿æ¥çŠ¶æ€æ£€æŸ¥
	configProvider config.Provider // æ–°å¢ï¼šç”¨äºè·å–æœ¬åœ°é“¾èº«ä»½è¿›è¡Œæ¯”å¯¹
	eventBus       event.EventBus  // ğŸ”§ Phase 3: äº‹ä»¶æ€»çº¿ï¼Œç”¨äºå‘å¸ƒé‡ç½®äº‹ä»¶
	eventBusMu     sync.RWMutex    // eventBuså­—æ®µä¿æŠ¤é”

	// æ ¸å¿ƒæ•°æ®ï¼ˆæ¥è‡ªdefs-back/kbucket/table.goçš„ç»“æ„ï¼‰
	ctx        context.Context
	ctxCancel  context.CancelFunc
	localID    []byte        // æœ¬åœ°èŠ‚ç‚¹ID
	buckets    []*Bucket     // Kæ¡¶æ•°ç»„
	bucketSize int           // æ¡¶å¤§å°
	maxLatency time.Duration // æœ€å¤§å»¶è¿Ÿ

	// é”ç®¡ç†
	tabLock        sync.RWMutex       // æ€»ä½“é”
	cplRefreshedAt map[uint]time.Time // CPLåˆ·æ–°æ—¶é—´

	// å›è°ƒå‡½æ•°
	peerAdded   func(peer.ID)
	peerRemoved func(peer.ID)

	// è¯Šæ–­ä¿¡æ¯ï¼šæœ€è¿‘ä¸€æ¬¡å…¥æ¡¶å°è¯•ç»“æœï¼ˆç”¨äº /debug/p2p/routingï¼‰
	lastAddMu sync.RWMutex
	lastAdd   *types.KBucketLastAdd

	// å®½é™æœŸï¼ˆæ¥è‡ªåŸå§‹ç®—æ³•ï¼‰
	usefulnessGracePeriod time.Duration

	// è¿è¡ŒçŠ¶æ€
	running  bool
	runMutex sync.RWMutex

	// ğŸ†• å°±ç»ªçŠ¶æ€ï¼ˆStartå®Œæˆä¸”localIDå·²åˆå§‹åŒ–ï¼‰
	ready      bool
	readyMutex sync.RWMutex

	// å¯è§‚æµ‹æ€§æŒ‡æ ‡
	metrics *KBucketMetrics

	// æ¢æµ‹å¹¶å‘æ§åˆ¶ï¼ˆPhase 2ï¼‰
	probeSemaphore chan struct{}
}

// NewRoutingTableManager åˆ›å»ºæ–°çš„è·¯ç”±è¡¨ç®¡ç†å™¨
// ä¸¥æ ¼æŒ‰ç…§defs-back/kbucket/table.goçš„NewRoutingTableé€»è¾‘
func NewRoutingTableManager(
	config kademlia.KBucketConfig,
	logger log.Logger,
	p2pService p2pi.Service,
	configProvider config.Provider, // æ–°å¢ï¼šç”¨äºè·å–æœ¬åœ°é“¾èº«ä»½è¿›è¡Œæ¯”å¯¹
) kademlia.RoutingTableManager {

	logger.Info("åˆ›å»ºKæ¡¶è·¯ç”±è¡¨ç®¡ç†å™¨")

	// åˆ›å»ºåˆå§‹æ¡¶ï¼ˆæ¥è‡ªåŸå§‹ç®—æ³•ï¼‰
	initialBucket := newBucket()

	manager := &RoutingTableManager{
		config:                config,
		logger:                logger,
		p2pService:            p2pService,
		configProvider:        configProvider,
		buckets:               []*Bucket{initialBucket},
		bucketSize:            config.GetBucketSize(),
		maxLatency:            config.GetMaxLatency(),
		cplRefreshedAt:        make(map[uint]time.Time),
		usefulnessGracePeriod: config.GetUsefulnessGracePeriod(),
		metrics:               &KBucketMetrics{},
		probeSemaphore:        make(chan struct{}, 5), // æœ€å¤š5ä¸ªå¹¶å‘æ¢æµ‹

		// é»˜è®¤ç©ºå›è°ƒ
		peerAdded:   func(peer.ID) {},
		peerRemoved: func(peer.ID) {},
	}

	// åˆ›å»ºä¸Šä¸‹æ–‡ï¼ˆæ¥è‡ªåŸå§‹ç®—æ³•ï¼‰
	manager.ctx, manager.ctxCancel = context.WithCancel(context.Background())

	logger.Info("Kæ¡¶è·¯ç”±è¡¨ç®¡ç†å™¨åˆ›å»ºå®Œæˆ")
	return manager
}

// Start å¯åŠ¨ç®¡ç†å™¨
func (rtm *RoutingTableManager) Start(ctx context.Context) error {
	rtm.runMutex.Lock()
	defer rtm.runMutex.Unlock()

	if rtm.running {
		return fmt.Errorf("routing table manager already running")
	}

	rtm.logger.Info("å¯åŠ¨Kæ¡¶è·¯ç”±è¡¨ç®¡ç†å™¨")

	// åˆå§‹åŒ–æœ¬åœ° DHT IDï¼ˆ32 bytesï¼‰ï¼Œç”¨äºæ­£ç¡®çš„ CPL/bucket è®¡ç®—ã€‚
	//
	// è¯´æ˜ï¼š
	// - ä¹‹å‰ localID ä¸ºç©ºä¼šå¯¼è‡´ CommonPrefixLen æ’ä¸º 0ï¼ˆæ‰€æœ‰èŠ‚ç‚¹è½åœ¨ bucket 0ï¼‰ï¼Œä¸ä¼šç›´æ¥é˜»å¡åŠŸèƒ½ï¼Œ
	//   ä½†ä¼šé™ä½ Kademlia é€‰æ‹©çš„è´¨é‡ï¼Œå½±å“åŒæ­¥/é€‰ä¸¾ç­‰ä¸Šå±‚ç­–ç•¥ã€‚
	// - è‹¥ P2P Host å°šæœªå°±ç»ªï¼Œåˆ™é€€åŒ–ä¸ºéšæœº IDï¼ˆä»ä¿è¯é•¿åº¦æ­£ç¡®ï¼‰ã€‚
	if rtm.p2pService != nil && rtm.p2pService.Host() != nil {
		rtm.localID = ConvertPeerID(rtm.p2pService.Host().ID())
	} else {
		rtm.localID = GenerateRandomID()
	}

	rtm.running = true

	// å¯åŠ¨ç»´æŠ¤åç¨‹
	go rtm.maintenanceLoop()

	// ğŸ”§ Phase 2ï¼šå¯åŠ¨æ¢æµ‹å·¥ä½œåç¨‹
	go rtm.probeWorker()

	// ğŸ†• æ ‡è®°å°±ç»ªçŠ¶æ€
	rtm.readyMutex.Lock()
	rtm.ready = true
	rtm.readyMutex.Unlock()

	rtm.logger.Info("âœ… Kæ¡¶è·¯ç”±è¡¨ç®¡ç†å™¨å·²å°±ç»ª")

	return nil
}

func (rtm *RoutingTableManager) setLastAdd(peerID peer.ID, result, reason string, err error) {
	if peerID == "" {
		return
	}
	errStr := ""
	if err != nil {
		errStr = err.Error()
	}
	la := types.NewKBucketLastAdd(peerID.String(), time.Now(), result, reason, errStr)
	rtm.lastAddMu.Lock()
	rtm.lastAdd = &la
	rtm.lastAddMu.Unlock()
}

// GetDiagnosticsSummary è¿”å› Kæ¡¶æ‘˜è¦ï¼ˆæ€»é‡/å¥åº·é‡/æœ€è¿‘å…¥æ¡¶åŸå› ï¼‰ï¼Œä¾›çº¿ä¸Šå¿«é€Ÿåˆ¤æ–­â€œç©ºæ¡¶é£é™©â€ã€‚
func (rtm *RoutingTableManager) GetDiagnosticsSummary() types.KBucketSummary {
	total, healthy := rtm.GetPeerCounts()
	var last *types.KBucketLastAdd
	rtm.lastAddMu.RLock()
	if rtm.lastAdd != nil {
		cp := *rtm.lastAdd
		last = &cp
	}
	rtm.lastAddMu.RUnlock()
	return types.KBucketSummary{
		TotalPeers:   total,
		HealthyPeers: healthy,
		LastAdd:      last,
	}
}

// Stop åœæ­¢ç®¡ç†å™¨
func (rtm *RoutingTableManager) Stop(ctx context.Context) error {
	rtm.runMutex.Lock()
	defer rtm.runMutex.Unlock()

	if !rtm.running {
		return nil
	}

	rtm.logger.Info("åœæ­¢Kæ¡¶è·¯ç”±è¡¨ç®¡ç†å™¨")
	rtm.ctxCancel()
	rtm.running = false

	// ğŸ†• æ¸…é™¤å°±ç»ªçŠ¶æ€
	rtm.readyMutex.Lock()
	rtm.ready = false
	rtm.readyMutex.Unlock()

	return nil
}

// IsRunning æ£€æŸ¥è¿è¡ŒçŠ¶æ€
func (rtm *RoutingTableManager) IsRunning() bool {
	rtm.runMutex.RLock()
	defer rtm.runMutex.RUnlock()
	return rtm.running
}

// ğŸ†• IsReady æ£€æŸ¥å°±ç»ªçŠ¶æ€ï¼ˆè¿è¡Œä¸­ä¸”å·²åˆå§‹åŒ–ï¼‰
func (rtm *RoutingTableManager) IsReady() bool {
	rtm.readyMutex.RLock()
	defer rtm.readyMutex.RUnlock()
	return rtm.ready && rtm.running
}

// ğŸ”§ Phase 3: SetEventBus è®¾ç½®äº‹ä»¶æ€»çº¿ï¼ˆç”±lifecycleæ³¨å…¥ï¼‰
func (rtm *RoutingTableManager) SetEventBus(eb event.EventBus) {
	rtm.eventBusMu.Lock()
	defer rtm.eventBusMu.Unlock()
	rtm.eventBus = eb
}

// AddPeer æ·»åŠ èŠ‚ç‚¹
// åŸºäºdefs-back/kbucket/table.goçš„TryAddPeeré€»è¾‘å®ç°
func (rtm *RoutingTableManager) AddPeer(ctx context.Context, addrInfo peer.AddrInfo) (bool, error) {
	if !rtm.IsRunning() {
		return false, fmt.Errorf("manager not running")
	}

	rtm.logger.Debugf("å°è¯•æ·»åŠ èŠ‚ç‚¹: %s", addrInfo.ID)

	// ğŸ”’ WESèŠ‚ç‚¹éªŒè¯ï¼šåªå…è®¸ä¸šåŠ¡èŠ‚ç‚¹è¿›å…¥Kæ¡¶
	if rtm.p2pService != nil {
		if isValidWES, err := rtm.validateWESPeer(ctx, addrInfo.ID); err != nil {
			// è¿™é‡Œå¿…é¡»è¿”å› errorï¼š
			// - è¯¥é”™è¯¯é€šå¸¸è¡¨ç¤º Identify/Peerstore å°šæœªå°±ç»ªã€Host æœªå°±ç»ªç­‰â€œå¯æ¢å¤â€é—®é¢˜ï¼›
			// - è¿”å› error èƒ½è§¦å‘ä¸Šå±‚ï¼ˆmodule.go çš„å»¶è¿Ÿé‡è¯•/å‘¨æœŸ reconcileï¼‰ç»§ç»­å°è¯•å…¥æ¡¶ï¼›
			// - åŒæ—¶ä¹Ÿèƒ½è®©æ—¥å¿—æ˜ç¡®æš´éœ²æ ¹å› ï¼Œé¿å…â€œæ°¸è¿œä¸å…¥æ¡¶â€ä½†çœ‹ä¸å‡ºåŸå› ã€‚
			rtm.logger.Debugf("èŠ‚ç‚¹ %s éªŒè¯å¤±è´¥ï¼ˆå¯æ¢å¤ï¼Œç¨åé‡è¯•ï¼‰: %v", addrInfo.ID, err)
			rtm.setLastAdd(addrInfo.ID, "error", "wes_check_error", err)
			return false, err
		} else if !isValidWES {
			rtm.logger.Debugf("æ‹’ç»å¤–éƒ¨èŠ‚ç‚¹è¿›å…¥Kæ¡¶: %s", addrInfo.ID)
			rtm.setLastAdd(addrInfo.ID, "rejected", "not_wes", nil)
			return false, nil // é™é»˜æ‹’ç»å¤–éƒ¨èŠ‚ç‚¹
		}
		// âœ… WESèŠ‚ç‚¹éªŒè¯é€šè¿‡ï¼Œç»§ç»­æ·»åŠ 
		rtm.logger.Debugf("WESèŠ‚ç‚¹éªŒè¯é€šè¿‡: %s", addrInfo.ID)
		rtm.setLastAdd(addrInfo.ID, "rejected", "weisyn_proto", nil) // å…ˆæ ‡è®°â€œé€šè¿‡WESè¯†åˆ«â€ï¼ŒæˆåŠŸå…¥æ¡¶ä¼šè¦†ç›–ä¸º added
	}

	// ğŸ”’ é“¾èº«ä»½éªŒè¯ï¼šæ£€æŸ¥ peer çš„é“¾èº«ä»½æ˜¯å¦åŒ¹é…
	if rtm.configProvider != nil && rtm.p2pService != nil {
		chainOK, reason, err := rtm.validatePeerChainIdentity(ctx, addrInfo.ID)
		if err != nil {
			// è§†ä¸ºâ€œå¯æ¢å¤é”™è¯¯â€ï¼šé€šå¸¸æ˜¯ Identify/peerstore å°šæœªå°±ç»ªï¼Œäº¤ç»™ä¸Šå±‚é‡è¯•/reconcileã€‚
			rtm.logger.Debugf("policy.chain_identity_error: peer=%s err=%v", addrInfo.ID, err)
			rtm.setLastAdd(addrInfo.ID, "error", "chain_identity_error", err)
			return false, err
		}
		if !chainOK {
			rtm.logger.Debugf("policy.reject_sync_peer: é“¾èº«ä»½ä¸åŒ¹é…/ç¼ºå¤±ï¼Œæ‹’ç»åŠ å…¥Kæ¡¶: peer=%s reason=%s", addrInfo.ID, reason)
			rtm.setLastAdd(addrInfo.ID, "rejected", reason, nil)
			return false, nil // é™é»˜æ‹’ç»ï¼Œä¸è¿”å›é”™è¯¯
		}
	}

	// å°†peer.IDè½¬æ¢ä¸ºDHT ID
	dhtID := ConvertPeerID(addrInfo.ID)

	// è®¡ç®—å…¬å…±å‰ç¼€é•¿åº¦æ¥ç¡®å®šæ¡¶ç´¢å¼•
	cpl := CommonPrefixLen(rtm.localID, dhtID)
	bucketIndex := cpl
	if bucketIndex >= len(rtm.buckets) {
		bucketIndex = len(rtm.buckets) - 1
	}

	rtm.tabLock.Lock()
	defer rtm.tabLock.Unlock()

	// ç¡®ä¿æ¡¶å­˜åœ¨
	rtm.ensureBucket(bucketIndex)

	bucket := rtm.buckets[bucketIndex]

	// æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å·²å­˜åœ¨
	if elem := bucket.find(addrInfo.ID); elem != nil {
		// èŠ‚ç‚¹å·²å­˜åœ¨ï¼Œç§»åˆ°å‰ç«¯ï¼ˆLRUæ›´æ–°ï¼‰
		bucket.moveToFront(elem)
		rtm.logger.Debugf("èŠ‚ç‚¹å·²å­˜åœ¨ï¼Œæ›´æ–°LRU: %s", addrInfo.ID)
		rtm.setLastAdd(addrInfo.ID, "already_exists", "unknown", nil)
		return true, nil
	}

	// æ£€æŸ¥æ¡¶æ˜¯å¦å·²æ»¡
	if bucket.len() >= rtm.bucketSize {
		// æ¡¶å·²æ»¡ï¼Œæ£€æŸ¥æœ€åä¸€ä¸ªèŠ‚ç‚¹æ˜¯å¦å¯æ›¿æ¢
		lastPeer := bucket.getPeers()[bucket.len()-1]
		if time.Since(lastPeer.LastUsefulAt) > rtm.usefulnessGracePeriod {
			// æœ€åä¸€ä¸ªèŠ‚ç‚¹å¤ªä¹…æœªä½¿ç”¨ï¼Œå¯ä»¥æ›¿æ¢
			bucket.remove(bucket.list.Back())
			rtm.logger.Debugf("æ›¿æ¢æœ€ä¹…æœªä½¿ç”¨çš„èŠ‚ç‚¹: %s -> %s", lastPeer.Id, addrInfo.ID)
		} else {
			rtm.logger.Debugf("æ¡¶å·²æ»¡ä¸”æ— æ³•æ›¿æ¢èŠ‚ç‚¹: %s", addrInfo.ID)
			rtm.setLastAdd(addrInfo.ID, "bucket_full", "bucket_full", nil)
			return false, nil
		}
	}

	// æ·»åŠ æ–°èŠ‚ç‚¹
	now := time.Now()
	peerInfo := &PeerInfo{
		Id:                            addrInfo.ID,
		Mode:                          0, // é»˜è®¤æ¨¡å¼
		LastUsefulAt:                  now,
		LastSuccessfulOutboundQueryAt: now,
		AddedAt:                       now,
		dhtId:                         dhtID,
		replaceable:                   false,
		peerState:                     PeerStateActive, // åˆå§‹çŠ¶æ€ä¸ºActive
		healthScore:                   100,             // åˆå§‹å¥åº·åˆ†100
		failureCount:                  0,
	}

	bucket.pushFront(peerInfo)

	// è§¦å‘å›è°ƒ
	rtm.peerAdded(addrInfo.ID)

	rtm.logger.Debugf("æˆåŠŸæ·»åŠ èŠ‚ç‚¹åˆ°æ¡¶ %d: %s", bucketIndex, addrInfo.ID)
	rtm.setLastAdd(addrInfo.ID, "added", "unknown", nil)
	return true, nil
}

// RemovePeer ç§»é™¤èŠ‚ç‚¹
func (rtm *RoutingTableManager) RemovePeer(peerID peer.ID) error {
	if !rtm.IsRunning() {
		return fmt.Errorf("manager not running")
	}

	rtm.logger.Debugf("ç§»é™¤èŠ‚ç‚¹: %s", peerID)

	rtm.tabLock.Lock()
	defer rtm.tabLock.Unlock()

	// éå†æ‰€æœ‰æ¡¶æŸ¥æ‰¾å¹¶ç§»é™¤èŠ‚ç‚¹
	for i, bucket := range rtm.buckets {
		if elem := bucket.find(peerID); elem != nil {
			bucket.remove(elem)
			rtm.peerRemoved(peerID)
			rtm.logger.Debugf("ä»æ¡¶ %d ç§»é™¤èŠ‚ç‚¹: %s", i, peerID)
			
			// âœ… ä¿®å¤ç¼ºé™·Mï¼šå–æ¶ˆä¿æŠ¤å·²åˆ é™¤çš„peerè¿æ¥
			// å½“peerä»Kæ¡¶åˆ é™¤æ—¶ï¼Œåº”å–æ¶ˆè¿æ¥ä¿æŠ¤ï¼Œå…è®¸è¿æ¥ç®¡ç†å™¨æ ¹æ®éœ€è¦æ·˜æ±°è¿™äº›è¿æ¥
			if rtm.p2pService != nil && rtm.p2pService.Host() != nil {
				if cm := rtm.p2pService.Host().ConnManager(); cm != nil {
					cm.Unprotect(peerID, "kbucket")
					rtm.logger.Debugf("ğŸ”“ å·²å–æ¶ˆä¿æŠ¤Kæ¡¶peerè¿æ¥: %s", peerID)
				}
			}
			
			return nil
		}
	}

	return fmt.Errorf("peer not found: %s", peerID)
}

// FindClosestPeers æŸ¥æ‰¾æœ€è¿‘èŠ‚ç‚¹ï¼ˆå¸¦å¥åº·è¿‡æ»¤ï¼‰
// åŸºäºdefs-back/kbucket/table.goçš„NearestPeersç®—æ³•å®ç°
func (rtm *RoutingTableManager) FindClosestPeers(target []byte, count int) []peer.ID {
	if !rtm.IsRunning() {
		rtm.logger.Warn("ç®¡ç†å™¨æœªè¿è¡Œ")
		return nil
	}

	if count <= 0 {
		return nil
	}

	rtm.logger.Debugf("æŸ¥æ‰¾è·ç¦»ç›®æ ‡æœ€è¿‘çš„%dä¸ªå¥åº·èŠ‚ç‚¹", count)

	rtm.tabLock.RLock()
	defer rtm.tabLock.RUnlock()

	// è·å–libp2p hostç”¨äºè¿æ¥çŠ¶æ€æ£€æŸ¥
	var libp2pHost interface{}
	if rtm.p2pService != nil {
		libp2pHost = rtm.p2pService.Host()
	}

	// è®¡ç®—ç›®æ ‡çš„å…¬å…±å‰ç¼€é•¿åº¦
	cpl := CommonPrefixLen(rtm.localID, target)

	// æ”¶é›†å€™é€‰èŠ‚ç‚¹ï¼ˆä»…æ”¶é›†å¥åº·çš„ï¼‰
	var candidates []peer.ID
	var suspectCandidates []peer.ID // SuspectèŠ‚ç‚¹å•ç‹¬æ”¶é›†ï¼Œä½œä¸ºç°åº¦æ¢æµ‹å€™é€‰

	// ä»ç›®æ ‡æ¡¶å¼€å§‹ï¼Œå‘å¤–æ‰©å±•æœç´¢
	bucketIndex := cpl
	if bucketIndex >= len(rtm.buckets) {
		bucketIndex = len(rtm.buckets) - 1
	}

	// æœç´¢ç­–ç•¥ï¼šä»ç›®æ ‡æ¡¶å¼€å§‹ï¼Œç„¶åå‘ä¸¤ä¾§æ‰©å±•
	visited := make(map[int]bool)

	for len(candidates)+len(suspectCandidates) < count*3 && len(visited) < len(rtm.buckets) {
		// æœç´¢å½“å‰æ¡¶
		if bucketIndex >= 0 && bucketIndex < len(rtm.buckets) && !visited[bucketIndex] {
			visited[bucketIndex] = true
			bucket := rtm.buckets[bucketIndex]
			peers := bucket.getPeers()

			for _, p := range peers {
				// å¥åº·è¿‡æ»¤
				if rtm.isPeerHealthy(p, libp2pHost) {
					candidates = append(candidates, p.Id)
				} else if p.GetState() == PeerStateSuspect {
					// SuspectèŠ‚ç‚¹ä¿ç•™ä½œä¸ºç°åº¦æ¢æµ‹å€™é€‰
					suspectCandidates = append(suspectCandidates, p.Id)
				}
			}
		}

		// äº¤æ›¿å‘ä¸¤ä¾§æ‰©å±•
		if bucketIndex > 0 {
			bucketIndex--
		} else if bucketIndex < len(rtm.buckets)-1 {
			bucketIndex++
		} else {
			break
		}
	}

	// å¦‚æœå€™é€‰èŠ‚ç‚¹ä¸å¤Ÿï¼Œä»æ‰€æœ‰æ¡¶æ”¶é›†
	if len(candidates)+len(suspectCandidates) < count {
		for i, bucket := range rtm.buckets {
			if !visited[i] {
				peers := bucket.getPeers()
				for _, p := range peers {
					if rtm.isPeerHealthy(p, libp2pHost) {
						candidates = append(candidates, p.Id)
					} else if p.GetState() == PeerStateSuspect {
						suspectCandidates = append(suspectCandidates, p.Id)
					}
				}
			}
		}
	}

	// ä½¿ç”¨èŠ‚ç‚¹é€‰æ‹©å™¨æŒ‰è·ç¦»æ’åºå¹¶é€‰æ‹©æœ€è¿‘çš„
	closest := SelectClosestPeers(candidates, target, count, rtm.logger)

	// å¦‚æœå¥åº·èŠ‚ç‚¹ä¸å¤Ÿï¼Œé€‚é‡æ·»åŠ SuspectèŠ‚ç‚¹ä½œä¸ºç°åº¦æ¢æµ‹
	if len(closest) < count && len(suspectCandidates) > 0 {
		remaining := count - len(closest)
		if remaining > len(suspectCandidates)/2 {
			remaining = len(suspectCandidates) / 2 // æœ€å¤šæ·»åŠ ä¸€åŠSuspectèŠ‚ç‚¹
		}
		suspectClosest := SelectClosestPeers(suspectCandidates, target, remaining, rtm.logger)
		closest = append(closest, suspectClosest...)
		rtm.logger.Debugf("æ·»åŠ %dä¸ªSuspectèŠ‚ç‚¹ä½œä¸ºç°åº¦æ¢æµ‹å€™é€‰", len(suspectClosest))
	}

	rtm.logger.Debugf("æ‰¾åˆ° %d ä¸ªå¥åº·èŠ‚ç‚¹ï¼ˆåŒ…å«%dä¸ªSuspectç°åº¦ï¼‰", len(closest), len(closest)-len(candidates))

	// ğŸ”§ Phase 3: è®°å½•FindClosestPeerså¤±è´¥äº‹ä»¶å¹¶è§¦å‘Discoveryé—´éš”é‡ç½®
	if len(closest) == 0 {
		if rtm.metrics != nil {
			rtm.metrics.RecordNoClosestPeers()
		}
		
		// å‘å¸ƒDiscoveryé—´éš”é‡ç½®äº‹ä»¶ï¼Œè®©å‘ç°å¾ªç¯ç«‹å³åŠ é€Ÿ
		rtm.eventBusMu.RLock()
		eb := rtm.eventBus
		rtm.eventBusMu.RUnlock()
		
		if eb != nil {
			resetData := &types.DiscoveryResetEventData{
				Reason:           "kbucket_degraded",
				Trigger:          "kademlia",
				RoutingTableSize: 0,
				Timestamp:        time.Now().Unix(),
			}
			eb.Publish(events.EventTypeDiscoveryIntervalReset, resetData)
			if rtm.logger != nil {
				rtm.logger.Infof("ğŸ”„ Kæ¡¶é€€åŒ–ï¼šFindClosestPeersæ‰¾ä¸åˆ°èŠ‚ç‚¹ï¼Œå·²è§¦å‘Discoveryé—´éš”é‡ç½®")
			}
		}
	}

	return closest
}

// isPeerHealthy æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å¥åº·ï¼ˆå¯è¢«é€‰ç”¨ï¼‰
func (rtm *RoutingTableManager) isPeerHealthy(p *PeerInfo, libp2pHostInterface interface{}) bool {
	// 1. æ£€æŸ¥å¥åº·åˆ†
	if p.GetHealthScore() < 50 {
		return false
	}

	// 2. æ£€æŸ¥æ˜¯å¦è¢«éš”ç¦»
	if p.IsQuarantined() {
		return false
	}

	// 3. æ£€æŸ¥è¿æ¥çŠ¶æ€ï¼ˆå¦‚æœæœ‰hostï¼‰
	if libp2pHostInterface != nil {
		// å°è¯•ä»rtm.p2pServiceè·å–è¿æ¥çŠ¶æ€ï¼ˆæ›´ç›´æ¥çš„æ–¹å¼ï¼‰
		if rtm.p2pService != nil {
			if h := rtm.p2pService.Host(); h != nil {
				// ç®€åŒ–è¿æ¥æ£€æŸ¥ï¼šé€šè¿‡æŸ¥è¯¢peerstoreåœ°å€æ˜¯å¦å­˜åœ¨
				addrs := h.Peerstore().Addrs(p.Id)
				if len(addrs) == 0 {
					return false // æ— åœ°å€ä¿¡æ¯ï¼Œè®¤ä¸ºä¸å¯è¾¾
				}
				// è¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å¦æœ‰æ´»è·ƒè¿æ¥
				conns := h.Network().ConnsToPeer(p.Id)
				if len(conns) == 0 {
					return false // æ— æ´»è·ƒè¿æ¥
				}
			}
		}
	}

	// 4. Activeæˆ–SuspectçŠ¶æ€èŠ‚ç‚¹å¯ä»¥è¢«é€‰ç”¨
	return p.GetState() == PeerStateActive || p.GetState() == PeerStateSuspect
}

// GetRoutingTable è·å–è·¯ç”±è¡¨å¿«ç…§
func (rtm *RoutingTableManager) GetRoutingTable() *kademlia.RoutingTable {
	rtm.tabLock.RLock()
	defer rtm.tabLock.RUnlock()

	buckets := make([]*kademlia.Bucket, len(rtm.buckets))
	totalPeers := 0

	for i, bucket := range rtm.buckets {
		peers := bucket.getPeers()
		totalPeers += len(peers)

		kbucketPeers := make([]*kademlia.PeerInfo, len(peers))
		for j, peer := range peers {
			kbucketPeers[j] = &kademlia.PeerInfo{
				ID:                peer.Id.String(),
				LastSeen:          types.Timestamp(peer.LastUsefulAt),
				LastUsefulAt:      types.Timestamp(peer.LastUsefulAt),
				AddedAt:           types.Timestamp(peer.AddedAt),
				ConnectionLatency: time.Duration(0), // å®é™…åº”ä»è¿æ¥ç›‘æ§è·å–
				IsReplaceable:     peer.replaceable,
				DHTId:             peer.dhtId,
				Mode:              peer.Mode,
			}
		}

		buckets[i] = &kademlia.Bucket{
			Index: i,
			Peers: kbucketPeers,
		}
	}

	return &kademlia.RoutingTable{
		LocalID:    string(rtm.localID),
		Buckets:    buckets,
		BucketSize: rtm.bucketSize,
		TableSize:  totalPeers,
		UpdatedAt:  types.Timestamp(time.Now()),
	}
}

// GetPeerCounts è·å–èŠ‚ç‚¹ç»Ÿè®¡ä¿¡æ¯
func (rtm *RoutingTableManager) GetPeerCounts() (totalPeers, healthyPeers int) {
	rtm.tabLock.RLock()
	defer rtm.tabLock.RUnlock()

	now := time.Now()

	// ç»Ÿè®¡æ‰€æœ‰æ¡¶ä¸­çš„èŠ‚ç‚¹
	for _, bucket := range rtm.buckets {
		peers := bucket.getPeers()
		totalPeers += len(peers)

		// å¥åº·æ£€æŸ¥ï¼šæœ€è¿‘æ´»è·ƒæ—¶é—´åœ¨å®½é™æœŸå†…çš„èŠ‚ç‚¹è®¤ä¸ºæ˜¯å¥åº·çš„
		for _, peer := range peers {
			if now.Sub(peer.LastUsefulAt) <= rtm.usefulnessGracePeriod {
				healthyPeers++
			}
		}
	}

	return totalPeers, healthyPeers
}

// SetPeerAddedCallback è®¾ç½®èŠ‚ç‚¹æ·»åŠ å›è°ƒ
func (rtm *RoutingTableManager) SetPeerAddedCallback(callback func(peer.ID)) {
	rtm.tabLock.Lock()
	defer rtm.tabLock.Unlock()
	rtm.peerAdded = callback
}

// SetPeerRemovedCallback è®¾ç½®èŠ‚ç‚¹ç§»é™¤å›è°ƒ
func (rtm *RoutingTableManager) SetPeerRemovedCallback(callback func(peer.ID)) {
	rtm.tabLock.Lock()
	defer rtm.tabLock.Unlock()
	rtm.peerRemoved = callback
}

// ensureBucket ç¡®ä¿æŒ‡å®šç´¢å¼•çš„æ¡¶å­˜åœ¨
func (rtm *RoutingTableManager) ensureBucket(index int) {
	for len(rtm.buckets) <= index {
		newBucket := newBucket()
		rtm.buckets = append(rtm.buckets, newBucket)
		rtm.logger.Debugf("åˆ›å»ºæ–°æ¡¶ï¼Œç´¢å¼•: %d", len(rtm.buckets)-1)
	}
}

// RecordPeerFailure è®°å½•èŠ‚ç‚¹å¤±è´¥
func (rtm *RoutingTableManager) RecordPeerFailure(peerID peer.ID) {
	rtm.tabLock.RLock()
	defer rtm.tabLock.RUnlock()

	// æŸ¥æ‰¾èŠ‚ç‚¹å¹¶è®°å½•å¤±è´¥
	for _, bucket := range rtm.buckets {
		if elem := bucket.find(peerID); elem != nil {
			p := elem.Value.(*PeerInfo)
			p.RecordFailure(rtm.config.GetFailureThreshold(), rtm.config.GetQuarantineDuration())
			rtm.logger.Debugf("è®°å½•èŠ‚ç‚¹å¤±è´¥: %s, çŠ¶æ€=%s, å¥åº·åˆ†=%.1f, å¤±è´¥æ¬¡æ•°=%d",
				peerID, p.GetState(), p.GetHealthScore(), p.failureCount)
			return
		}
	}
}

// RecordPeerSuccess è®°å½•èŠ‚ç‚¹æˆåŠŸ
func (rtm *RoutingTableManager) RecordPeerSuccess(peerID peer.ID) {
	rtm.tabLock.RLock()
	defer rtm.tabLock.RUnlock()

	// æŸ¥æ‰¾èŠ‚ç‚¹å¹¶è®°å½•æˆåŠŸ
	for _, bucket := range rtm.buckets {
		if elem := bucket.find(peerID); elem != nil {
			p := elem.Value.(*PeerInfo)
			p.RecordSuccess()
			rtm.logger.Debugf("è®°å½•èŠ‚ç‚¹æˆåŠŸ: %s, çŠ¶æ€=%s, å¥åº·åˆ†=%.1f",
				peerID, p.GetState(), p.GetHealthScore())
			// æˆåŠŸåç§»åˆ°æ¡¶å‰ç«¯ï¼ˆLRUæ›´æ–°ï¼‰
			bucket.moveToFront(elem)
			return
		}
	}
}

// QuarantineIncompatiblePeer ç›´æ¥éš”ç¦»ä¸å…¼å®¹çš„èŠ‚ç‚¹
//
// ğŸ†• 2025-12-18ï¼šç”¨äºå¤„ç†æ˜ç¡®ä¸æ”¯æŒ WES åè®®çš„èŠ‚ç‚¹
//
// ä¸ RecordPeerFailure çš„åŒºåˆ«ï¼š
// - RecordPeerFailure: éœ€è¦å¤šæ¬¡å¤±è´¥æ‰ä¼šè¿›å…¥éš”ç¦»çŠ¶æ€ï¼ˆæ¸è¿›å¼é™çº§ï¼‰
// - QuarantineIncompatiblePeer: ç›´æ¥è¿›å…¥éš”ç¦»çŠ¶æ€ï¼ˆåè®®ä¸å…¼å®¹æ˜¯æ˜ç¡®çš„ä¸å…¼å®¹ï¼Œæ— éœ€æ¸è¿›ï¼‰
//
// éš”ç¦»æ•ˆæœï¼š
// - èŠ‚ç‚¹çŠ¶æ€è®¾ç½®ä¸º Quarantined
// - å¥åº·åˆ†è®¾ç½®ä¸º 0
// - éš”ç¦»æ—¶é—´ä¸ºé…ç½®çš„éš”ç¦»æœŸï¼ˆé»˜è®¤ 1 å°æ—¶ï¼‰
// - éš”ç¦»æœŸé—´èŠ‚ç‚¹ä¸ä¼šè¢«é€‰ä¸ºèšåˆå™¨/åŒæ­¥ä¸Šæ¸¸
//
// å‚æ•°ï¼š
// - peerID: è¦éš”ç¦»çš„èŠ‚ç‚¹ ID
// - reason: éš”ç¦»åŸå› ï¼ˆç”¨äºæ—¥å¿—ï¼‰
func (rtm *RoutingTableManager) QuarantineIncompatiblePeer(peerID peer.ID, reason string) {
	rtm.tabLock.RLock()
	defer rtm.tabLock.RUnlock()

	quarantineDuration := rtm.config.GetQuarantineDuration()

	// æŸ¥æ‰¾èŠ‚ç‚¹å¹¶ç›´æ¥éš”ç¦»
	for _, bucket := range rtm.buckets {
		if elem := bucket.find(peerID); elem != nil {
			p := elem.Value.(*PeerInfo)

			p.stateLock.Lock()
			p.peerState = PeerStateQuarantined
			p.healthScore = 0
			p.quarantinedUntil = time.Now().Add(quarantineDuration)
			p.stateLock.Unlock()

			rtm.logger.Infof("ğŸ”’ éš”ç¦»ä¸å…¼å®¹èŠ‚ç‚¹: peer=%s reason=%s duration=%s",
				peerID.String()[:12], reason, quarantineDuration)
			return
		}
	}

	// èŠ‚ç‚¹ä¸åœ¨ K æ¡¶ä¸­ï¼Œè®°å½•æ—¥å¿—ï¼ˆå¯èƒ½å·²è¢«æ¸…ç†ï¼‰
	rtm.logger.Debugf("å°è¯•éš”ç¦»ä¸å­˜åœ¨çš„èŠ‚ç‚¹: peer=%s reason=%s", peerID.String()[:12], reason)
}

// maintenanceLoop ç»´æŠ¤åç¨‹ï¼šå‘¨æœŸæ€§æ‰§è¡Œå¥åº·ç®¡ç†ä»»åŠ¡
func (rtm *RoutingTableManager) maintenanceLoop() {
	ticker := time.NewTicker(rtm.config.GetMaintainInterval())
	defer ticker.Stop()

	for {
		select {
		case <-rtm.ctx.Done():
			rtm.logger.Info("ç»´æŠ¤åç¨‹æ”¶åˆ°åœæ­¢ä¿¡å·")
			return
		case <-ticker.C:
			rtm.runMaintenance()
		}
	}
}

// runMaintenance æ‰§è¡Œç»´æŠ¤ä»»åŠ¡
func (rtm *RoutingTableManager) runMaintenance() {
	if !rtm.IsRunning() {
		return
	}

	rtm.tabLock.Lock()
	defer rtm.tabLock.Unlock()

	// è®°å½•ç»´æŠ¤æ‰§è¡Œ
	if rtm.metrics != nil {
		rtm.metrics.RecordMaintenanceRun()
	}

	now := time.Now()
	halfLife := rtm.config.GetHealthDecayHalfLife()
	minPeers := rtm.config.GetMinPeersPerBucket()
	gracePeriod := rtm.usefulnessGracePeriod

	// ğŸ”§ ä¸ºæ‰€æœ‰å·²è¿æ¥peeræ›´æ–°LastUsefulAtï¼ˆè‡ªåŠ¨ç»­æœŸï¼‰
	if rtm.p2pService != nil && rtm.p2pService.Host() != nil {
		host := rtm.p2pService.Host()
		for _, bucket := range rtm.buckets {
			bucket.updateAllWith(func(p *PeerInfo) {
				if host.Network().Connectedness(p.Id) == libnetwork.Connected {
					// è¿æ¥ä¸­çš„peerï¼Œè‡ªåŠ¨ç»­æœŸLastUsefulAt
					p.LastUsefulAt = now
				}
			})
		}
	}

	for bucketIdx, bucket := range rtm.buckets {
		if bucket.len() == 0 {
			continue
		}

		// 1. å¥åº·åˆ†è¡°å‡ï¼ˆåŸºäºÎ”tï¼‰
		bucket.updateAllWith(func(p *PeerInfo) {
			p.DecayHealth(now, halfLife)
		})

		// 2. æ£€æŸ¥å¹¶è§£é™¤è¿‡æœŸçš„éš”ç¦»
		bucket.updateAllWith(func(p *PeerInfo) {
			if p.CheckQuarantineExpired() {
				rtm.logger.Debugf("èŠ‚ç‚¹éš”ç¦»æœŸè¿‡æœŸï¼Œé™çº§ä¸ºSuspect: %s", p.Id)
			}
		})

		// 3. ğŸ†• ä¸»åŠ¨æ¸…ç†SuspectèŠ‚ç‚¹ï¼ˆä¿®å¤å†…å­˜æ³„æ¼ï¼‰
		rtm.cleanupSuspectPeers(bucket, bucketIdx)

		// 4. æ¸…ç†é•¿æœŸä¸å¯è¾¾ä¸”ä¸å¥åº·çš„èŠ‚ç‚¹ï¼ˆä»…å½“æ¡¶æœ‰ä½™é‡æ—¶ï¼‰
		if bucket.len() > minPeers {
			rtm.cleanupUnhealthyPeers(bucket, bucketIdx, gracePeriod)
		}

		// 5. ğŸ”§ Phase 2ï¼šæœ€ç»ˆæ¸…ç†ï¼ˆåªåˆ é™¤æ¢æµ‹ç¡®è®¤å¤±è´¥çš„peerï¼‰
		rtm.finalCleanup(bucket, bucketIdx)
	}

	// 6. ğŸ†• æ£€æŸ¥æ€»peeræ•°é‡ï¼Œå¦‚æœè¿‡å¤šåˆ™å¼ºåˆ¶æ¸…ç†
	totalPeers := rtm.sizeNoLock()
	if totalPeers > 500 {
		rtm.logger.Warnf("Peeræ€»æ•°è¿‡å¤š(%d)ï¼Œæ‰§è¡Œå¼ºåˆ¶æ¸…ç†", totalPeers)
		rtm.forceCleanupOldestSuspect(50)
	}

	// 7. æ›´æ–°çŠ¶æ€åˆ†å¸ƒæŒ‡æ ‡
	if rtm.metrics != nil {
		var active, suspect, quarantined, evicted int64
		for _, bucket := range rtm.buckets {
			for e := bucket.list.Front(); e != nil; e = e.Next() {
				p := e.Value.(*PeerInfo)
				switch p.GetState() {
				case PeerStateActive:
					active++
				case PeerStateSuspect:
					suspect++
				case PeerStateQuarantined:
					quarantined++
				case PeerStateEvicted:
					evicted++
				}
			}
		}
		rtm.metrics.UpdateStateDistribution(active, suspect, quarantined, evicted)
	}
}

// cleanupUnhealthyPeers æ¸…ç†ä¸å¥åº·çš„èŠ‚ç‚¹
func (rtm *RoutingTableManager) cleanupUnhealthyPeers(bucket *Bucket, bucketIdx int, gracePeriod time.Duration) {
	now := time.Now()
	minPeers := rtm.config.GetMinPeersPerBucket()
	cleanupGracePeriod := rtm.config.GetCleanupGracePeriod()
	lowHealthThreshold := rtm.config.GetLowHealthThreshold()
	addrProtectionGracePeriod := rtm.config.GetAddrProtectionGracePeriod()

	// è·å–hostç”¨äºæ£€æŸ¥è¿æ¥çŠ¶æ€
	var host lphost.Host
	if rtm.p2pService != nil {
		host = rtm.p2pService.Host()
	}

	// ğŸ”§ Phase 2ï¼šæ ‡è®°å¾…æ¸…ç†peerï¼ˆä¸ç«‹å³åˆ é™¤ï¼‰
	for e := bucket.list.Front(); e != nil; e = e.Next() {
		p := e.Value.(*PeerInfo)

		// ğŸ”§ ç¡¬çº¦æŸï¼šå¿…é¡»å…ˆæ£€æŸ¥è¿æ¥çŠ¶æ€
		isConnected := false
		if host != nil {
			connectedness := host.Network().Connectedness(p.Id)
			isConnected = (connectedness == libnetwork.Connected)
			if isConnected {
				// ä»è¿æ¥çš„peerï¼Œè·³è¿‡æ¸…ç†
				rtm.logger.Debugf("è·³è¿‡æ¸…ç†å·²è¿æ¥peer: bucket=%d, peer=%s, state=%s, health=%.1f",
					bucketIdx, p.Id, p.GetState(), p.GetHealthScore())
				continue
			}
		}

		// === P0-010ï¼šæ¸…ç†æ¡ä»¶ä¿å®ˆåŒ– ===
		// 1) â€œé•¿æœŸæ— ç”¨â€ä¸å†ç”¨ gracePeriod*3ï¼ˆé»˜è®¤çº¦3åˆ†é’Ÿï¼Œè¿‡äºæ¿€è¿›ï¼‰ï¼Œæ”¹ä¸ºç‹¬ç«‹çš„ CleanupGracePeriodï¼ˆé»˜è®¤10åˆ†é’Ÿï¼‰
		// 2) ä½å¥åº·é˜ˆå€¼ä» 20 é™åˆ° 10ï¼ˆæ›´ä¿å®ˆï¼‰
		// 3) è‹¥ peerstore ä¸­ä»æœ‰åœ°å€ï¼Œç»™äºˆæ›´é•¿çš„ä¿æŠ¤çª—å£ï¼ˆ30åˆ†é’Ÿï¼‰ï¼Œå‡å°‘è¯¯æ¸…ç†å¯¼è‡´ç½‘ç»œå­¤å²›
		//
		// æ³¨æ„ï¼šLastUsefulAt å¯èƒ½ä¸ºé›¶å€¼ï¼ˆå†å²/æ„é€ æµ‹è¯•ï¼‰ï¼Œæ­¤æ—¶å›é€€åˆ° AddedAtï¼Œé¿å…è¢«è¯¯åˆ¤ä¸ºâ€œå¾ˆä¹…ä»¥å‰â€
		lastUsefulRef := p.LastUsefulAt
		if lastUsefulRef.IsZero() {
			lastUsefulRef = p.AddedAt
		}
		if lastUsefulRef.IsZero() {
			lastUsefulRef = now
		}

		// æ–­è¿ peer çš„â€œé‡è¿å®½é™æœŸâ€ï¼šåœ¨ cleanupGracePeriod å†…ä¸è¿›å…¥æ¸…ç†æµç¨‹
		if now.Sub(lastUsefulRef) < cleanupGracePeriod {
			continue
		}

		// è‹¥ä»æœ‰åœ°å€ï¼Œé¢å¤–ä¿æŠ¤ï¼ˆç”±é…ç½®é¡¹æ§åˆ¶ï¼‰
		if host != nil {
			if addrs := host.Peerstore().Addrs(p.Id); len(addrs) > 0 {
				if now.Sub(lastUsefulRef) < addrProtectionGracePeriod {
					continue
				}
			}
		}

		// æ¸…ç†æ¡ä»¶ï¼šæ–­è¿ + (é•¿æœŸæ— ç”¨ + ä½å¥åº·) OR EvictedçŠ¶æ€ + æ¡¶æœ‰ä½™é‡
		longTimeUnused := now.Sub(lastUsefulRef) > cleanupGracePeriod
		lowHealth := p.GetHealthScore() < lowHealthThreshold
		isEvicted := p.GetState() == PeerStateEvicted

		// ğŸ”§ Phase 2ï¼šä¸ç«‹å³åˆ é™¤ï¼Œæ ‡è®°ä¸ºå¾…æ¢æµ‹
		if (longTimeUnused && lowHealth) || isEvicted {
			if bucket.len() > minPeers {
				// æ ‡è®°ä¸ºå¾…æ¢æµ‹ï¼ˆè€Œéç«‹å³åˆ é™¤ï¼‰
				p.stateLock.Lock()
				if p.probeStatus == ProbeNotNeeded || p.probeStatus == ProbeSuccess {
					p.probeStatus = ProbePending
					p.lastProbeAt = time.Time{} // é‡ç½®æ¢æµ‹æ—¶é—´
					p.probeFailCount = 0        // é‡ç½®å¤±è´¥è®¡æ•°
					rtm.logger.Debugf("æ ‡è®°peerå¾…æ¢æµ‹æ¸…ç†: bucket=%d, peer=%s, state=%s, health=%.1f",
						bucketIdx, p.Id, p.GetState(), p.GetHealthScore())
				}
				p.stateLock.Unlock()
			}
		}
	}
}

// cleanupSuspectPeers æ¸…ç†æ–­è¿çš„Suspect/QuarantinedèŠ‚ç‚¹
func (rtm *RoutingTableManager) cleanupSuspectPeers(bucket *Bucket, bucketIdx int) {
	// è·å–hostç”¨äºæ£€æŸ¥è¿æ¥çŠ¶æ€
	var host lphost.Host
	if rtm.p2pService != nil {
		host = rtm.p2pService.Host()
	}

	if host == nil {
		return // æ— æ³•æ£€æŸ¥è¿æ¥çŠ¶æ€ï¼Œè·³è¿‡æ¸…ç†
	}

	now := time.Now()

	// ğŸ”§ Phase 2ï¼šæ ‡è®°å¾…æ¢æµ‹ï¼ˆä¸ç«‹å³åˆ é™¤ï¼‰
	for e := bucket.list.Front(); e != nil; e = e.Next() {
		p := e.Value.(*PeerInfo)

		// ğŸ”§ ç¡¬çº¦æŸï¼šåªæ¸…ç†å·²æ–­è¿çš„peer
		connectedness := host.Network().Connectedness(p.Id)
		if connectedness == libnetwork.Connected {
			continue
		}

		// ğŸ”§ Phase 2ï¼šä¸ç«‹å³åˆ é™¤ï¼Œæ ‡è®°ä¸ºå¾…æ¢æµ‹
		// Suspectæ–­è¿ä¸”é•¿æœŸæ— ç”¨ï¼ˆæ›´ä¿å®ˆçš„é˜ˆå€¼ï¼š10åˆ†é’Ÿï¼‰
		if p.GetState() == PeerStateSuspect {
			if now.Sub(p.LastUsefulAt) > 10*time.Minute {
				p.stateLock.Lock()
				if p.probeStatus == ProbeNotNeeded || p.probeStatus == ProbeSuccess {
					p.probeStatus = ProbePending
					p.lastProbeAt = time.Time{}
					p.probeFailCount = 0
					rtm.logger.Debugf("æ ‡è®°Suspect peerå¾…æ¢æµ‹æ¸…ç†: bucket=%d, peer=%s",
						bucketIdx, p.Id)
				}
				p.stateLock.Unlock()
			}
		}

		// Quarantinedæ–­è¿ä¸”éš”ç¦»æœŸè¿‡æœŸè¶…è¿‡10åˆ†é’Ÿ
		if p.GetState() == PeerStateQuarantined {
			if now.Sub(p.LastUsefulAt) > 10*time.Minute {
				p.stateLock.Lock()
				if p.probeStatus == ProbeNotNeeded || p.probeStatus == ProbeSuccess {
					p.probeStatus = ProbePending
					p.lastProbeAt = time.Time{}
					p.probeFailCount = 0
					rtm.logger.Debugf("æ ‡è®°Quarantined peerå¾…æ¢æµ‹æ¸…ç†: bucket=%d, peer=%s",
						bucketIdx, p.Id)
				}
				p.stateLock.Unlock()
			}
		}
	}

	// Phase 2ï¼šä¸å†åœ¨æ­¤å¤„æ‰§è¡Œæ¸…ç†ï¼Œæ”¹ç”±finalCleanupå¤„ç†
}

// forceCleanupOldestSuspect ğŸ†• å¼ºåˆ¶æ¸…ç†æœ€è€çš„SuspectèŠ‚ç‚¹ï¼ˆä¿®å¤å†…å­˜æ³„æ¼ï¼‰
func (rtm *RoutingTableManager) forceCleanupOldestSuspect(count int) {
	type suspectPeer struct {
		bucket *Bucket
		elem   *list.Element
		peer   *PeerInfo
	}

	var suspects []suspectPeer

	// æ”¶é›†æ‰€æœ‰Suspectå’ŒQuarantinedèŠ‚ç‚¹
	for _, bucket := range rtm.buckets {
		for e := bucket.list.Front(); e != nil; e = e.Next() {
			p := e.Value.(*PeerInfo)
			if p.GetState() == PeerStateSuspect || p.GetState() == PeerStateQuarantined {
				suspects = append(suspects, suspectPeer{
					bucket: bucket,
					elem:   e,
					peer:   p,
				})
			}
		}
	}

	// æŒ‰LastUsefulAtæ’åºï¼ˆæœ€è€çš„åœ¨å‰é¢ï¼‰
	// ç®€åŒ–å®ç°ï¼šæ¸…ç†å‰Nä¸ª
	cleanCount := count
	if cleanCount > len(suspects) {
		cleanCount = len(suspects)
	}

	for i := 0; i < cleanCount; i++ {
		sp := suspects[i]
		sp.bucket.remove(sp.elem)
		rtm.peerRemoved(sp.peer.Id)
		rtm.logger.Infof("å¼ºåˆ¶æ¸…ç†SuspectèŠ‚ç‚¹: peer=%s", sp.peer.Id)
	}
}

// sizeNoLock ğŸ†• è·å–peeræ€»æ•°ï¼ˆä¸åŠ é”ç‰ˆæœ¬ï¼Œåœ¨å·²åŠ é”çš„æƒ…å†µä¸‹è°ƒç”¨ï¼‰
func (rtm *RoutingTableManager) sizeNoLock() int {
	count := 0
	for _, bucket := range rtm.buckets {
		count += bucket.len()
	}
	return count
}

// validateWESPeer éªŒè¯èŠ‚ç‚¹æ˜¯å¦ä¸ºWESä¸šåŠ¡èŠ‚ç‚¹
// åŸºäºåè®®èƒ½åŠ›æ£€æŸ¥å®ç°ç®€å•çš„èŠ‚ç‚¹åˆ†ç±»
func (rtm *RoutingTableManager) validateWESPeer(ctx context.Context, peerID peer.ID) (bool, error) {
	if rtm.p2pService == nil {
		return false, fmt.Errorf("p2p service not available")
	}

	host := rtm.p2pService.Host()
	if host == nil {
		return false, fmt.Errorf("libp2p host not available")
	}

	// è·å–èŠ‚ç‚¹æ”¯æŒçš„åè®®ï¼ˆå…ˆè·å–ï¼Œç”¨äºåç»­è¿æ¥çŠ¶æ€åˆ¤å®šï¼‰
	peerProtocols, err := host.Peerstore().GetProtocols(peerID)
	if err != nil {
		return false, fmt.Errorf("failed to get protocols for peer %s: %v", peerID, err)
	}

	// æ£€æŸ¥èŠ‚ç‚¹è¿æ¥çŠ¶æ€
	//
	// âœ… ä¿®å¤ç¼ºé™·Kï¼šè¿æ¥çŠ¶æ€æ—¶åºç«æ€
	// - libp2p è¿æ¥çŠ¶æ€åœ¨çŸ­æ—¶é—´å†…å¯èƒ½ä» Connected æŠ–åŠ¨åˆ° CanConnect/NotConnectedï¼›
	// - è‹¥åªå…è®¸ Connectedï¼Œä¼šåœ¨ "connected äº‹ä»¶è§¦å‘å…¥æ¡¶" çš„å»¶è¿Ÿçª—å£å†…è¯¯æ‹’ç»ä¸šåŠ¡èŠ‚ç‚¹ï¼Œå¯¼è‡´ Kæ¡¶é•¿æœŸä¸ºç©ºã€‚
	// - æ”¾å®½ä¸º Connected æˆ– CanConnectï¼šå…è®¸"å¯å»ºç«‹è¿æ¥ä½†å½“å‰æœªè¿æ¥"çš„ peer è¿›å…¥å€™é€‰é›†åˆï¼Œ
	//   åç»­ä¾èµ–å¥åº·æ¢æµ‹/ä¸¤é˜¶æ®µæ¸…ç†æœºåˆ¶æ·˜æ±°é•¿æœŸå¤±è”èŠ‚ç‚¹ã€‚
	//
	// âœ… ä¿®å¤ç¼ºé™·Lï¼šè¿æ¥ç®¡ç†å™¨æ·˜æ±°å¯¼è‡´çš„Kæ¡¶å…¥è¡¨å¤±è´¥
	// - å½“è¿æ¥æ•°è¶…è¿‡ HighWater æ—¶ï¼Œè¿æ¥ç®¡ç†å™¨ä¼šä¸»åŠ¨æ·˜æ±°è¿æ¥ï¼Œå¯¼è‡´ peer ä» Connected å˜ä¸º NotConnected
	// - å¦‚æœ peer æœ‰åè®®ç¼“å­˜ï¼ˆè¯´æ˜ä¹‹å‰æˆåŠŸ Identifyï¼‰ï¼Œå³ä½¿å½“å‰ NotConnected ä¹Ÿå…è®¸å…¥æ¡¶
	// - ç†ç”±ï¼šä¾èµ–åç»­å¥åº·æ¢æµ‹æœºåˆ¶æ·˜æ±°é•¿æœŸå¤±è”èŠ‚ç‚¹ï¼Œé¿å…å› æš‚æ—¶æ–­è¿è€Œé”™å¤±ä¸šåŠ¡èŠ‚ç‚¹
	connectedness := host.Network().Connectedness(peerID)
	if connectedness == libnetwork.Connected || connectedness == libnetwork.CanConnect {
		// å·²è¿æ¥æˆ–å¯è¿æ¥ï¼Œç›´æ¥é€šè¿‡è¿æ¥çŠ¶æ€æ£€æŸ¥
	} else if connectedness == libnetwork.NotConnected && len(peerProtocols) > 0 {
		// NotConnected ä½†æœ‰åè®®ç¼“å­˜ï¼šå…è®¸å…¥æ¡¶ï¼ˆå¯èƒ½æ˜¯è¿æ¥ç®¡ç†å™¨æ·˜æ±°æˆ–ä¸´æ—¶æ–­è¿ï¼‰
		// åç»­å¥åº·æ¢æµ‹ä¼šæ·˜æ±°é•¿æœŸå¤±è”çš„èŠ‚ç‚¹
	} else {
		// å®Œå…¨æ— è¿æ¥ä¸”æ— åè®®ä¿¡æ¯ï¼šæ‹’ç»
		return false, nil
	}

	// âœ… æœ€é«˜ä¼˜å…ˆçº§ï¼šåªè¦å¯¹ç«¯å®£å‘Šè¿‡ä»»æ„ "/weisyn/" åè®®ï¼Œå³è®¤ä¸ºæ˜¯ WES ä¸šåŠ¡èŠ‚ç‚¹ã€‚
	// è¿™æ ·å¯ä»¥é¿å…â€œåè®®æšä¸¾ä¸å…¨/æ–°åè®®æœªåˆ—å…¥ baseCandidatesâ€å¯¼è‡´çš„è¯¯åˆ¤ã€‚
	for _, p := range peerProtocols {
		sp := string(p)
		if strings.Contains(sp, "/weisyn/") {
			return true, nil
		}
	}

	// æ£€æŸ¥æ˜¯å¦æ”¯æŒ WES ä¸šåŠ¡åè®®ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦â€œæœ¬é“¾ä¸šåŠ¡èŠ‚ç‚¹â€ï¼Œå¯è¿›å…¥ K æ¡¶å‚ä¸åŒæ­¥/è·¯ç”±/é€‰ä¸¾ï¼‰ã€‚
	//
	// âš ï¸ å…³é”®ä¿®å¤ï¼š
	// ä¹‹å‰ä»…ç”¨ ProtocolBlockSubmission ä½œä¸ºâ€œWESèŠ‚ç‚¹è¯†åˆ«â€æ¡ä»¶ï¼Œä½†è¯¥åè®®é€šå¸¸åªä¼šåœ¨èšåˆå™¨/ç‰¹å®šå…±è¯†è§’è‰²ä¸Šæ³¨å†Œ
	//ï¼ˆçŸ¿å·¥/æ™®é€š full èŠ‚ç‚¹å¯èƒ½ä¸ä¼šæ³¨å†Œ handlerï¼Œå› æ­¤ Peerstore åè®®åˆ—è¡¨é‡Œä¸åŒ…å«å®ƒï¼‰ï¼Œä¼šå¯¼è‡´ï¼š
	// - åŒä¸€å¥— weisyn èŠ‚ç‚¹äº’è”æˆåŠŸï¼Œä½†è¢«è¯¯åˆ¤ä¸ºâ€œé WES èŠ‚ç‚¹â€ -> ä¸å…¥æ¡¶ -> æ— æ³•åŒæ­¥
	//
	// æ­£ç¡®ç­–ç•¥ï¼šåªè¦å¯¹ç«¯æ”¯æŒä»»ä¸€ weisyn çš„åŸºç¡€/åŒæ­¥åè®®ï¼Œå³å¯è®¤ä¸ºæ˜¯ WES ä¸šåŠ¡èŠ‚ç‚¹ã€‚
	// å…±è¯†ä¾§å¦‚æœéœ€è¦ä¸¥æ ¼åè®®èƒ½åŠ›ï¼ˆå¦‚ block_submissionï¼‰ï¼Œåº”åœ¨å…±è¯†æ¨¡å—å†…å•ç‹¬æ ¡éªŒï¼ˆå·²æœ‰ï¼‰ã€‚
	baseCandidates := []string{
		// åŸºç¡€ç®¡ç†/å‘ç°ï¼ˆåªè¦æ˜¯ weisyn èŠ‚ç‚¹é€šå¸¸éƒ½ä¼šæœ‰ï¼‰
		protocols.ProtocolNodeInfo,
		protocols.ProtocolHeartbeat,

		// åŒæ­¥ç›¸å…³ï¼ˆå¿…é¡»ï¼‰
		protocols.ProtocolBlockSync,
		protocols.ProtocolHeaderSync,
		protocols.ProtocolStateSync,
		protocols.ProtocolKBucketSync,
		protocols.ProtocolRangePaginated,

		// äº¤æ˜“ç›´è¿ï¼ˆå¯é€‰ï¼‰
		protocols.ProtocolTransactionDirect,

		// å…±è¯†æäº¤ï¼ˆå¯é€‰ï¼šèšåˆå™¨/ç‰¹å®šè§’è‰²ï¼‰
		protocols.ProtocolBlockSubmission,
	}

	ns := ""
	if rtm.configProvider != nil {
		func() {
			defer func() { _ = recover() }()
			ns = rtm.configProvider.GetNetworkNamespace()
		}()
	}

	match := func(sp, base string) bool {
		if sp == base {
			return true
		}
		if ns != "" {
			q := protocols.QualifyProtocol(base, ns)
			return sp == q
		}
		return false
	}

	for _, p := range peerProtocols {
		sp := string(p)
		for _, base := range baseCandidates {
			if match(sp, base) {
				return true, nil
			}
		}
	}

	// ä¸æ”¯æŒWESæ ¸å¿ƒåè®®ï¼Œè®¤ä¸ºæ˜¯å¤–éƒ¨èŠ‚ç‚¹
	return false, nil
}

// FindClosestPeersForProtocol è¿”å›â€œè·ç¦» target æœ€è¿‘ä¸”æ”¯æŒæŒ‡å®šåè®®â€çš„å€™é€‰èŠ‚ç‚¹é›†åˆã€‚
//
// è®¾è®¡çº¦æŸï¼š
// - è¯¥æ–¹æ³•å¿…é¡»æ˜¯â€œçº¯æœ¬åœ°å¿«è·¯å¾„â€ï¼Œä¸å¾— DialPeerï¼Œä¸å¾—åšä»»ä½•ç½‘ç»œæ¢æµ‹ï¼›
// - åè®®æ”¯æŒåˆ¤æ–­ä»…åŸºäº peerstore ä¸­å·²ç¼“å­˜çš„åè®®åˆ—è¡¨ï¼ˆIdentify ç»“æœï¼‰ã€‚
//
// ç”¨é€”ï¼š
// - å…±è¯†é€‰ä¸¾/è½¬å‘ï¼šé¿å…åœ¨çƒ­è·¯å¾„è°ƒç”¨ CheckProtocolSupport -> DialPeerï¼›
// - åŒæ­¥ä¸Šæ¸¸é€‰æ‹©ï¼šä¼˜å…ˆé€‰æ‹©ç¡®å®æ”¯æŒåŒæ­¥åè®®çš„ peerã€‚
func (rtm *RoutingTableManager) FindClosestPeersForProtocol(target []byte, count int, requiredProto string) []peer.ID {
	if count <= 0 {
		return nil
	}
	// å…ˆå–æ›´å¤šå€™é€‰ï¼Œå†æŒ‰åè®®è¿‡æ»¤ï¼Œé¿å…è¿‡æ»¤åä¸è¶³
	candidates := rtm.FindClosestPeers(target, count*3)
	if len(candidates) == 0 {
		return nil
	}
	out := make([]peer.ID, 0, count)
	for _, pid := range candidates {
		if pid == "" {
			continue
		}
		ok, err := rtm.peerSupportsProtocolFromPeerstore(pid, requiredProto)
		if err != nil || !ok {
			continue
		}
		out = append(out, pid)
		if len(out) >= count {
			break
		}
	}
	return out
}

// SupportsProtocol è¿”å› peer æ˜¯å¦æ”¯æŒæŒ‡å®šåè®®ï¼ˆçº¯æœ¬åœ°å¿«è·¯å¾„ï¼Œä¸æ‹¨å·ï¼‰ã€‚
func (rtm *RoutingTableManager) SupportsProtocol(peerID peer.ID, protoID string) (bool, error) {
	return rtm.peerSupportsProtocolFromPeerstore(peerID, protoID)
}

// SupportsProtocolWithRefresh æ£€æŸ¥ peer æ˜¯å¦æ”¯æŒåè®®ï¼Œå¦‚æœ peerstore ä¸­æ²¡æœ‰ç¼“å­˜åˆ™å°è¯•åˆ·æ–°
// ğŸ†• 2025-12-19 æ–°å¢ï¼šè§£å†³ peerstore åè®®åˆ—è¡¨æœªåŠæ—¶æ›´æ–°å¯¼è‡´çš„è¯¯åˆ¤
//
// ç­–ç•¥ï¼š
// 1. é¦–å…ˆæ£€æŸ¥ peerstore ç¼“å­˜ï¼ˆå¿«è·¯å¾„ï¼‰
// 2. å¦‚æœç¼“å­˜ä¸ºç©ºä¸” peer å·²è¿æ¥ï¼Œå°è¯•ä» identify æœåŠ¡åˆ·æ–°åè®®åˆ—è¡¨
// 3. å†æ¬¡æ£€æŸ¥åˆ·æ–°åçš„åè®®åˆ—è¡¨
//
// æ³¨æ„ï¼šæ­¤æ–¹æ³•å¯èƒ½è§¦å‘ç½‘ç»œæ“ä½œï¼Œä»…åœ¨å¿…è¦æ—¶ä½¿ç”¨ï¼ˆå¦‚èšåˆå™¨é€‰æ‹©å¤±è´¥åçš„é‡è¯•ï¼‰
func (rtm *RoutingTableManager) SupportsProtocolWithRefresh(ctx context.Context, peerID peer.ID, protoID string) (bool, error) {
	// 1. å¿«è·¯å¾„ï¼šå…ˆæ£€æŸ¥ peerstore ç¼“å­˜
	supported, err := rtm.peerSupportsProtocolFromPeerstore(peerID, protoID)
	if err == nil && supported {
		return true, nil
	}

	// 2. æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°
	if rtm.p2pService == nil || rtm.p2pService.Host() == nil {
		return false, fmt.Errorf("p2p service not available for protocol refresh")
	}

	h := rtm.p2pService.Host()

	// æ£€æŸ¥ peer æ˜¯å¦å·²è¿æ¥
	if h.Network().Connectedness(peerID) != libnetwork.Connected {
		// æœªè¿æ¥ï¼Œæ— æ³•åˆ·æ–°
		if rtm.logger != nil {
			rtm.logger.Debugf("æ— æ³•åˆ·æ–°åè®®åˆ—è¡¨ï¼špeer %s æœªè¿æ¥", peerID.String()[:12])
		}
		return false, nil
	}

	// 3. å°è¯•è§¦å‘ identify åˆ·æ–°ï¼ˆå¦‚æœæœ‰ identify æœåŠ¡ï¼‰
	// æ³¨æ„ï¼šlibp2p çš„ identify æœåŠ¡ä¼šè‡ªåŠ¨åœ¨è¿æ¥æ—¶äº¤æ¢åè®®ä¿¡æ¯
	// è¿™é‡Œæˆ‘ä»¬åªæ˜¯ç­‰å¾…ä¸€å°æ®µæ—¶é—´è®© identify å®Œæˆ
	select {
	case <-ctx.Done():
		return false, ctx.Err()
	case <-time.After(100 * time.Millisecond):
		// ç­‰å¾… identify å¯èƒ½çš„æ›´æ–°
	}

	// 4. å†æ¬¡æ£€æŸ¥åè®®æ”¯æŒ
	supported, err = rtm.peerSupportsProtocolFromPeerstore(peerID, protoID)
	if err != nil {
		return false, err
	}

	if supported {
		if rtm.logger != nil {
			rtm.logger.Debugf("åè®®åˆ—è¡¨åˆ·æ–°åï¼Œpeer %s æ”¯æŒåè®® %s", peerID.String()[:12], protoID)
		}
	}

	return supported, nil
}

// GetPeerProtocols è·å– peer æ”¯æŒçš„æ‰€æœ‰åè®®åˆ—è¡¨ï¼ˆè°ƒè¯•ç”¨ï¼‰
func (rtm *RoutingTableManager) GetPeerProtocols(peerID peer.ID) ([]string, error) {
	if rtm.p2pService == nil || rtm.p2pService.Host() == nil {
		return nil, fmt.Errorf("host not available")
	}

	h := rtm.p2pService.Host()
	ps, err := h.Peerstore().GetProtocols(peerID)
	if err != nil {
		return nil, err
	}

	result := make([]string, len(ps))
	for i, p := range ps {
		result[i] = string(p)
	}
	return result, nil
}

// IsWESNode æ£€æŸ¥ peer æ˜¯å¦æ˜¯ WES èŠ‚ç‚¹
// é€šè¿‡æ£€æŸ¥æ˜¯å¦æ”¯æŒ WES æ ¸å¿ƒåè®®æ¥åˆ¤æ–­
func (rtm *RoutingTableManager) IsWESNode(peerID peer.ID) bool {
	// WES èŠ‚ç‚¹å¿…é¡»æ”¯æŒä»¥ä¸‹æ ¸å¿ƒåè®®ä¹‹ä¸€
	coreProtocols := []string{
		protocols.ProtocolBlockSubmission,
		protocols.ProtocolSyncHelloV2,
		protocols.ProtocolKBucketSync,
	}

	for _, proto := range coreProtocols {
		supported, err := rtm.peerSupportsProtocolFromPeerstore(peerID, proto)
		if err == nil && supported {
			return true
		}
	}
	return false
}

// PeerType èŠ‚ç‚¹ç±»å‹æšä¸¾
type PeerType string

const (
	// PeerTypeWESFull WES å®Œæ•´èŠ‚ç‚¹ï¼ˆæ”¯æŒæ‰€æœ‰æ ¸å¿ƒåè®®ï¼‰
	PeerTypeWESFull PeerType = "wes_full"
	// PeerTypeWESPartial WES éƒ¨åˆ†èŠ‚ç‚¹ï¼ˆæ”¯æŒéƒ¨åˆ†æ ¸å¿ƒåè®®ï¼Œå¯èƒ½ç‰ˆæœ¬ä¸åŒï¼‰
	PeerTypeWESPartial PeerType = "wes_partial"
	// PeerTypeWESIncompatible WES èŠ‚ç‚¹ä½†ç‰ˆæœ¬ä¸å…¼å®¹
	PeerTypeWESIncompatible PeerType = "wes_incompatible"
	// PeerTypeExternalLibp2p å¤–éƒ¨ libp2p èŠ‚ç‚¹ï¼ˆé WESï¼‰
	PeerTypeExternalLibp2p PeerType = "external_libp2p"
	// PeerTypeUnknown æœªçŸ¥ç±»å‹ï¼ˆæ— æ³•ç¡®å®šï¼‰
	PeerTypeUnknown PeerType = "unknown"
)

// PeerCompatibilityInfo èŠ‚ç‚¹å…¼å®¹æ€§ä¿¡æ¯
type PeerCompatibilityInfo struct {
	PeerID             peer.ID
	Type               PeerType
	SupportedProtocols []string          // æ”¯æŒçš„åè®®åˆ—è¡¨
	MissingProtocols   []string          // ç¼ºå¤±çš„æ ¸å¿ƒåè®®
	VersionMismatch    map[string]string // ç‰ˆæœ¬ä¸åŒ¹é…çš„åè®®: æœŸæœ›ç‰ˆæœ¬ -> å®é™…ç‰ˆæœ¬
	IncompatibleReason string            // ä¸å…¼å®¹åŸå› 
	IsCompatible       bool              // æ˜¯å¦å…¼å®¹
}

// AnalyzePeerCompatibility åˆ†æèŠ‚ç‚¹å…¼å®¹æ€§
// è¿”å›è¯¦ç»†çš„èŠ‚ç‚¹ç±»å‹è¯†åˆ«å’Œå…¼å®¹æ€§ä¿¡æ¯
func (rtm *RoutingTableManager) AnalyzePeerCompatibility(peerID peer.ID) *PeerCompatibilityInfo {
	info := &PeerCompatibilityInfo{
		PeerID:          peerID,
		Type:            PeerTypeUnknown,
		VersionMismatch: make(map[string]string),
		IsCompatible:    false,
	}

	// 1. è·å– peer æ”¯æŒçš„æ‰€æœ‰åè®®
	peerProtocols, err := rtm.GetPeerProtocols(peerID)
	if err != nil {
		info.IncompatibleReason = fmt.Sprintf("æ— æ³•è·å–åè®®åˆ—è¡¨: %v", err)
		return info
	}
	info.SupportedProtocols = peerProtocols

	if len(peerProtocols) == 0 {
		info.Type = PeerTypeUnknown
		info.IncompatibleReason = "åè®®åˆ—è¡¨ä¸ºç©ºï¼ˆå¯èƒ½ identify æœªå®Œæˆï¼‰"
		return info
	}

	// 2. æ£€æŸ¥ WES æ ¸å¿ƒåè®®æ”¯æŒæƒ…å†µ
	coreProtocols := []string{
		protocols.ProtocolBlockSubmission,
		protocols.ProtocolSyncHelloV2,
		protocols.ProtocolKBucketSync,
	}

	supportedCoreCount := 0
	for _, coreProto := range coreProtocols {
		supported, _ := rtm.peerSupportsProtocolFromPeerstore(peerID, coreProto)
		if supported {
			supportedCoreCount++
		} else {
			info.MissingProtocols = append(info.MissingProtocols, coreProto)

			// æ£€æŸ¥æ˜¯å¦æœ‰ç‰ˆæœ¬ä¸åŒ¹é…çš„æƒ…å†µ
			for _, peerProto := range peerProtocols {
				basePath := protocols.ExtractProtocolBasePath(coreProto)
				peerBasePath := protocols.ExtractProtocolBasePath(peerProto)
				if basePath != "" && basePath == peerBasePath {
					// åŒä¸€åè®®ä½†ç‰ˆæœ¬ä¸åŒ
					expectedVersion := protocols.GetProtocolVersion(coreProto)
					actualVersion := protocols.GetProtocolVersion(peerProto)
					info.VersionMismatch[coreProto] = fmt.Sprintf("æœŸæœ› %s, å®é™… %s", expectedVersion, actualVersion)
				}
			}
		}
	}

	// 3. åˆ¤æ–­èŠ‚ç‚¹ç±»å‹
	switch {
	case supportedCoreCount == len(coreProtocols):
		info.Type = PeerTypeWESFull
		info.IsCompatible = true
	case supportedCoreCount > 0:
		if len(info.VersionMismatch) > 0 {
			info.Type = PeerTypeWESIncompatible
			info.IncompatibleReason = fmt.Sprintf("åè®®ç‰ˆæœ¬ä¸åŒ¹é…: %v", info.VersionMismatch)
		} else {
			info.Type = PeerTypeWESPartial
			info.IsCompatible = true // éƒ¨åˆ†å…¼å®¹ä¹Ÿç®—å…¼å®¹
		}
	default:
		// æ£€æŸ¥æ˜¯å¦æ˜¯ libp2p èŠ‚ç‚¹ï¼ˆé€šè¿‡æŸ¥æ‰¾å¸¸è§ libp2p åè®®ï¼‰
		libp2pProtocols := []string{"/ipfs/", "/libp2p/", "/meshsub/", "/floodsub/"}
		isLibp2p := false
		for _, peerProto := range peerProtocols {
			for _, libp2pPrefix := range libp2pProtocols {
				if len(peerProto) > len(libp2pPrefix) && peerProto[:len(libp2pPrefix)] == libp2pPrefix {
					isLibp2p = true
					break
				}
			}
			if isLibp2p {
				break
			}
		}
		if isLibp2p {
			info.Type = PeerTypeExternalLibp2p
			info.IncompatibleReason = "å¤–éƒ¨ libp2p èŠ‚ç‚¹ï¼Œä¸æ”¯æŒ WES åè®®"
		} else {
			info.Type = PeerTypeUnknown
			info.IncompatibleReason = "æœªçŸ¥èŠ‚ç‚¹ç±»å‹"
		}
	}

	return info
}

// QuarantineWithAnalysis å¸¦åˆ†æçš„éš”ç¦»
// æ ¹æ®èŠ‚ç‚¹ç±»å‹é‡‡å–ä¸åŒçš„éš”ç¦»ç­–ç•¥
func (rtm *RoutingTableManager) QuarantineWithAnalysis(peerID peer.ID, requiredProto string) *PeerCompatibilityInfo {
	// 1. åˆ†æèŠ‚ç‚¹å…¼å®¹æ€§
	info := rtm.AnalyzePeerCompatibility(peerID)

	// 2. æ ¹æ®èŠ‚ç‚¹ç±»å‹å†³å®šéš”ç¦»ç­–ç•¥
	var reason string
	var quarantineDuration time.Duration

	defaultQuarantineDuration := rtm.config.GetQuarantineDuration()

	switch info.Type {
	case PeerTypeExternalLibp2p:
		// å¤–éƒ¨ libp2p èŠ‚ç‚¹ï¼šé•¿æœŸéš”ç¦»ï¼ˆè¿™äº›èŠ‚ç‚¹å‡ ä¹ä¸å¯èƒ½å˜æˆ WES èŠ‚ç‚¹ï¼‰
		quarantineDuration = defaultQuarantineDuration * 2
		reason = fmt.Sprintf("external_libp2p_node:missing_%s", requiredProto)
	case PeerTypeWESIncompatible:
		// WES ç‰ˆæœ¬ä¸å…¼å®¹ï¼šä¸­ç­‰æ—¶é—´éš”ç¦»ï¼ˆå¯èƒ½éœ€è¦å‡çº§ï¼‰
		quarantineDuration = defaultQuarantineDuration
		reason = fmt.Sprintf("wes_version_incompatible:%s", requiredProto)
	case PeerTypeUnknown:
		// æœªçŸ¥ç±»å‹ï¼šçŸ­æœŸéš”ç¦»ï¼ˆå¯èƒ½æ˜¯æš‚æ—¶é—®é¢˜ï¼‰
		quarantineDuration = defaultQuarantineDuration / 2
		if quarantineDuration < time.Minute*5 {
			quarantineDuration = time.Minute * 5
		}
		reason = fmt.Sprintf("unknown_peer_type:missing_%s", requiredProto)
	default:
		// WES èŠ‚ç‚¹ä½†ç¼ºå¤±ç‰¹å®šåè®®ï¼šæ ‡å‡†éš”ç¦»
		quarantineDuration = defaultQuarantineDuration
		reason = fmt.Sprintf("wes_partial:missing_%s", requiredProto)
	}

	// 3. æ‰§è¡Œéš”ç¦»
	rtm.tabLock.RLock()
	defer rtm.tabLock.RUnlock()

	for _, bucket := range rtm.buckets {
		if elem := bucket.find(peerID); elem != nil {
			p := elem.Value.(*PeerInfo)

			p.stateLock.Lock()
			p.peerState = PeerStateQuarantined
			p.healthScore = 0
			p.quarantinedUntil = time.Now().Add(quarantineDuration)
			p.stateLock.Unlock()

			if rtm.logger != nil {
				rtm.logger.Infof("ğŸ”’ åˆ†æåéš”ç¦»èŠ‚ç‚¹: peer=%s type=%s reason=%s duration=%s",
					peerID.String()[:12], info.Type, reason, quarantineDuration)
			}
			break
		}
	}

	return info
}

func (rtm *RoutingTableManager) peerSupportsProtocolFromPeerstore(peerID peer.ID, protoID string) (bool, error) {
	if protoID == "" || rtm.p2pService == nil || rtm.p2pService.Host() == nil {
		return false, fmt.Errorf("host/proto not available")
	}
	h := rtm.p2pService.Host()
	ps, err := h.Peerstore().GetProtocols(peerID)
	if err != nil {
		return false, err
	}

	// ğŸ†• 2025-12-19 ä¼˜åŒ–ï¼šä½¿ç”¨åè®®å˜ä½“è¿›è¡Œæ›´å…¨é¢çš„åŒ¹é…
	// æ”¯æŒï¼šåŸå§‹åè®®IDã€å¸¦å‘½åç©ºé—´çš„åè®®IDã€ä¸åŒç‰ˆæœ¬çš„åè®®ID
	ns := ""
	if rtm.configProvider != nil {
		ns = rtm.configProvider.GetNetworkNamespace()
	}

	// è·å–åè®®çš„æ‰€æœ‰å˜ä½“ï¼ˆåŸå§‹ã€å¸¦å‘½åç©ºé—´ã€ä¸åŒç‰ˆæœ¬ï¼‰
	candidates := protocols.GetProtocolVariants(protoID, ns)

	// è½¬æ¢ä¸º map ä»¥ä¾¿å¿«é€ŸæŸ¥æ‰¾
	candidateSet := make(map[string]struct{}, len(candidates))
	for _, c := range candidates {
		candidateSet[c] = struct{}{}
	}

	for _, p := range ps {
		if _, ok := candidateSet[string(p)]; ok {
			return true, nil
		}
	}
	return false, nil
}

// validatePeerChainIdentity éªŒè¯ peer çš„é“¾èº«ä»½æ˜¯å¦ä¸æœ¬åœ°åŒ¹é…ï¼ˆvNextï¼šä¸å‘åå…¼å®¹ï¼‰ã€‚
//
// ä» peer çš„ UserAgent ä¸­è§£æé“¾èº«ä»½ä¿¡æ¯ï¼Œä¸æœ¬åœ°é“¾èº«ä»½æ¯”å¯¹ã€‚
// å¦‚æœä¸åŒ¹é…ï¼Œè¯´æ˜æ˜¯å¤–é“¾èŠ‚ç‚¹ï¼Œä¸åº”åŠ å…¥ K æ¡¶ã€‚
//
// è¿”å›ï¼š
// - chainOKï¼šæ˜¯å¦åŒé“¾
// - reasonï¼šæ‹’ç»åŸå› ï¼ˆç”¨äºè¯Šæ–­ä¸ lastAddï¼‰
// - errï¼šå¯æ¢å¤é”™è¯¯ï¼ˆä¾‹å¦‚ peerstore/identify æœªå°±ç»ªï¼‰ï¼Œéœ€è¦ä¸Šå±‚é‡è¯•
func (rtm *RoutingTableManager) validatePeerChainIdentity(ctx context.Context, peerID peer.ID) (chainOK bool, reason string, err error) {
	if rtm.configProvider == nil || rtm.p2pService == nil {
		// ç¼ºå°‘æœ¬åœ°é“¾èº«ä»½æ¥æº/hostï¼šå±äºç³»ç»Ÿé…ç½®é—®é¢˜ï¼Œä¸åº”â€œæ”¾è¡Œâ€å¯¼è‡´è·¨é“¾æ±¡æŸ“ã€‚
		return false, "chain_identity_unavailable", fmt.Errorf("configProvider or p2pService is nil")
	}

	host := rtm.p2pService.Host()
	if host == nil {
		return false, "chain_identity_unavailable", fmt.Errorf("libp2p host not available")
	}

	// è·å–æœ¬åœ°é“¾èº«ä»½
	appCfg := rtm.configProvider.GetAppConfig()
	if appCfg == nil {
		return false, "chain_identity_unavailable", fmt.Errorf("app config not available")
	}

	unifiedGenesis := rtm.configProvider.GetUnifiedGenesisConfig()
	if unifiedGenesis == nil {
		return false, "chain_identity_unavailable", fmt.Errorf("genesis config not available")
	}

	genesisHash, err := node.CalculateGenesisHash(unifiedGenesis)
	if err != nil {
		return false, "chain_identity_unavailable", fmt.Errorf("calculate genesis hash failed: %w", err)
	}

	localIdentity := node.BuildLocalChainIdentity(appCfg, genesisHash)

	// âœ… ä¼˜å…ˆä½¿ç”¨â€œç³»ç»Ÿè·¯å¾„ç¼“å­˜â€çš„é“¾èº«ä»½ï¼ˆæ¥è‡ª SyncHelloV2 / KBucketSync å“åº”ï¼‰ï¼Œé¿å…ä¾èµ– UserAgent
	if host != nil {
		if v, err := host.Peerstore().Get(peerID, constants.PeerstoreKeyChainIdentity); err == nil {
			if s, ok := v.(string); ok && s != "" {
				var cached types.ChainIdentity
				if uerr := json.Unmarshal([]byte(s), &cached); uerr == nil && cached.IsValid() {
					if localIdentity.IsSameChain(cached) {
						return true, "ok_cached_chain_identity", nil
					}
					return false, "chain_mismatch_cached_identity", nil
				}
			}
		}
	}

	// ç¯å¢ƒï¼šdev/test/prodï¼ˆé»˜è®¤ devï¼‰
	// - dev/testï¼šå…è®¸ä¸€å®šçš„è¿ç§»æœŸå…¼å®¹ï¼ˆé¿å…å› ä¸ºå†å² UserAgent/å¯åŠ¨æ—¶åºå¯¼è‡´â€œæ°¸è¿œä¸å…¥æ¡¶â€ï¼‰
	// - prodï¼šåšæŒ fail-closedï¼ˆUserAgent å¿…é¡»æºå¸¦å®Œæ•´é“¾èº«ä»½ï¼‰ï¼Œé˜²æ­¢è·¨é“¾æ±¡æŸ“ Kæ¡¶ å½±å“å…±è¯†/åŒæ­¥é€‰è·¯
	env := "dev"
	if appCfg != nil {
		func() {
			defer func() { _ = recover() }()
			if e := strings.ToLower(string(appCfg.GetEnvironment())); e != "" {
				env = e
			}
		}()
	}

	// å…¼å®¹ç­–ç•¥ï¼ˆè¿ç§»æœŸï¼‰ï¼š
	// æŸäº›å†å²ç‰ˆæœ¬çš„ weisyn èŠ‚ç‚¹ UserAgent åªåŒ…å«ä»£ç ç‰ˆæœ¬ï¼ˆå¦‚ "github.com/weisyn/v1@xxxx"ï¼‰ï¼Œä¸åŒ…å«é“¾èº«ä»½æ®µã€‚
	// ä½†å®ƒä»¬çš„åè®® ID å¾€å¾€å·²ç»æ˜¯å‘½åç©ºé—´åŒ–çš„ï¼ˆå¦‚ "/weisyn/<ns>/sync/hello/2.0.0"ï¼‰ï¼Œå¯ç”¨äºâ€œåŒ namespaceâ€çº§åˆ«çš„é“¾å½’å±åˆ¤æ–­ã€‚
	//
	// å®‰å…¨è¾¹ç•Œï¼š
	// - ä»…åœ¨ dev/test ç¯å¢ƒå…è®¸è¯¥å…œåº•ï¼ˆprod å¿…é¡»ä¸¥æ ¼é“¾èº«ä»½ï¼‰ï¼›
	// - ä»…å½“æœ¬åœ° ns éç©ºæ—¶æ‰å…è®¸é€šè¿‡â€œå‘½åç©ºé—´åŒ–åè®®â€æ¨æ–­åŒé“¾ï¼›
	// - ä¸€æ—¦ UserAgent ä¸­æºå¸¦äº†å¯è§£æé“¾èº«ä»½ï¼Œåˆ™èµ°ä¸¥æ ¼æ ¡éªŒï¼ˆchain_id / ns / mode / genesisHash8ï¼‰ã€‚
	allowByNamespaceProtocol := func() (bool, error) {
		if env == "prod" {
			return false, nil
		}
		ns := localIdentity.NetworkNamespace
		if ns == "" {
			return false, nil
		}
		ps, err := host.Peerstore().GetProtocols(peerID)
		if err != nil {
			// Identify/peerstore å¯èƒ½å°šæœªå°±ç»ªï¼šäº¤ç»™ä¸Šå±‚é‡è¯•
			return false, fmt.Errorf("get peer protocols failed: %w", err)
		}
		want := "/weisyn/" + ns + "/"
		for _, p := range ps {
			if strings.Contains(string(p), want) {
				return true, nil
			}
		}
		return false, nil
	}

	// å°è¯•ä» UserAgent è§£æ peer çš„é“¾èº«ä»½
	// libp2p çš„ UserAgent å­˜å‚¨åœ¨ peerstore ä¸­
	userAgent, err := host.Peerstore().Get(peerID, "AgentVersion")
	if err != nil {
		// Identify/peerstore å¯èƒ½å°šæœªå°±ç»ªï¼šäº¤ç»™ä¸Šå±‚é‡è¯•
		return false, "chain_identity_not_ready", fmt.Errorf("get AgentVersion failed: %w", err)
	}

	userAgentStr, ok := userAgent.(string)
	if !ok || userAgentStr == "" {
		// vNextï¼šprod å¿…é¡»æ‹’ç»â€œæœªæºå¸¦é“¾èº«ä»½â€çš„èŠ‚ç‚¹ï¼›dev/test å…è®¸ç”¨â€œå‘½åç©ºé—´åŒ–åè®®â€æ¨æ–­åŒé“¾ï¼ˆä»…åŒ nsï¼‰ã€‚
		okByNS, nsErr := allowByNamespaceProtocol()
		if nsErr != nil {
			return false, "chain_identity_not_ready", nsErr
		}
		if okByNS {
			return true, "ok_by_ns_proto", nil
		}
		return false, "chain_identity_missing", nil
	}

	// è§£æ UserAgent ä¸­çš„é“¾èº«ä»½ï¼ˆä¸¥æ ¼æ ¼å¼æ‰æ ¡éªŒï¼Œä¸ä¸¥æ ¼åˆ™å‘åå…¼å®¹æ”¾è¡Œï¼‰
	//
	// æœŸæœ›æ ¼å¼ï¼ˆç”± p2p.Options.UserAgent ç”Ÿæˆï¼‰ï¼š
	//   <version>/<ns>/<mode>/<chainID>@<genesisHash8>
	// ç¤ºä¾‹ï¼š
	//   "github.com/weisyn/v1@98ef22e/public-testnet-demo/public/12001@fc536d38"
	//
	// å¸¸è§â€œéä¸¥æ ¼â€æ ¼å¼ï¼ˆå†å²ç‰ˆæœ¬/å¤–éƒ¨èŠ‚ç‚¹ï¼‰ï¼š
	//   "github.com/weisyn/v1@98ef22e"   â€”â€” ä»…åŒ…å«ä»£ç ç‰ˆæœ¬ï¼Œä¸åŒ…å«é“¾èº«ä»½
	//
	// âš ï¸ å…³é”®ä¿®å¤ï¼š
	// æ—§é€»è¾‘ä¼šæŠŠä¸Šè¿°éä¸¥æ ¼æ ¼å¼ä¸­çš„ "@98ef22e" è¯¯å½“ä½œ genesis hash8ï¼Œä»è€Œé”™è¯¯æ‹’ç»åŒé“¾èŠ‚ç‚¹ï¼ˆchain_ok=falseï¼‰ã€‚
	parts := strings.Split(userAgentStr, "/")
	if len(parts) < 2 {
		okByNS, nsErr := allowByNamespaceProtocol()
		if nsErr != nil {
			return false, "chain_identity_not_ready", nsErr
		}
		if okByNS {
			return true, "ok_by_ns_proto", nil
		}
		return false, "chain_identity_missing", nil
	}

	// é“¾èº«ä»½é€šå¸¸å‡ºç°åœ¨æœ«å°¾ä¸‰æ®µï¼šns / mode / (chainID@hash8)
	if len(parts) < 4 {
		// vNextï¼šæ²¡æœ‰è¶³å¤Ÿæ®µæ•°æ‰¿è½½é“¾èº«ä»½ï¼Œè§†ä¸ºâ€œæœªæºå¸¦é“¾èº«ä»½â€ï¼Œæ‹’ç»
		okByNS, nsErr := allowByNamespaceProtocol()
		if nsErr != nil {
			return false, "chain_identity_not_ready", nsErr
		}
		if okByNS {
			return true, "ok_by_ns_proto", nil
		}
		return false, "chain_identity_missing", nil
	}

	identityStr := parts[len(parts)-1] // "12001@fc536d38" or "v1@98ef22e"
	identityParts := strings.Split(identityStr, "@")
	if len(identityParts) != 2 {
		okByNS, nsErr := allowByNamespaceProtocol()
		if nsErr != nil {
			return false, "chain_identity_not_ready", nsErr
		}
		if okByNS {
			return true, "ok_by_ns_proto", nil
		}
		return false, "chain_identity_missing", nil
	}

	remoteChainID := identityParts[0]
	remoteHash8 := identityParts[1]

	// vNextï¼šå¿…é¡»æºå¸¦å¯è§£æçš„ chain_idï¼ˆæ•°å­—ä¸²ï¼‰ï¼Œå¦åˆ™æ‹’ç»
	isDigits := func(s string) bool {
		if s == "" {
			return false
		}
		for i := 0; i < len(s); i++ {
			if s[i] < '0' || s[i] > '9' {
				return false
			}
		}
		return true
	}
	if !isDigits(remoteChainID) {
		okByNS, nsErr := allowByNamespaceProtocol()
		if nsErr != nil {
			return false, "chain_identity_not_ready", nsErr
		}
		if okByNS {
			return true, "ok_by_ns_proto", nil
		}
		return false, "chain_identity_missing", nil
	}

	// ä¸¥æ ¼æ ¡éªŒï¼šchain_id å¿…é¡»ä¸€è‡´
	if remoteChainID != localIdentity.ChainID {
		rtm.logger.Debugf("policy.reject_sync_peer: é“¾èº«ä»½ä¸åŒ¹é… (chain_id), peer=%s remote_chain_id=%s local_chain_id=%s",
			peerID.String()[:8], remoteChainID, localIdentity.ChainID)
		return false, "chain_mismatch_chain_id", nil
	}

	// ä¸¥æ ¼æ ¡éªŒï¼šnamespace/modeï¼ˆä»æœ«å°¾å€’æ•°ç¬¬3/ç¬¬2æ®µå–ï¼‰
	remoteNamespace := parts[len(parts)-3]
	if remoteNamespace != "" && remoteNamespace != localIdentity.NetworkNamespace {
		rtm.logger.Debugf("policy.reject_sync_peer: é“¾èº«ä»½ä¸åŒ¹é… (namespace), peer=%s remote_ns=%s local_ns=%s",
			peerID.String()[:8], remoteNamespace, localIdentity.NetworkNamespace)
		return false, "chain_mismatch_namespace", nil
	}

	remoteMode := parts[len(parts)-2]
	localMode := string(localIdentity.ChainMode)
	if remoteMode != "" && localMode != "" && remoteMode != localMode {
		rtm.logger.Debugf("policy.reject_sync_peer: é“¾èº«ä»½ä¸åŒ¹é… (mode), peer=%s remote_mode=%s local_mode=%s",
			peerID.String()[:8], remoteMode, localMode)
		return false, "chain_mismatch_mode", nil
	}

	// ä¸¥æ ¼æ ¡éªŒï¼šgenesis hash å‰8ä½
	if len(remoteHash8) >= 8 && len(localIdentity.GenesisHash) >= 8 {
		if remoteHash8[:8] != localIdentity.GenesisHash[:8] {
			rtm.logger.Debugf("policy.reject_sync_peer: é“¾èº«ä»½ä¸åŒ¹é… (genesis hash å‰8ä½), peer=%s remote_hash8=%s local_hash8=%s",
				peerID.String()[:8], remoteHash8[:8], localIdentity.GenesisHash[:8])
			return false, "chain_mismatch_genesis", nil
		}
	}

	return true, "ok", nil
}

// ============================================================================
// Phase 2ï¼šæ¸…ç†å‰æ¢æµ‹æœºåˆ¶
// ============================================================================

// probeWorker æ¢æµ‹å·¥ä½œåç¨‹ï¼Œå®šæœŸæ‰«æå¾…æ¢æµ‹peerå¹¶æ‰§è¡Œä¸»åŠ¨è¿æ¥éªŒè¯
func (rtm *RoutingTableManager) probeWorker() {
	ticker := time.NewTicker(10 * time.Second) // æ¯10ç§’æ‰«æä¸€æ¬¡
	defer ticker.Stop()

	rtm.logger.Info("å¯åŠ¨æ¢æµ‹å·¥ä½œåç¨‹")

	for {
		select {
		case <-rtm.ctx.Done():
			rtm.logger.Info("æ¢æµ‹å·¥ä½œåç¨‹æ”¶åˆ°åœæ­¢ä¿¡å·")
			return
		case <-ticker.C:
			rtm.executePendingProbes()
		}
	}
}

// executePendingProbes æ‰§è¡Œå¾…æ¢æµ‹peerçš„æ‰«æå’Œæ¢æµ‹
func (rtm *RoutingTableManager) executePendingProbes() {
	if !rtm.IsRunning() {
		return
	}

	rtm.tabLock.RLock()
	defer rtm.tabLock.RUnlock()

	now := time.Now()
	probeIntervalMin := 30 * time.Second // æœ€å°æ¢æµ‹é—´éš”

	// æ”¶é›†éœ€è¦æ¢æµ‹çš„peers
	var pendingProbes []*PeerInfo

	for _, bucket := range rtm.buckets {
		for e := bucket.list.Front(); e != nil; e = e.Next() {
			p := e.Value.(*PeerInfo)

			p.stateLock.RLock()
			status := p.probeStatus
			lastProbe := p.lastProbeAt
			p.stateLock.RUnlock()

			if status != ProbePending {
				continue
			}

			// é™åˆ¶æ¢æµ‹é¢‘ç‡ï¼ˆè‡³å°‘é—´éš”30ç§’ï¼‰
			if !lastProbe.IsZero() && now.Sub(lastProbe) < probeIntervalMin {
				continue
			}

			pendingProbes = append(pendingProbes, p)
		}
	}

	if len(pendingProbes) == 0 {
		return
	}

	rtm.logger.Debugf("å‘ç°%dä¸ªå¾…æ¢æµ‹peer", len(pendingProbes))

	// å¹¶å‘æ¢æµ‹ï¼ˆå¼‚æ­¥æ‰§è¡Œï¼Œä¸é˜»å¡ï¼‰
	for _, p := range pendingProbes {
		go rtm.probePeer(p)
	}
}

// probePeer å¯¹å•ä¸ªpeeræ‰§è¡Œä¸»åŠ¨è¿æ¥æ¢æµ‹
func (rtm *RoutingTableManager) probePeer(p *PeerInfo) {
	// å¹¶å‘æ§åˆ¶ï¼šè·å–ä¿¡å·é‡
	select {
	case rtm.probeSemaphore <- struct{}{}:
		defer func() { <-rtm.probeSemaphore }()
	case <-rtm.ctx.Done():
		return
	}

	// æ›´æ–°æœ€åæ¢æµ‹æ—¶é—´
	p.stateLock.Lock()
	p.lastProbeAt = time.Now()
	p.stateLock.Unlock()

	// è·å–peerstoreä¸­çš„åœ°å€
	if rtm.p2pService == nil || rtm.p2pService.Host() == nil {
		rtm.logger.Debugf("æ¢æµ‹å¤±è´¥ï¼šhostæœªå°±ç»ª, peer=%s", p.Id)
		rtm.recordProbeFailure(p)
		return
	}

	host := rtm.p2pService.Host()
	addrs := host.Peerstore().Addrs(p.Id)
	if len(addrs) == 0 {
		rtm.logger.Debugf("æ¢æµ‹å¤±è´¥ï¼šæ— åœ°å€ä¿¡æ¯, peer=%s", p.Id)
		rtm.recordProbeFailure(p)
		return
	}

	// åˆ›å»ºå¸¦è¶…æ—¶çš„ä¸Šä¸‹æ–‡ï¼ˆ5ç§’è¶…æ—¶ï¼‰
	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	defer cancel()

	// å°è¯•é‡æ–°è¿æ¥
	addrInfo := peer.AddrInfo{
		ID:    p.Id,
		Addrs: addrs,
	}

	err := host.Connect(ctx, addrInfo)
	if err == nil {
		// ğŸ¯ è¿æ¥æˆåŠŸï¼å–æ¶ˆæ¸…ç†ï¼Œæ¢å¤ActiveçŠ¶æ€
		rtm.recordProbeSuccess(p)
		rtm.logger.Infof("âœ… æ¢æµ‹æˆåŠŸï¼Œpeeræ¢å¤: peer=%s", p.Id)

		// è®°å½•æŒ‡æ ‡
		if rtm.metrics != nil {
			rtm.metrics.ProbeSuccessCount++
			rtm.metrics.ProbePreventedCleanup++ // é˜²æ­¢äº†ä¸€æ¬¡æ¸…ç†
		}
	} else {
		// âŒ è¿æ¥å¤±è´¥
		rtm.recordProbeFailure(p)
		rtm.logger.Debugf("æ¢æµ‹å¤±è´¥: peer=%s, fail_count=%d, err=%v",
			p.Id, p.probeFailCount, err)

		// è®°å½•æŒ‡æ ‡
		if rtm.metrics != nil {
			rtm.metrics.ProbeFailCount++
		}
	}

	// è®°å½•æ¢æµ‹å°è¯•
	if rtm.metrics != nil {
		rtm.metrics.ProbeAttempts++
	}
}

// recordProbeSuccess è®°å½•æ¢æµ‹æˆåŠŸ
func (rtm *RoutingTableManager) recordProbeSuccess(p *PeerInfo) {
	p.stateLock.Lock()
	defer p.stateLock.Unlock()

	p.probeStatus = ProbeSuccess
	p.probeFailCount = 0

	// æ¢å¤å¥åº·çŠ¶æ€
	p.healthScore = 100
	p.failureCount = 0
	p.LastUsefulAt = time.Now()
	p.LastSuccessfulOutboundQueryAt = time.Now()
	p.peerState = PeerStateActive
	p.quarantinedUntil = time.Time{}
}

// recordProbeFailure è®°å½•æ¢æµ‹å¤±è´¥
func (rtm *RoutingTableManager) recordProbeFailure(p *PeerInfo) {
	p.stateLock.Lock()
	defer p.stateLock.Unlock()

	p.probeFailCount++

	// è¿ç»­3æ¬¡å¤±è´¥æ‰æ ‡è®°ä¸ºProbeFailed
	if p.probeFailCount >= 3 {
		p.probeStatus = ProbeFailed
		rtm.logger.Warnf("æ¢æµ‹è¿ç»­å¤±è´¥3æ¬¡ï¼Œç¡®è®¤æ¸…ç†: peer=%s", p.Id)
	}
}

// finalCleanup æœ€ç»ˆæ¸…ç†ï¼šåªåˆ é™¤æ¢æµ‹ç¡®è®¤å¤±è´¥çš„peer
func (rtm *RoutingTableManager) finalCleanup(bucket *Bucket, bucketIdx int) {
	var toRemove []*list.Element

	for e := bucket.list.Front(); e != nil; e = e.Next() {
		p := e.Value.(*PeerInfo)

		p.stateLock.RLock()
		status := p.probeStatus
		p.stateLock.RUnlock()

		// åªæ¸…ç†æ¢æµ‹ç¡®è®¤å¤±è´¥çš„peer
		if status == ProbeFailed {
			toRemove = append(toRemove, e)
		}
	}

	if len(toRemove) == 0 {
		return
	}

	// æ‰§è¡Œæ¸…ç†
	for _, elem := range toRemove {
		p := elem.Value.(*PeerInfo)

		bucket.remove(elem)
		rtm.peerRemoved(p.Id)

		// è®°å½•æŒ‡æ ‡
		if rtm.metrics != nil {
			rtm.metrics.RecordCleanup("probe_failed", false)
		}

		rtm.logger.Infof("æœ€ç»ˆæ¸…ç†æ¢æµ‹å¤±è´¥peer: bucket=%d, peer=%s, fail_count=%d",
			bucketIdx, p.Id, p.probeFailCount)

		// å°è¯•ä»æ›¿æ¢ç¼“å­˜æå‡èŠ‚ç‚¹
		if replacement := bucket.promoteFromReplacementCache(); replacement != nil {
			bucket.pushFront(replacement)
			rtm.logger.Infof("ä»æ›¿æ¢ç¼“å­˜æå‡èŠ‚ç‚¹: bucket=%d, peer=%s", bucketIdx, replacement.Id)
		}
	}
}
