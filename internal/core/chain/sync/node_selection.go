package sync

import (
	"context"
	"fmt"
	"math/rand"
	"sort"
	"strings"
	"sync"
	"time"

	libnetwork "github.com/libp2p/go-libp2p/core/network"
	peer "github.com/libp2p/go-libp2p/core/peer"

	kbucketimpl "github.com/weisyn/v1/internal/core/infrastructure/kademlia"
	"github.com/weisyn/v1/pkg/constants/protocols"
	"github.com/weisyn/v1/pkg/interfaces/config"
	"github.com/weisyn/v1/pkg/interfaces/infrastructure/kademlia"
	"github.com/weisyn/v1/pkg/interfaces/infrastructure/log"
	p2pi "github.com/weisyn/v1/pkg/interfaces/p2p"
	"github.com/weisyn/v1/pkg/types"
)

func shufflePeersInPlace(peers []peer.ID) {
	if len(peers) <= 1 {
		return
	}
	r := rand.New(rand.NewSource(time.Now().UnixNano()))
	for i := len(peers) - 1; i > 0; i-- {
		j := r.Intn(i + 1)
		peers[i], peers[j] = peers[j], peers[i]
	}
}

// peerHintContextKey ç”¨äºåœ¨ä¸Šä¸‹æ–‡ä¸­å­˜å‚¨åŒæ­¥èŠ‚ç‚¹æç¤ºä¿¡æ¯
type peerHintContextKey struct{}

// syncUrgentContextKey ç”¨äºåœ¨ä¸Šä¸‹æ–‡ä¸­æ ‡è®°â€œç´§æ€¥åŒæ­¥â€è§¦å‘
// - ç´§æ€¥åŒæ­¥ç”¨äºâ€œç¼ºå—è¡¥é½/åˆ†å‰å¤„ç†â€ç­‰å¿…é¡»ç«‹å³æ‰§è¡Œçš„åœºæ™¯
// - ç´§æ€¥åŒæ­¥å¿…é¡»ç»•è¿‡ TriggerSync çš„å»æŠ–ä¸ recently-synced è¿‡æ»¤ï¼ˆä»å— singleflight/é”çº¦æŸï¼‰
type syncUrgentContextKey struct{}

// syncReasonContextKey ç”¨äºåœ¨ä¸Šä¸‹æ–‡ä¸­æºå¸¦â€œè§¦å‘åŸå› â€ï¼ˆä¾¿äºå¯è§‚æµ‹æ€§ä¸è¯Šæ–­ï¼‰
type syncReasonContextKey struct{}

// ======================= è·¯ç”±è¡¨ä¸ºç©ºæ—¶çš„æ—¥å¿—èŠ‚æµç­–ç•¥ =======================

// è¯´æ˜ï¼š
// - å†·å¯åŠ¨æˆ–ç½‘ç»œä¸ä½³æ—¶ï¼Œè·¯ç”±è¡¨å¯èƒ½é•¿æ—¶é—´ä¸ºç©º
// - é€‰æ‹©åŒæ­¥èŠ‚ç‚¹ä¼šé¢‘ç¹å¤±è´¥ï¼Œå¦‚æœæ¯æ¬¡éƒ½æ‰“å° warn æ—¥å¿—ï¼Œä¼šå¯¼è‡´åˆ·å±
// - è¿™é‡Œå®ç°ä¸€ä¸ªç®€å•çš„â€œæŒ‡æ•°é€€é¿ + æœ€å¤§é—´éš”â€çš„èŠ‚æµç­–ç•¥ï¼š
//     * åˆå§‹é—´éš”ï¼š5s
//     * æ¯æ¬¡æ‰“å°åï¼Œé—´éš”ç¿»å€ï¼š5s -> 10s -> 20s -> 40s ...
//     * ä¸Šé™ï¼š60s
//     * ä¸€æ—¦æˆåŠŸé€‰åˆ°èŠ‚ç‚¹æˆ–ä½¿ç”¨åˆ° peer hintï¼Œä¼šç«‹å³é‡ç½®ä¸ºåˆå§‹å€¼

var (
	noPeerLogMu          sync.Mutex
	noPeerLastLog        time.Time
	noPeerCurrentBackoff = 5 * time.Second

	noPeerBackoffInitial = 5 * time.Second
	noPeerBackoffMax     = 60 * time.Second
)

// ======================= ä¸Šæ¸¸èŠ‚ç‚¹â€œè®°å¿†â€ç¼“å­˜ï¼ˆæŠ—ç½‘ç»œæŠ–åŠ¨ï¼‰ =======================
//
// èƒŒæ™¯ï¼š
// - åœ¨çœŸå®ç½‘ç»œä¸­ï¼Œè¿æ¥æŠ–åŠ¨ã€Kæ¡¶è½è¡¨æ—¶åºã€é“¾èº«ä»½/åè®®è¿‡æ»¤ç­‰åŸå› ä¼šå¯¼è‡´â€œçŸ­æ—¶é—´å†…é€‰ä¸åˆ°ä¸Šæ¸¸â€ï¼›
// - å¦‚æœæ¯æ¬¡éƒ½é€€åŒ–ä¸º no-opï¼ŒåŒæ­¥ä¼šè¢«é¢‘ç¹æ‰“æ–­ï¼Œéš¾ä»¥è¿½å¹³é«˜åº¦ã€‚
//
// ç›®æ ‡ï¼š
// - ä¸€æ—¦æˆåŠŸé€‰åˆ°ä¸€ä¸ªå¯ç”¨ä¸Šæ¸¸ï¼ˆKæ¡¶æˆ– fallbackï¼‰ï¼Œç¼“å­˜ä¸º lastGoodUpstreamï¼›
// - å½“ Kæ¡¶ä¸´æ—¶ä¸ºç©º/é€‰ä¸åˆ°æ—¶ï¼Œä¼˜å…ˆå¤ç”¨è¯¥ä¸Šæ¸¸ï¼ˆéœ€ä»å¤„äº Connected ä¸”æœªè¢«æ ‡è®° badï¼‰ã€‚
var (
	lastUpstreamMu  sync.RWMutex
	lastUpstream    peer.ID
	lastUpstreamAt  time.Time
	lastUpstreamTTL = 10 * time.Minute

	lastUpstreamFailures               int
	lastUpstreamMaxConsecutiveFailures = 3
)

func applyUpstreamMemoryConfig(configProvider config.Provider) {
	// é»˜è®¤å€¼ï¼šä¸â€œæ—§å®ç°â€ä¿æŒä¸€è‡´
	ttl := 10 * time.Minute
	maxFails := 3

	if configProvider != nil {
		if bc := configProvider.GetBlockchain(); bc != nil {
			if bc.Sync.Advanced.UpstreamMemoryTTLSeconds > 0 {
				ttl = time.Duration(bc.Sync.Advanced.UpstreamMemoryTTLSeconds) * time.Second
			}
			if bc.Sync.Advanced.UpstreamMaxConsecutiveFailures > 0 {
				maxFails = bc.Sync.Advanced.UpstreamMaxConsecutiveFailures
			}
		}
	}

	lastUpstreamMu.Lock()
	lastUpstreamTTL = ttl
	lastUpstreamMaxConsecutiveFailures = maxFails
	lastUpstreamMu.Unlock()
}

func setLastGoodUpstream(pid peer.ID) {
	if pid == "" {
		return
	}
	lastUpstreamMu.Lock()
	lastUpstream = pid
	lastUpstreamAt = time.Now()
	lastUpstreamFailures = 0
	lastUpstreamMu.Unlock()
}

func clearLastGoodUpstreamLocked() peer.ID {
	// caller must hold lastUpstreamMu (write)
	old := lastUpstream
	lastUpstream = ""
	lastUpstreamAt = time.Time{}
	lastUpstreamFailures = 0
	return old
}

func recordUpstreamSuccess(pid peer.ID) {
	if pid == "" {
		return
	}
	// æˆåŠŸæ„å‘³ç€è¯¥ peer å¯ç”¨ï¼šæ›´æ–° lastUpstream å¹¶æ¸…é›¶å¤±è´¥è®¡æ•°
	setLastGoodUpstream(pid)
}

func recordUpstreamFailure(pid peer.ID, logger log.Logger) {
	if pid == "" {
		return
	}
	lastUpstreamMu.Lock()
	defer lastUpstreamMu.Unlock()

	if lastUpstream == "" || pid != lastUpstream {
		return
	}

	lastUpstreamFailures++
	if lastUpstreamMaxConsecutiveFailures <= 0 {
		return
	}
	if lastUpstreamFailures < lastUpstreamMaxConsecutiveFailures {
		return
	}

	cleared := clearLastGoodUpstreamLocked()
	if logger != nil && cleared != "" {
		logger.Warnf("ğŸ§¹ bad_upstream_fast_switch: è¿ç»­å¤±è´¥è¾¾åˆ°é˜ˆå€¼ï¼Œæ¸…é™¤lastGoodUpstreamå¹¶åˆ‡æ¢ä¸Šæ¸¸: peer=%s failures=%d threshold=%d",
			cleared.String(), lastUpstreamFailures, lastUpstreamMaxConsecutiveFailures)
	}
}

func getLastGoodUpstream(localPeerID peer.ID, p2pService p2pi.Service) (peer.ID, bool) {
	lastUpstreamMu.RLock()
	pid := lastUpstream
	ts := lastUpstreamAt
	lastUpstreamMu.RUnlock()

	if pid == "" || pid == localPeerID || IsBadPeer(pid) {
		return "", false
	}
	if !ts.IsZero() && time.Since(ts) > lastUpstreamTTL {
		return "", false
	}
	if p2pService == nil || p2pService.Host() == nil || p2pService.Host().Network() == nil {
		return "", false
	}
	if p2pService.Host().Network().Connectedness(pid) != libnetwork.Connected {
		return "", false
	}
	return pid, true
}

func resetNoPeerBackoff() {
	noPeerLogMu.Lock()
	defer noPeerLogMu.Unlock()

	noPeerLastLog = time.Time{}
	noPeerCurrentBackoff = noPeerBackoffInitial
}

func shouldLogNoPeer(now time.Time) bool {
	noPeerLogMu.Lock()
	defer noPeerLogMu.Unlock()

	if noPeerCurrentBackoff <= 0 {
		noPeerCurrentBackoff = noPeerBackoffInitial
	}

	// ç¬¬ä¸€æ¬¡æˆ–å·²è¶…è¿‡å½“å‰é€€é¿é—´éš”ï¼Œå…è®¸è¾“å‡ºæ—¥å¿—
	if noPeerLastLog.IsZero() || now.Sub(noPeerLastLog) >= noPeerCurrentBackoff {
		noPeerLastLog = now
		// é—´éš”ç¿»å€ï¼Œç›´è‡³è¾¾åˆ°ä¸Šé™
		next := noPeerCurrentBackoff * 2
		if next > noPeerBackoffMax {
			next = noPeerBackoffMax
		}
		noPeerCurrentBackoff = next
		return true
	}
	return false
}

// ContextWithPeerHint å°†æŒ‡å®šçš„ peer ID å†™å…¥ä¸Šä¸‹æ–‡ï¼Œä¾›åŒæ­¥èŠ‚ç‚¹é€‰æ‹©é˜¶æ®µä½¿ç”¨
func ContextWithPeerHint(ctx context.Context, hint peer.ID) context.Context {
	if hint == "" {
		return ctx
	}
	return context.WithValue(ctx, peerHintContextKey{}, hint)
}

// PeerHintFromContext å°è¯•ä»ä¸Šä¸‹æ–‡ä¸­è¯»å– peer æç¤ºä¿¡æ¯ï¼ˆå¯¹å¤–å¯¼å‡ºï¼‰
func PeerHintFromContext(ctx context.Context) (peer.ID, bool) {
	return peerHintFromContext(ctx)
}

// ContextWithUrgentSync æ ‡è®°æœ¬æ¬¡ TriggerSync ä¸ºâ€œç´§æ€¥åŒæ­¥â€ï¼ˆå¯é€‰æºå¸¦ reasonï¼‰
func ContextWithUrgentSync(ctx context.Context, reason string) context.Context {
	if ctx == nil {
		ctx = context.Background()
	}
	ctx = context.WithValue(ctx, syncUrgentContextKey{}, true)
	if strings.TrimSpace(reason) != "" {
		ctx = context.WithValue(ctx, syncReasonContextKey{}, strings.TrimSpace(reason))
	}
	return ctx
}

// urgentSyncFromContext è¯»å–â€œç´§æ€¥åŒæ­¥â€æ ‡è®°ä¸åŸå› 
func urgentSyncFromContext(ctx context.Context) (urgent bool, reason string) {
	if ctx == nil {
		return false, ""
	}
	if v := ctx.Value(syncUrgentContextKey{}); v != nil {
		if b, ok := v.(bool); ok && b {
			urgent = true
		}
	}
	if v := ctx.Value(syncReasonContextKey{}); v != nil {
		if s, ok := v.(string); ok {
			reason = strings.TrimSpace(s)
		}
	}
	return urgent, reason
}

// peerHintFromContext å°è¯•ä»ä¸Šä¸‹æ–‡ä¸­è¯»å– peer æç¤ºä¿¡æ¯
func peerHintFromContext(ctx context.Context) (peer.ID, bool) {
	if ctx == nil {
		return "", false
	}

	val := ctx.Value(peerHintContextKey{})
	if val == nil {
		return "", false
	}

	switch v := val.(type) {
	case peer.ID:
		if v == "" {
			return "", false
		}
		return v, true
	case string:
		if v == "" {
			return "", false
		}
		return peer.ID(v), true
	default:
		return "", false
	}
}

// selectKBucketPeersForSync åŸºäºKæ¡¶ç®—æ³•é€‰æ‹©åŒæ­¥èŠ‚ç‚¹ï¼Œå¿…è¦æ—¶ä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„ peer æç¤ºå…œåº•
func selectKBucketPeersForSync(
	ctx context.Context,
	routingManager kademlia.RoutingTableManager,
	p2pService p2pi.Service,
	configProvider config.Provider,
	chainInfo *types.ChainInfo,
	logger log.Logger,
) ([]peer.ID, error) {
	if logger != nil {
		logger.Debug("ğŸ” å¼€å§‹Kæ¡¶èŠ‚ç‚¹é€‰æ‹©")
	}

	// æ¯æ¬¡é€‰æ‹©å‰è¯»å–ä¸€æ¬¡é…ç½®ï¼ˆæ”¯æŒè¿è¡Œæ—¶è°ƒæ•´ï¼‰
	applyUpstreamMemoryConfig(configProvider)

	// ğŸ”’ é˜²å¾¡å¼ç¼–ç¨‹ï¼šRoutingTableManager åœ¨ fx ä¸­æ ‡è®°ä¸º optional
	// åœ¨æŸäº›å•èŠ‚ç‚¹/æµ‹è¯•åœºæ™¯ä¸‹å¯èƒ½æœªæ³¨å…¥ï¼Œæ­¤æ—¶ç›´æ¥è®¿é—®ä¼šå¯¼è‡´ panicã€‚
	localPeerID := peer.ID("")
	if p2pService != nil && p2pService.Host() != nil {
		localPeerID = p2pService.Host().ID()
	}

	if routingManager == nil {
		if logger != nil {
			logger.Warn("âš ï¸ RoutingTableManager æœªæ³¨å…¥ï¼Œå°è¯•ä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„ peer hint ä½œä¸ºåŒæ­¥ç›®æ ‡")
		}
		return buildPeersFromHint(ctx, localPeerID, p2pService, configProvider, logger)
	}

	if localPeerID == "" {
		// å•æµ‹/æ—©æœŸå¯åŠ¨åœºæ™¯ï¼šHost å¯èƒ½è¿˜æ²¡å‡†å¤‡å¥½ï¼Œä¼˜å…ˆå°è¯• peer hint
		if logger != nil {
			logger.Warn("âš ï¸ P2P Host æœªå°±ç»ªï¼Œæ— æ³•è·å–æœ¬åœ°èŠ‚ç‚¹IDï¼Œå°è¯•ä½¿ç”¨ peer hint ä½œä¸ºåŒæ­¥ç›®æ ‡")
		}
		return buildPeersFromHint(ctx, localPeerID, p2pService, configProvider, logger)
	}

	routingTable := routingManager.GetRoutingTable()
	if routingTable == nil {
		if logger != nil {
			logger.Warnf("âš ï¸ routingManager.GetRoutingTable() è¿”å› nilï¼šroutingManager=%T localPeerID=%sï¼Œå°†å›é€€åˆ° peer hint/lastGoodUpstream/connected-peers",
				routingManager, localPeerID.String())
		}
		return buildPeersFromHint(ctx, localPeerID, p2pService, configProvider, logger)
	}

	target := []byte(localPeerID)
	// æŒ‰é…ç½®å†³å®šâ€œæœ€ç»ˆè¿”å›çš„å€™é€‰æ•°â€ï¼ŒåŒæ—¶ä»è·¯ç”±è¡¨æ‹‰ä¸€ä¸ªæ›´å¤§çš„å€™é€‰æ± ä»¥æ”¯æŒ random/mixed ç­–ç•¥ã€‚
	selectionCount := 8
	strategy := "mixed"
	if configProvider != nil {
		if bc := configProvider.GetBlockchain(); bc != nil {
			if bc.Sync.Advanced.KBucketSelectionCount > 0 {
				selectionCount = bc.Sync.Advanced.KBucketSelectionCount
			}
			if s := strings.ToLower(strings.TrimSpace(bc.Sync.Advanced.KBucketSelectionStrategy)); s != "" {
				strategy = s
			}
		}
	}
	if selectionCount <= 0 {
		selectionCount = 8
	}
	if selectionCount > 32 {
		selectionCount = 32
	}

	// å€™é€‰æ± å¤§å°ï¼šè¶Šå¤§è¶Šåˆ©äºéšæœºæ€§ï¼Œä½†ä¹Ÿè¦æ§åˆ¶å¼€é”€ã€‚
	candidatePool := selectionCount * 4
	if candidatePool < 16 {
		candidatePool = 16
	}
	if candidatePool > 64 {
		candidatePool = 64
	}

	// âœ… vNextï¼šä¼˜å…ˆé€‰æ‹©â€œæ˜ç¡®æ”¯æŒ SyncHelloV2â€çš„ peerï¼Œé¿å…åç»­ hello å¤±è´¥é€ æˆæŠ–åŠ¨ã€‚
	// è¯¥è¿‡æ»¤å¿…é¡»æ˜¯çº¯æœ¬åœ°å¿«è·¯å¾„ï¼ˆåªè¯» peerstore åè®®ç¼“å­˜ï¼‰ï¼Œä¸å¾—è§¦å‘ DialPeerã€‚
	var selectedPeers []peer.ID
	if rm, ok := routingManager.(*kbucketimpl.RoutingTableManager); ok {
		selectedPeers = rm.FindClosestPeersForProtocol(target, candidatePool, protocols.ProtocolSyncHelloV2)
	} else {
		selectedPeers = routingManager.FindClosestPeers(target, candidatePool)
	}
	if len(selectedPeers) == 0 {
		return buildPeersFromHint(ctx, localPeerID, p2pService, configProvider, logger)
	}

	var filteredPeers []peer.ID
	for _, pid := range selectedPeers {
		if pid != localPeerID && !IsBadPeer(pid) {
			// ğŸ”¥ è¿‡æ»¤æ‰ä¸å¥åº·çš„èŠ‚ç‚¹ï¼ˆç†”æ–­ä¸­ï¼‰
			if !IsHealthy(pid) {
				if logger != nil {
					logger.Warnf("âš ï¸ èŠ‚ç‚¹å·²ç†”æ–­ï¼Œè·³è¿‡: %s", pid.String()[:12]+"...")
				}
				continue
			}
			filteredPeers = append(filteredPeers, pid)
		}
	}

	if len(filteredPeers) == 0 {
		if logger != nil {
			logger.Warn("âš ï¸ è¿‡æ»¤åæ²¡æœ‰å¯ç”¨èŠ‚ç‚¹ï¼ˆå·²æ’é™¤ bad peers å’Œç†”æ–­èŠ‚ç‚¹ï¼‰")
		}
		return buildPeersFromHint(ctx, localPeerID, p2pService, configProvider, logger)
	}

	// æ ¹æ®ç­–ç•¥ç”Ÿæˆæœ€ç»ˆå€™é€‰åˆ—è¡¨ï¼š
	// - distance: ä¿æŒè·ç¦»æ’åºï¼ˆç”± FindClosestPeers è¿”å›é¡ºåºå†³å®šï¼‰ï¼Œå–å‰ N
	// - random: åœ¨å€™é€‰æ± ä¸­éšæœºå– N
	// - mixed: å‰åŠå– closestï¼ŒååŠä»å‰©ä½™å€™é€‰éšæœºè¡¥é½
	final := make([]peer.ID, 0, selectionCount)
	switch strategy {
	case "distance":
		if len(filteredPeers) > selectionCount {
			final = append(final, filteredPeers[:selectionCount]...)
		} else {
			final = append(final, filteredPeers...)
		}
	case "random":
		shufflePeersInPlace(filteredPeers)
		if len(filteredPeers) > selectionCount {
			final = append(final, filteredPeers[:selectionCount]...)
		} else {
			final = append(final, filteredPeers...)
		}
	case "mixed":
		fallthrough
	default:
		closestN := selectionCount / 2
		if closestN < 1 {
			closestN = 1
		}
		if closestN > selectionCount {
			closestN = selectionCount
		}
		if len(filteredPeers) < closestN {
			closestN = len(filteredPeers)
		}
		final = append(final, filteredPeers[:closestN]...)

		rest := append([]peer.ID(nil), filteredPeers[closestN:]...)
		shufflePeersInPlace(rest)
		need := selectionCount - len(final)
		if need > 0 {
			if len(rest) > need {
				final = append(final, rest[:need]...)
			} else {
				final = append(final, rest...)
			}
		}
	}

	// é˜²å¾¡ï¼šå»é‡ï¼ˆè™½ç„¶ç†è®ºä¸Š FindClosestPeers ä¸ä¼šè¿”å›é‡å¤ï¼‰
	uniq := make([]peer.ID, 0, len(final))
	seen := make(map[peer.ID]struct{}, len(final))
	for _, pid := range final {
		if _, ok := seen[pid]; ok {
			continue
		}
		seen[pid] = struct{}{}
		uniq = append(uniq, pid)
	}
	final = uniq

	// èƒ½å¤ŸæˆåŠŸé€‰åˆ°å¯ç”¨èŠ‚ç‚¹ï¼Œè¯´æ˜ç½‘ç»œçŠ¶æ€å·²æ¢å¤ï¼Œé‡ç½®é€€é¿ç­–ç•¥
	resetNoPeerBackoff()
	// è®°å½•ä¸€ä¸ªâ€œå¯ç”¨ä¸Šæ¸¸â€ç”¨äºæŠ–åŠ¨æ—¶å¤ç”¨
	if len(final) > 0 {
		recordUpstreamSuccess(final[0])
	}

	if logger != nil {
		logger.Debugf("âœ… Kæ¡¶èŠ‚ç‚¹é€‰æ‹©å®Œæˆ: strategy=%s, å€™é€‰æ± =%d, æœ€ç»ˆ=%d", strategy, len(filteredPeers), len(final))
		for i, pid := range final {
			if i >= 3 {
				break
			}
			logger.Debugf("  èŠ‚ç‚¹[%d]: %s", i+1, pid.String())
		}
	}

	return final, nil
}

// buildPeersFromHint æ ¹æ®ä¸Šä¸‹æ–‡ä¸­çš„ peer æç¤ºæ„é€ åŒæ­¥ç›®æ ‡
func buildPeersFromHint(ctx context.Context, localPeerID peer.ID, p2pService p2pi.Service, configProvider config.Provider, logger log.Logger) ([]peer.ID, error) {
	// å…œåº•é€‰æ‹©ä¹Ÿéœ€è¦ä½¿ç”¨æœ€æ–°çš„ TTL/é˜ˆå€¼
	applyUpstreamMemoryConfig(configProvider)

	if hint, ok := peerHintFromContext(ctx); ok && hint != "" && hint != localPeerID {
		if logger != nil {
			logger.Infof("ğŸª¢ ä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„peer hintä½œä¸ºåŒæ­¥ç›®æ ‡: %s", hint.String())
		}
		// æˆåŠŸä½¿ç”¨ hintï¼Œè§†ä¸ºç½‘ç»œå¯ç”¨ï¼Œé‡ç½®é€€é¿
		resetNoPeerBackoff()
		recordUpstreamSuccess(hint)
		return []peer.ID{hint}, nil
	}

	// === æŠ—æŠ–åŠ¨ï¼šä¼˜å…ˆå¤ç”¨ä¸Šä¸€æ¬¡æˆåŠŸçš„ä¸Šæ¸¸èŠ‚ç‚¹ ===
	if pid, ok := getLastGoodUpstream(localPeerID, p2pService); ok {
		if logger != nil {
			logger.Infof("ğŸ§· Kæ¡¶ä¸ºç©º/ä¸å¯ç”¨ï¼šå¤ç”¨ä¸Šä¸€æ¬¡å¯ç”¨ä¸Šæ¸¸èŠ‚ç‚¹: %s", pid.String())
		}
		resetNoPeerBackoff()
		return []peer.ID{pid}, nil
	}

	// === å…œåº•ç­–ç•¥ï¼šå½“ K æ¡¶ä¸ºç©ºæ—¶ï¼Œä»â€œå·²è¿æ¥ peersâ€é‡ŒæŒ‘é€‰ä¸Šæ¸¸ ===
	//
	// ç›®æ ‡ï¼šæˆ‘ä»¬çš„æ ¸å¿ƒç›®çš„æ˜¯åŒºå—åŒæ­¥ã€‚K æ¡¶ä¸ºç©ºåœ¨å†·å¯åŠ¨/è¿‡æ»¤è¯¯åˆ¤/äº‹ä»¶æ—¶åºç­‰åœºæ™¯ä¼šæŒç»­è¾ƒä¹…ï¼Œ
	// å¦‚æœæ­¤å¤„ç›´æ¥è¿”å›â€œæ— å¯ç”¨èŠ‚ç‚¹â€ï¼ŒåŒæ­¥å°†æ°¸ä¹… no-opï¼Œé“¾é«˜åº¦ä¸ä¼šæ”¶æ•›ã€‚
	//
	// ç­–ç•¥ï¼š
	// - ä»…è€ƒè™‘å·²è¿æ¥çš„ peerï¼ˆConnectedness=Connectedï¼‰ï¼Œé¿å…æ— æ„ä¹‰æ‹¨å·ï¼›
	// - è¿‡æ»¤ bad peers/selfï¼›
	// - ä¼˜å…ˆé€‰æ‹©å£°æ˜äº† WES åŒæ­¥ç›¸å…³åè®®çš„ peerï¼ˆpeerstore protocols ä¸­åŒ…å« /weisyn/.../sync/... æˆ– /weisyn/.../blockchain/...ï¼‰ã€‚
	if peers := selectConnectedPeersForSync(localPeerID, p2pService, configProvider, logger); len(peers) > 0 {
		resetNoPeerBackoff()
		recordUpstreamSuccess(peers[0])
		return peers, nil
	}

	if logger != nil {
		// ä½¿ç”¨æŒ‡æ•°é€€é¿æ§åˆ¶æ—¥å¿—é¢‘ç‡ï¼Œé¿å…å†·å¯åŠ¨/ç½‘ç»œæŠ–åŠ¨æ—¶æœŸçš„åˆ·å±
		now := time.Now()
		if shouldLogNoPeer(now) {
			logger.Warn("âš ï¸ è·¯ç”±è¡¨ä¸­æ²¡æœ‰å¯ç”¨èŠ‚ç‚¹ï¼Œä¸”ä¸Šä¸‹æ–‡æœªæä¾›æœ‰æ•ˆpeer hint")
		} else {
			logger.Debug("âš ï¸ è·¯ç”±è¡¨ä¸­æ²¡æœ‰å¯ç”¨èŠ‚ç‚¹ï¼ˆæ—¥å¿—å·²æŒ‰é€€é¿ç­–ç•¥èŠ‚æµï¼‰")
		}
	}
	return nil, fmt.Errorf("è·¯ç”±è¡¨ä¸­æ²¡æœ‰å¯ç”¨èŠ‚ç‚¹")
}

// selectConnectedPeersForSync ä» libp2p å·²è¿æ¥ peers ä¸­é€‰æ‹©å¯ä½œä¸ºä¸Šæ¸¸çš„å€™é€‰èŠ‚ç‚¹ï¼ˆKæ¡¶ä¸ºç©ºæ—¶çš„å…œåº•ï¼‰ã€‚
//
// è¿”å›å€¼ï¼š
// - è‹¥æ‰¾ä¸åˆ°ä»»ä½•å€™é€‰ï¼Œè¿”å›ç©ºåˆ‡ç‰‡ã€‚
func selectConnectedPeersForSync(localPeerID peer.ID, p2pService p2pi.Service, configProvider config.Provider, logger log.Logger) []peer.ID {
	if localPeerID == "" || p2pService == nil || p2pService.Host() == nil {
		return nil
	}
	host := p2pService.Host()
	net := host.Network()
	if net == nil {
		return nil
	}
	list := net.Peers()
	if len(list) == 0 {
		return nil
	}

	// ä¸ºæ¯ä¸ª peer è®¡ç®—â€œåŒæ­¥å€™é€‰åˆ†æ•°â€ï¼šå¿…é¡»æ”¯æŒ SyncHelloV2 åè®®ï¼ˆè¿ç§»æœŸåŒæ—¶å…¼å®¹ original/qualifiedï¼‰ã€‚
	type scored struct {
		id    peer.ID
		score int
	}
	scoredPeers := make([]scored, 0, len(list))

	ns := ""
	if configProvider != nil {
		ns = configProvider.GetNetworkNamespace()
	}
	wantHello := map[string]struct{}{protocols.ProtocolSyncHelloV2: {}}
	if ns != "" {
		wantHello[protocols.QualifyProtocol(protocols.ProtocolSyncHelloV2, ns)] = struct{}{}
	}

	for _, pid := range list {
		if pid == "" || pid == localPeerID || IsBadPeer(pid) {
			continue
		}
		// å¿…é¡»æ˜¯å·²è¿æ¥
		//ï¼ˆPeers() ç†è®ºä¸Šéƒ½æ˜¯ connectedï¼Œä½†è¿™é‡Œé˜²å¾¡å¼æ ¡éªŒï¼‰
		if net.Connectedness(pid) != libnetwork.Connected {
			continue
		}
		score := 0
		// vNextï¼šä¸¥æ ¼è¦æ±‚æ”¯æŒ SyncHelloV2ï¼›å¦åˆ™è¯¥ peer ä¸å…·å¤‡â€œä½œä¸ºåŒæ­¥ä¸Šæ¸¸â€çš„æœ€ä½èƒ½åŠ›ã€‚
		if ps, err := host.Peerstore().GetProtocols(pid); err == nil && len(ps) > 0 {
			for _, p := range ps {
				if _, ok := wantHello[string(p)]; ok {
					score = 100
					break
				}
			}
		}
		if score > 0 {
			scoredPeers = append(scoredPeers, scored{id: pid, score: score})
		}
	}

	if len(scoredPeers) == 0 {
		return nil
	}

	sort.Slice(scoredPeers, func(i, j int) bool {
		if scoredPeers[i].score != scoredPeers[j].score {
			return scoredPeers[i].score > scoredPeers[j].score
		}
		return scoredPeers[i].id.String() < scoredPeers[j].id.String()
	})

	const maxPeers = 4
	out := make([]peer.ID, 0, maxPeers)
	for _, sp := range scoredPeers {
		out = append(out, sp.id)
		if len(out) >= maxPeers {
			break
		}
	}

	if logger != nil {
		logger.Infof("ğŸ›Ÿ Kæ¡¶ä¸ºç©ºï¼šä½¿ç”¨å·²è¿æ¥ peers ä½œä¸ºåŒæ­¥ä¸Šæ¸¸å€™é€‰: %d", len(out))
		for i, pid := range out {
			if i >= 3 {
				break
			}
			logger.Debugf("  fallback_peer[%d]=%s", i+1, pid.String())
		}
	}
	return out
}

// ======================= ä½é«˜åº¦èŠ‚ç‚¹è®°å½•ï¼ˆSYNC-005/SYNC-101ä¿®å¤ï¼‰ =======================
//
// èƒŒæ™¯ï¼š
// - åœ¨é˜¶æ®µ2çš„ SyncHelloV2 ä¸­ï¼Œå¦‚æœå¯¹ç«¯è¿”å› REMOTE_BEHINDï¼ˆå¯¹ç«¯é«˜åº¦ä½äºæœ¬åœ°ï¼‰ï¼Œ
//   æˆ–è€…è§‚å¯Ÿåˆ°å¯¹ç«¯é«˜åº¦è¿œä½äºæƒå¨ç½‘ç»œé«˜åº¦ï¼Œåˆ™å°†è¯¥èŠ‚ç‚¹æ ‡è®°ä¸º"ä½é«˜åº¦èŠ‚ç‚¹"ã€‚
// - çŸ­æœŸå†…ï¼ˆé»˜è®¤10åˆ†é’Ÿï¼‰ä¸å†é€‰æ‹©è¯¥èŠ‚ç‚¹ä½œä¸ºåŒæ­¥ä¸Šæ¸¸ï¼Œé¿å…é‡å¤ä½æ•ˆåŒæ­¥ã€‚

var (
	lowHeightPeersMu    sync.RWMutex
	lowHeightPeers      = make(map[peer.ID]lowHeightInfo)
	lowHeightPeerTTL    = 10 * time.Minute
)

type lowHeightInfo struct {
	Height     uint64
	RecordedAt time.Time
}

// recordLowHeightPeer è®°å½•ä¸€ä¸ªä½é«˜åº¦èŠ‚ç‚¹
func recordLowHeightPeer(pid peer.ID, height uint64, logger log.Logger) {
	if pid == "" {
		return
	}
	lowHeightPeersMu.Lock()
	lowHeightPeers[pid] = lowHeightInfo{
		Height:     height,
		RecordedAt: time.Now(),
	}
	lowHeightPeersMu.Unlock()
	
	if logger != nil {
		logger.Debugf("ğŸ“ è®°å½•ä½é«˜åº¦èŠ‚ç‚¹: peer=%s height=%d", 
			pid.String()[:12]+"...", height)
	}
}

// isLowHeightPeer æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦ä¸ºä½é«˜åº¦èŠ‚ç‚¹ï¼ˆåœ¨TTLå†…ï¼‰
func isLowHeightPeer(pid peer.ID) bool {
	lowHeightPeersMu.RLock()
	info, exists := lowHeightPeers[pid]
	lowHeightPeersMu.RUnlock()

	if !exists {
		return false
	}

	// æ£€æŸ¥TTLï¼šè¿‡æœŸåˆ™åœ¨å†™é”ä¸‹æ¸…ç†ï¼ˆé¿å…åœ¨RLockä¸‹deleteå¯¼è‡´å¹¶å‘é—®é¢˜ï¼‰
	if time.Since(info.RecordedAt) > lowHeightPeerTTL {
		lowHeightPeersMu.Lock()
		// äºŒæ¬¡ç¡®è®¤ï¼Œé¿å…ç«æ€
		if info2, ok := lowHeightPeers[pid]; ok {
			if time.Since(info2.RecordedAt) > lowHeightPeerTTL {
				delete(lowHeightPeers, pid)
			}
		}
		lowHeightPeersMu.Unlock()
		return false
	}

	return true
}

// clearExpiredLowHeightPeers æ¸…ç†æ‰€æœ‰è¿‡æœŸçš„ä½é«˜åº¦èŠ‚ç‚¹è®°å½•
// ğŸ†• SYNC-HIGH002ä¿®å¤ï¼šåœ¨æ— å€™é€‰èŠ‚ç‚¹æ—¶è°ƒç”¨ï¼Œç»™è¿‡æœŸèŠ‚ç‚¹ç¬¬äºŒæ¬¡æœºä¼š
func clearExpiredLowHeightPeers() {
	lowHeightPeersMu.Lock()
	defer lowHeightPeersMu.Unlock()

	now := time.Now()
	for pid, info := range lowHeightPeers {
		if now.Sub(info.RecordedAt) > lowHeightPeerTTL {
			delete(lowHeightPeers, pid)
		}
	}
}

// getLowHeightPeersStats è·å–ä½é«˜åº¦èŠ‚ç‚¹ç»Ÿè®¡ä¿¡æ¯
func getLowHeightPeersStats() (total int, expired int) {
	lowHeightPeersMu.RLock()
	defer lowHeightPeersMu.RUnlock()

	now := time.Now()
	for _, info := range lowHeightPeers {
		total++
		if now.Sub(info.RecordedAt) > lowHeightPeerTTL {
			expired++
		}
	}
	return
}

// reduceLowHeightPeerTTL ä¸´æ—¶ç¼©çŸ­ä½é«˜åº¦èŠ‚ç‚¹ TTLï¼ˆç´§æ€¥æ¢å¤ï¼‰
// ğŸ†• SYNC-HIGH002ä¿®å¤ï¼šåœ¨æç«¯æƒ…å†µä¸‹åŠ é€ŸèŠ‚ç‚¹æ¢å¤
func reduceLowHeightPeerTTL(factor float64) {
	if factor <= 0 || factor >= 1 {
		return
	}
	lowHeightPeersMu.Lock()
	defer lowHeightPeersMu.Unlock()

	// ä¸´æ—¶ç¼©çŸ­ TTLï¼Œè®©æ›´å¤šèŠ‚ç‚¹æœ‰æœºä¼šè¢«é‡è¯•
	reducedTTL := time.Duration(float64(lowHeightPeerTTL) * factor)
	now := time.Now()

	for pid, info := range lowHeightPeers {
		if now.Sub(info.RecordedAt) > reducedTTL {
			delete(lowHeightPeers, pid)
		}
	}
}

// ======================= å¸¦é™çº§ç­–ç•¥çš„èŠ‚ç‚¹é€‰æ‹©ï¼ˆç†”æ–­+Fallbackï¼‰ =======================

// filterHealthyPeers è¿‡æ»¤å¥åº·çš„èŠ‚ç‚¹ï¼ˆæ’é™¤ç†”æ–­ä¸­çš„èŠ‚ç‚¹ï¼‰
func filterHealthyPeers(peers []peer.ID, logger log.Logger) []peer.ID {
	healthy := make([]peer.ID, 0, len(peers))
	for _, pid := range peers {
		if IsHealthy(pid) {
			healthy = append(healthy, pid)
		} else {
			if logger != nil {
				logger.Debugf("âš ï¸ èŠ‚ç‚¹å·²ç†”æ–­ï¼Œè·³è¿‡: %s", pid.String()[:12]+"...")
			}
		}
	}
	return healthy
}

// selectRandomPeers ä»åˆ—è¡¨ä¸­éšæœºé€‰æ‹©æœ€å¤š n ä¸ªèŠ‚ç‚¹
func selectRandomPeers(peers []peer.ID, n int) []peer.ID {
	if len(peers) <= n {
		return peers
	}
	
	// å¤åˆ¶ä¸€ä»½é¿å…ä¿®æ”¹åŸå§‹åˆ—è¡¨
	copied := make([]peer.ID, len(peers))
	copy(copied, peers)
	
	// éšæœºæ‰“ä¹±
	shufflePeersInPlace(copied)
	
	return copied[:n]
}

// getBootstrapPeers è·å– Bootstrap èŠ‚ç‚¹åˆ—è¡¨ï¼ˆä»é…ç½®ä¸­è¯»å–ï¼‰
func getBootstrapPeers(configProvider config.Provider) []peer.ID {
	if configProvider == nil {
		return nil
	}
	
	nodeConfig := configProvider.GetNode()
	if nodeConfig == nil || len(nodeConfig.Discovery.BootstrapPeers) == 0 {
		return nil
	}
	
	var bootstrapPeers []peer.ID
	for _, addrStr := range nodeConfig.Discovery.BootstrapPeers {
		// âœ… public bootstrapï¼ˆbootstrap.libp2p.io ç­‰ï¼‰ä»…ç”¨äº discovery/è¿é€šæ€§ï¼Œä¸åº”ä½œä¸ºâ€œåŒºå—åŒæ­¥ä¸Šæ¸¸â€
		// ä¿ç•™åœ¨é…ç½®é‡Œï¼Œä½†ä»åŒæ­¥å€™é€‰ä¸­å‰”é™¤ã€‚
		if strings.Contains(addrStr, "bootstrap.libp2p.io") || strings.Contains(addrStr, "ipfs") {
			continue
		}
		// å°è¯•ä» multiaddr ä¸­æå– peer ID
		// æ ¼å¼å¦‚: /ip4/1.2.3.4/tcp/5000/p2p/QmXXX
		parts := strings.Split(addrStr, "/p2p/")
		if len(parts) == 2 {
			if pid, err := peer.Decode(parts[1]); err == nil {
				bootstrapPeers = append(bootstrapPeers, pid)
			}
		}
	}
	
	return bootstrapPeers
}

// selectCandidatePeersWithFallback å¸¦é™çº§ç­–ç•¥çš„èŠ‚ç‚¹é€‰æ‹©
//
// é™çº§ç­–ç•¥ï¼š
//   1. ä¼˜å…ˆä½¿ç”¨ Kæ¡¶èŠ‚ç‚¹ï¼ˆå·²è¿‡æ»¤ç†”æ–­èŠ‚ç‚¹ï¼‰
//   2. å¦‚æœ Kæ¡¶æ— å¯ç”¨èŠ‚ç‚¹ï¼Œé™çº§åˆ° DHT å·²è¿æ¥èŠ‚ç‚¹
//   3. å¦‚æœ DHT èŠ‚ç‚¹ä¹Ÿä¸å¯ç”¨ï¼Œå°è¯• Bootstrap èŠ‚ç‚¹
func selectCandidatePeersWithFallback(
	ctx context.Context,
	routingManager kademlia.RoutingTableManager,
	p2pService p2pi.Service,
	configProvider config.Provider,
	chainInfo *types.ChainInfo,
	logger log.Logger,
) ([]peer.ID, error) {
	if logger != nil {
		logger.Debug("ğŸ” å¼€å§‹å¸¦é™çº§ç­–ç•¥çš„èŠ‚ç‚¹é€‰æ‹©")
	}

	localPeerID := peer.ID("")
	if p2pService != nil && p2pService.Host() != nil {
		localPeerID = p2pService.Host().ID()
	}

	// é˜¶æ®µ1ï¼šå°è¯• Kæ¡¶èŠ‚ç‚¹ï¼ˆæœ€ä¼˜ï¼‰
	candidates, err := selectKBucketPeersForSync(ctx, routingManager, p2pService, configProvider, chainInfo, logger)
	if err == nil && len(candidates) > 0 {
		// Kæ¡¶èŠ‚ç‚¹é€‰æ‹©æˆåŠŸï¼Œå·²ç»è¿‡æ»¤äº†ç†”æ–­èŠ‚ç‚¹
		healthyCandidates := filterHealthyPeers(candidates, logger)
		if len(healthyCandidates) > 0 {
			if logger != nil {
				logger.Infof("âœ… Kæ¡¶èŠ‚ç‚¹å¯ç”¨: %d ä¸ª", len(healthyCandidates))
			}
			return healthyCandidates, nil
		}
	}

	// é˜¶æ®µ2ï¼šKæ¡¶èŠ‚ç‚¹å…¨éƒ¨ä¸å¯ç”¨ï¼Œé™çº§åˆ° DHT å·²è¿æ¥èŠ‚ç‚¹
	if logger != nil {
		logger.Warn("âš ï¸ Kæ¡¶èŠ‚ç‚¹å…¨éƒ¨ä¸å¯ç”¨ï¼Œé™çº§åˆ°DHTå·²è¿æ¥èŠ‚ç‚¹")
	}

	if p2pService != nil && p2pService.Host() != nil {
		host := p2pService.Host()
		net := host.Network()
		if net != nil {
			connectedPeers := net.Peers()
			
			// è¿‡æ»¤ï¼šæ’é™¤è‡ªå·±ã€bad peersã€ç†”æ–­èŠ‚ç‚¹
			var validPeers []peer.ID
			for _, pid := range connectedPeers {
				if pid == "" || pid == localPeerID || IsBadPeer(pid) {
					continue
				}
				if !IsHealthy(pid) {
					continue
				}
				validPeers = append(validPeers, pid)
			}

			if len(validPeers) > 0 {
				if logger != nil {
					logger.Infof("âœ… DHTå·²è¿æ¥èŠ‚ç‚¹å¯ç”¨: %d ä¸ª", len(validPeers))
				}
				// éšæœºé€‰æ‹©æœ€å¤š 8 ä¸ªèŠ‚ç‚¹
				k := 8
				if configProvider != nil {
					if bc := configProvider.GetBlockchain(); bc != nil {
						if bc.Sync.Advanced.KBucketSelectionCount > 0 {
							k = bc.Sync.Advanced.KBucketSelectionCount
						}
					}
				}
				return selectRandomPeers(validPeers, k), nil
			}
		}
	}

	// é˜¶æ®µ3ï¼šè¿æ¥èŠ‚ç‚¹ä¹Ÿä¸å¯ç”¨ï¼Œå°è¯• Bootstrap èŠ‚ç‚¹
	if logger != nil {
		logger.Warn("âš ï¸ DHTèŠ‚ç‚¹ä¹Ÿä¸å¯ç”¨ï¼Œå°è¯•BootstrapèŠ‚ç‚¹")
	}

	bootstrapPeers := getBootstrapPeers(configProvider)
	if len(bootstrapPeers) > 0 {
		// è¿‡æ»¤å¥åº·çš„ Bootstrap èŠ‚ç‚¹
		healthyBootstrap := filterHealthyPeers(bootstrapPeers, logger)
		if len(healthyBootstrap) > 0 {
			if logger != nil {
				logger.Infof("âœ… BootstrapèŠ‚ç‚¹å¯ç”¨: %d ä¸ª", len(healthyBootstrap))
			}
			return healthyBootstrap, nil
		}
	}

	return nil, fmt.Errorf("æ²¡æœ‰å¯ç”¨çš„åŒæ­¥èŠ‚ç‚¹ï¼ˆKæ¡¶ã€DHTã€Bootstrapå‡ä¸å¯ç”¨ï¼‰")
}
