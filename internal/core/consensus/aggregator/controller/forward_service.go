// forward_service.go - åŒºå—è½¬å‘æœåŠ¡
// ğŸ†• MEDIUM-001 ä¿®å¤ï¼šä¼˜åŒ–åŒºå—è½¬å‘æœºåˆ¶
package controller

import (
	"context"
	"errors"
	"sync"
	"sync/atomic"
	"time"

	"github.com/libp2p/go-libp2p/core/peer"
	consensusconfig "github.com/weisyn/v1/internal/config/consensus"
	"github.com/weisyn/v1/pkg/constants/protocols"
	"github.com/weisyn/v1/pkg/interfaces/infrastructure/kademlia"
	"github.com/weisyn/v1/pkg/interfaces/infrastructure/log"
	netiface "github.com/weisyn/v1/pkg/interfaces/network"
)

// ForwardService åŒºå—è½¬å‘æœåŠ¡
// è´Ÿè´£ç®¡ç†åŒºå—è½¬å‘çš„é‡è¯•ã€è¶…æ—¶å’Œå¥åº·åˆ†
type ForwardService struct {
	logger         log.Logger
	networkService netiface.Network
	routingManager kademlia.RoutingTableManager
	config         consensusconfig.BlockForwardConfig

	// åŠ¨æ€è¶…æ—¶ç®¡ç†
	currentTimeout time.Duration
	timeoutMu      sync.RWMutex

	// è½¬å‘ç»Ÿè®¡
	totalForwards     uint64
	successForwards   uint64
	failedForwards    uint64
	timeoutForwards   uint64
	retryForwards     uint64

	// å¤‡ç”¨èŠ‚ç‚¹ç¼“å­˜
	backupNodes   map[uint64][]peer.ID // height -> backup nodes
	backupNodesMu sync.RWMutex
}

// NewForwardService åˆ›å»ºè½¬å‘æœåŠ¡
func NewForwardService(
	logger log.Logger,
	networkService netiface.Network,
	routingManager kademlia.RoutingTableManager,
	config consensusconfig.BlockForwardConfig,
) *ForwardService {
	// è®¾ç½®é»˜è®¤å€¼
	if config.MaxRetries <= 0 {
		config.MaxRetries = 3
	}
	if config.RetryBackoffBase <= 0 {
		config.RetryBackoffBase = 500 * time.Millisecond
	}
	if config.RetryBackoffMax <= 0 {
		config.RetryBackoffMax = 10 * time.Second
	}
	if config.RetryBackoffFactor <= 0 {
		config.RetryBackoffFactor = 2.0
	}
	if config.CallTimeout <= 0 {
		config.CallTimeout = 15 * time.Second
	}
	if config.MinTimeout <= 0 {
		config.MinTimeout = 5 * time.Second
	}
	if config.MaxTimeout <= 0 {
		config.MaxTimeout = 30 * time.Second
	}
	if config.BackupNodeCount <= 0 {
		config.BackupNodeCount = 2
	}

	return &ForwardService{
		logger:         logger,
		networkService: networkService,
		routingManager: routingManager,
		config:         config,
		currentTimeout: config.CallTimeout,
		backupNodes:    make(map[uint64][]peer.ID),
	}
}

// ForwardResult è½¬å‘ç»“æœ
type ForwardResult struct {
	Success     bool
	Attempts    int
	Duration    time.Duration
	Error       error
	UsedBackup  bool
	FinalTarget peer.ID
}

// ForwardWithRetry å¸¦é‡è¯•çš„åŒºå—è½¬å‘
func (fs *ForwardService) ForwardWithRetry(
	ctx context.Context,
	target peer.ID,
	height uint64,
	data []byte,
) (*ForwardResult, error) {
	atomic.AddUint64(&fs.totalForwards, 1)

	startTime := time.Now()
	result := &ForwardResult{
		FinalTarget: target,
	}

	// è·å–å¤‡ç”¨èŠ‚ç‚¹åˆ—è¡¨
	backupNodes := fs.getBackupNodes(height, target)

	// æ‰€æœ‰å€™é€‰èŠ‚ç‚¹ï¼ˆä¸»èŠ‚ç‚¹ + å¤‡ç”¨èŠ‚ç‚¹ï¼‰
	candidates := append([]peer.ID{target}, backupNodes...)

	// é‡è¯•é€€é¿
	backoff := newForwardBackoff(fs.config.RetryBackoffBase, fs.config.RetryBackoffMax, fs.config.RetryBackoffFactor)

	var lastErr error

	for attempt := 0; attempt < fs.config.MaxRetries; attempt++ {
		for i, candidate := range candidates {
			result.Attempts++

			// è·å–å½“å‰è¶…æ—¶æ—¶é—´
			timeout := fs.getCurrentTimeout()

			// åˆ›å»ºå¸¦è¶…æ—¶çš„ä¸Šä¸‹æ–‡
			callCtx, cancel := context.WithTimeout(ctx, timeout)

			// æ‰§è¡Œç½‘ç»œè°ƒç”¨
			_, err := fs.networkService.Call(callCtx, candidate, protocols.ProtocolBlockSubmission, data, nil)
			cancel()

			if err == nil {
				// è½¬å‘æˆåŠŸ
				result.Success = true
				result.FinalTarget = candidate
				result.UsedBackup = i > 0
				result.Duration = time.Since(startTime)

				atomic.AddUint64(&fs.successForwards, 1)

				// è®°å½•æˆåŠŸåˆ°å¥åº·ç³»ç»Ÿ
				if fs.routingManager != nil {
					fs.routingManager.RecordPeerSuccess(candidate)
				}

				// åŠ¨æ€è°ƒæ•´è¶…æ—¶ï¼ˆæˆåŠŸæ—¶å‡å°‘ï¼‰
				if fs.config.EnableDynamicTimeout {
					fs.adjustTimeout(true)
				}

				if fs.logger != nil {
					fs.logger.Infof("âœ… åŒºå—è½¬å‘æˆåŠŸ: target=%s, height=%d, attempts=%d, used_backup=%v, duration=%s",
						candidate.String()[:12], height, result.Attempts, result.UsedBackup, result.Duration)
				}

				return result, nil
			}

			// è®°å½•é”™è¯¯
			lastErr = err

			// æ£€æŸ¥æ˜¯å¦ä¸ºè¶…æ—¶é”™è¯¯
			if errors.Is(err, context.DeadlineExceeded) {
				atomic.AddUint64(&fs.timeoutForwards, 1)
				if fs.config.EnableDynamicTimeout {
					fs.adjustTimeout(false)
				}
			}

			// è®°å½•å¤±è´¥åˆ°å¥åº·ç³»ç»Ÿ
			if fs.routingManager != nil {
				fs.routingManager.RecordPeerFailure(candidate)
			}

			if fs.logger != nil {
				fs.logger.Warnf("âš ï¸ åŒºå—è½¬å‘å¤±è´¥: target=%s, height=%d, attempt=%d, error=%v",
					candidate.String()[:12], height, result.Attempts, err)
			}

			// æ£€æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦å·²å–æ¶ˆ
			select {
			case <-ctx.Done():
				result.Error = ctx.Err()
				result.Duration = time.Since(startTime)
				atomic.AddUint64(&fs.failedForwards, 1)
				return result, ctx.Err()
			default:
			}
		}

		// æ‰€æœ‰å€™é€‰éƒ½å¤±è´¥ï¼Œç­‰å¾…é€€é¿æ—¶é—´åé‡è¯•
		if attempt < fs.config.MaxRetries-1 {
			atomic.AddUint64(&fs.retryForwards, 1)
			backoffDuration := backoff.Next()

			if fs.logger != nil {
				fs.logger.Infof("ğŸ”„ åŒºå—è½¬å‘é‡è¯•: height=%d, attempt=%d/%d, backoff=%s",
					height, attempt+2, fs.config.MaxRetries, backoffDuration)
			}

			select {
			case <-ctx.Done():
				result.Error = ctx.Err()
				result.Duration = time.Since(startTime)
				atomic.AddUint64(&fs.failedForwards, 1)
				return result, ctx.Err()
			case <-time.After(backoffDuration):
			}
		}
	}

	// æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
	result.Error = lastErr
	result.Duration = time.Since(startTime)
	atomic.AddUint64(&fs.failedForwards, 1)

	if fs.logger != nil {
		fs.logger.Errorf("ğŸš« åŒºå—è½¬å‘æœ€ç»ˆå¤±è´¥: height=%d, attempts=%d, duration=%s, error=%v",
			height, result.Attempts, result.Duration, lastErr)
	}

	return result, lastErr
}

// getBackupNodes è·å–å¤‡ç”¨èŠ‚ç‚¹
func (fs *ForwardService) getBackupNodes(height uint64, excludeTarget peer.ID) []peer.ID {
	if !fs.config.EnableBackupNodes {
		return nil
	}

	fs.backupNodesMu.RLock()
	cached, ok := fs.backupNodes[height]
	fs.backupNodesMu.RUnlock()

	if ok && len(cached) > 0 {
		// è¿‡æ»¤æ‰ä¸»ç›®æ ‡
		filtered := make([]peer.ID, 0, len(cached))
		for _, p := range cached {
			if p != excludeTarget {
				filtered = append(filtered, p)
			}
		}
		return filtered
	}

	// ä»è·¯ç”±è¡¨è·å–å¤‡ç”¨èŠ‚ç‚¹
	if fs.routingManager == nil {
		return nil
	}

	// è·å–æœ€è¿‘çš„èŠ‚ç‚¹ä½œä¸ºå¤‡ç”¨ï¼ˆä½¿ç”¨ target çš„å­—èŠ‚è¡¨ç¤ºï¼‰
	targetBytes := []byte(excludeTarget)
	closestPeers := fs.routingManager.FindClosestPeers(targetBytes, fs.config.BackupNodeCount+1)

	backups := make([]peer.ID, 0, fs.config.BackupNodeCount)
	for _, p := range closestPeers {
		if p != excludeTarget && len(backups) < fs.config.BackupNodeCount {
			backups = append(backups, p)
		}
	}

	// ç¼“å­˜å¤‡ç”¨èŠ‚ç‚¹
	fs.backupNodesMu.Lock()
	fs.backupNodes[height] = backups
	fs.backupNodesMu.Unlock()

	return backups
}

// ClearBackupCache æ¸…ç†å¤‡ç”¨èŠ‚ç‚¹ç¼“å­˜
func (fs *ForwardService) ClearBackupCache(height uint64) {
	fs.backupNodesMu.Lock()
	delete(fs.backupNodes, height)
	fs.backupNodesMu.Unlock()
}

// ClearAllBackupCache æ¸…ç†æ‰€æœ‰å¤‡ç”¨èŠ‚ç‚¹ç¼“å­˜
func (fs *ForwardService) ClearAllBackupCache() {
	fs.backupNodesMu.Lock()
	fs.backupNodes = make(map[uint64][]peer.ID)
	fs.backupNodesMu.Unlock()
}

// getCurrentTimeout è·å–å½“å‰è¶…æ—¶æ—¶é—´
func (fs *ForwardService) getCurrentTimeout() time.Duration {
	fs.timeoutMu.RLock()
	defer fs.timeoutMu.RUnlock()
	return fs.currentTimeout
}

// adjustTimeout åŠ¨æ€è°ƒæ•´è¶…æ—¶æ—¶é—´
func (fs *ForwardService) adjustTimeout(success bool) {
	fs.timeoutMu.Lock()
	defer fs.timeoutMu.Unlock()

	if success {
		// æˆåŠŸæ—¶å‡å°‘è¶…æ—¶æ—¶é—´ï¼ˆæ›´æ¿€è¿›ï¼‰
		newTimeout := time.Duration(float64(fs.currentTimeout) * 0.95)
		if newTimeout < fs.config.MinTimeout {
			newTimeout = fs.config.MinTimeout
		}
		fs.currentTimeout = newTimeout
	} else {
		// å¤±è´¥æ—¶å¢åŠ è¶…æ—¶æ—¶é—´
		newTimeout := time.Duration(float64(fs.currentTimeout) * 1.2)
		if newTimeout > fs.config.MaxTimeout {
			newTimeout = fs.config.MaxTimeout
		}
		fs.currentTimeout = newTimeout
	}
}

// GetStats è·å–è½¬å‘ç»Ÿè®¡
func (fs *ForwardService) GetStats() ForwardStats {
	return ForwardStats{
		TotalForwards:   atomic.LoadUint64(&fs.totalForwards),
		SuccessForwards: atomic.LoadUint64(&fs.successForwards),
		FailedForwards:  atomic.LoadUint64(&fs.failedForwards),
		TimeoutForwards: atomic.LoadUint64(&fs.timeoutForwards),
		RetryForwards:   atomic.LoadUint64(&fs.retryForwards),
		CurrentTimeout:  fs.getCurrentTimeout(),
	}
}

// ForwardStats è½¬å‘ç»Ÿè®¡ä¿¡æ¯
type ForwardStats struct {
	TotalForwards   uint64
	SuccessForwards uint64
	FailedForwards  uint64
	TimeoutForwards uint64
	RetryForwards   uint64
	CurrentTimeout  time.Duration
}

// forwardBackoff è½¬å‘é€€é¿ç­–ç•¥
type forwardBackoff struct {
	base    time.Duration
	max     time.Duration
	factor  float64
	current time.Duration
	mu      sync.Mutex
}

// newForwardBackoff åˆ›å»ºè½¬å‘é€€é¿
func newForwardBackoff(base, max time.Duration, factor float64) *forwardBackoff {
	return &forwardBackoff{
		base:    base,
		max:     max,
		factor:  factor,
		current: base,
	}
}

// Next è·å–ä¸‹ä¸€ä¸ªé€€é¿æ—¶é—´
func (fb *forwardBackoff) Next() time.Duration {
	fb.mu.Lock()
	defer fb.mu.Unlock()

	current := fb.current
	fb.current = time.Duration(float64(fb.current) * fb.factor)
	if fb.current > fb.max {
		fb.current = fb.max
	}
	return current
}

// Reset é‡ç½®é€€é¿
func (fb *forwardBackoff) Reset() {
	fb.mu.Lock()
	defer fb.mu.Unlock()
	fb.current = fb.base
}

// HealthScoreRecoveryDaemon å¥åº·åˆ†æ¢å¤å®ˆæŠ¤è¿›ç¨‹
// æ³¨ï¼šå¥åº·åˆ†æ¢å¤é€»è¾‘å·²æ•´åˆåˆ° Kademlia çš„ç»´æŠ¤åç¨‹ä¸­
// æ­¤ç±»å‹ä¿ç•™ç”¨äºæœªæ¥æ‰©å±•ï¼Œæˆ–å¯é€šè¿‡è·¯ç”±è¡¨äº‹ä»¶å®ç°æ›´ç»†ç²’åº¦çš„æ¢å¤ç­–ç•¥
type HealthScoreRecoveryDaemon struct {
	routingManager kademlia.RoutingTableManager
	logger         log.Logger
	config         consensusconfig.BlockForwardConfig

	// æˆåŠŸçš„ peer è®°å½•ï¼ˆç”¨äºæ¸è¿›æ¢å¤ï¼‰
	successfulPeers   map[peer.ID]time.Time
	successfulPeersMu sync.RWMutex

	ctx    context.Context
	cancel context.CancelFunc
	wg     sync.WaitGroup
}

// NewHealthScoreRecoveryDaemon åˆ›å»ºå¥åº·åˆ†æ¢å¤å®ˆæŠ¤è¿›ç¨‹
func NewHealthScoreRecoveryDaemon(
	routingManager kademlia.RoutingTableManager,
	logger log.Logger,
	config consensusconfig.BlockForwardConfig,
) *HealthScoreRecoveryDaemon {
	ctx, cancel := context.WithCancel(context.Background())
	return &HealthScoreRecoveryDaemon{
		routingManager:  routingManager,
		logger:          logger,
		config:          config,
		successfulPeers: make(map[peer.ID]time.Time),
		ctx:             ctx,
		cancel:          cancel,
	}
}

// Start å¯åŠ¨å¥åº·åˆ†æ¢å¤å®ˆæŠ¤è¿›ç¨‹
func (d *HealthScoreRecoveryDaemon) Start() {
	if d.config.RecoveryInterval <= 0 {
		return
	}

	d.wg.Add(1)
	go d.recoveryLoop()

	if d.logger != nil {
		d.logger.Info("ğŸ¥ å¥åº·åˆ†æ¢å¤å®ˆæŠ¤è¿›ç¨‹å·²å¯åŠ¨")
	}
}

// Stop åœæ­¢å¥åº·åˆ†æ¢å¤å®ˆæŠ¤è¿›ç¨‹
func (d *HealthScoreRecoveryDaemon) Stop() {
	d.cancel()
	d.wg.Wait()

	if d.logger != nil {
		d.logger.Info("ğŸ¥ å¥åº·åˆ†æ¢å¤å®ˆæŠ¤è¿›ç¨‹å·²åœæ­¢")
	}
}

// RecordSuccess è®°å½•æˆåŠŸçš„ peerï¼ˆä¾›å¤–éƒ¨è°ƒç”¨ï¼‰
func (d *HealthScoreRecoveryDaemon) RecordSuccess(peerID peer.ID) {
	d.successfulPeersMu.Lock()
	d.successfulPeers[peerID] = time.Now()
	d.successfulPeersMu.Unlock()

	// åŒæ—¶é€šçŸ¥è·¯ç”±ç®¡ç†å™¨
	if d.routingManager != nil {
		d.routingManager.RecordPeerSuccess(peerID)
	}
}

// recoveryLoop å¥åº·åˆ†æ¢å¤å¾ªç¯
func (d *HealthScoreRecoveryDaemon) recoveryLoop() {
	defer d.wg.Done()

	ticker := time.NewTicker(d.config.RecoveryInterval)
	defer ticker.Stop()

	for {
		select {
		case <-d.ctx.Done():
			return
		case <-ticker.C:
			d.performRecovery()
		}
	}
}

// performRecovery æ‰§è¡Œå¥åº·åˆ†æ¢å¤
// åŸºäºæœ€è¿‘æˆåŠŸè®°å½•çš„ peer åˆ—è¡¨è¿›è¡Œæ¸è¿›æ¢å¤
func (d *HealthScoreRecoveryDaemon) performRecovery() {
	if d.routingManager == nil {
		return
	}

	d.successfulPeersMu.Lock()
	defer d.successfulPeersMu.Unlock()

	recoveredCount := 0
	now := time.Now()
	expireDuration := d.config.RecoveryInterval * 3 // ä¿ç•™3ä¸ªå‘¨æœŸçš„è®°å½•

	for peerID, lastSuccess := range d.successfulPeers {
		// æ¸…ç†è¿‡æœŸè®°å½•
		if now.Sub(lastSuccess) > expireDuration {
			delete(d.successfulPeers, peerID)
			continue
		}

		// å¯¹æœ€è¿‘æˆåŠŸçš„ peer è®°å½•ä¸€æ¬¡æˆåŠŸï¼ˆæ¸è¿›æ¢å¤å¥åº·åˆ†ï¼‰
		d.routingManager.RecordPeerSuccess(peerID)
		recoveredCount++
	}

	if recoveredCount > 0 && d.logger != nil {
		d.logger.Debugf("ğŸ¥ å¥åº·åˆ†æ¢å¤: recovered=%d peers", recoveredCount)
	}
}

