// Package submitter å®ç°çŸ¿å·¥æäº¤åçš„ç­‰å¾…ä¸æŸ¥è¯¢æœºåˆ¶
//
// ğŸ¯ **V2 æ–°å¢**ï¼šæäº¤è€…ç­‰å¾…å¹¿æ’­ä¸ä¸»åŠ¨æŸ¥è¯¢æœºåˆ¶
//
// æ ¸å¿ƒåŠŸèƒ½ï¼š
// 1. è®¡ç®—ç­‰å¾…è¶…æ—¶æ—¶é—´ï¼ˆåŸºäºé…ç½®çš„ CollectionWindowDuration + DistributionTimeout + NetworkBufferï¼‰
// 2. è®¢é˜… ConsensusResultBroadcast å¹¿æ’­æ¶ˆæ¯
// 3. è¶…æ—¶åä¸»åŠ¨æŸ¥è¯¢èšåˆå™¨çŠ¶æ€
// 4. å¤„ç†èšåˆå™¨ç¦»çº¿/åœ¨çº¿ä½†æœªå®Œæˆ/å·²å®Œæˆç­‰æƒ…å†µ
// 5. æ”¯æŒé‡é€‰æœºåˆ¶ï¼ˆå¦‚æœèšåˆå™¨ç¦»çº¿æˆ–è¿”å› NOT_AGGREGATORï¼‰
//
// ä½œè€…ï¼šWESå¼€å‘å›¢é˜Ÿ
// åˆ›å»ºæ—¶é—´ï¼š2025-12-15

package submitter

import (
	"context"
	"fmt"
	"time"

	"github.com/libp2p/go-libp2p/core/peer"
	consensusconfig "github.com/weisyn/v1/internal/config/consensus"
	"github.com/weisyn/v1/internal/core/consensus/interfaces"
	"github.com/weisyn/v1/pb/network/protocol"
	"github.com/weisyn/v1/pkg/constants/protocols"
	"github.com/weisyn/v1/pkg/interfaces/infrastructure/log"
	netiface "github.com/weisyn/v1/pkg/interfaces/network"
	p2pi "github.com/weisyn/v1/pkg/interfaces/p2p"
	"google.golang.org/protobuf/proto"
)

// WaitAndQueryService ç­‰å¾…ä¸æŸ¥è¯¢æœåŠ¡
//
// V2 æ–°å¢ï¼šæäº¤è€…ç­‰å¾…å¹¿æ’­ä¸ä¸»åŠ¨æŸ¥è¯¢æœºåˆ¶
type WaitAndQueryService struct {
	logger          log.Logger
	config          *consensusconfig.ConsensusOptions
	networkService  netiface.Network
	p2pService      p2pi.Service
	electionService interfaces.AggregatorElection
}

// NewWaitAndQueryService åˆ›å»ºç­‰å¾…ä¸æŸ¥è¯¢æœåŠ¡
func NewWaitAndQueryService(
	logger log.Logger,
	config *consensusconfig.ConsensusOptions,
	networkService netiface.Network,
	p2pService p2pi.Service,
	electionService interfaces.AggregatorElection,
) *WaitAndQueryService {
	return &WaitAndQueryService{
		logger:          logger,
		config:          config,
		networkService:  networkService,
		p2pService:      p2pService,
		electionService: electionService,
	}
}

// WaitForAggregationResult ç­‰å¾…èšåˆç»“æœ
//
// # V2 æ–°å¢ï¼šæäº¤å€™é€‰åŒºå—åï¼Œç­‰å¾…èšåˆå™¨å¹¿æ’­æœ€ç»ˆåŒºå—æˆ–ä¸»åŠ¨æŸ¥è¯¢çŠ¶æ€
//
// æµç¨‹ï¼š
// 1. è®¢é˜… ConsensusResultBroadcast å¹¿æ’­æ¶ˆæ¯
// 2. è®¡ç®—ç­‰å¾…è¶…æ—¶æ—¶é—´ï¼ˆCollectionWindowDuration + DistributionTimeout + NetworkBufferï¼‰
// 3. ç­‰å¾…è¶…æ—¶åï¼Œä¸»åŠ¨æŸ¥è¯¢èšåˆå™¨çŠ¶æ€
// 4. å¤„ç†èšåˆå™¨ç¦»çº¿/åœ¨çº¿ä½†æœªå®Œæˆ/å·²å®Œæˆç­‰æƒ…å†µ
// 5. æ”¯æŒé‡é€‰æœºåˆ¶ï¼ˆå¦‚æœèšåˆå™¨ç¦»çº¿æˆ–è¿”å› NOT_AGGREGATORï¼‰
//
// @param ctx ä¸Šä¸‹æ–‡
// @param height å€™é€‰åŒºå—é«˜åº¦
// @param aggregatorID èšåˆå™¨èŠ‚ç‚¹ID
// @return error ç­‰å¾…è¿‡ç¨‹ä¸­çš„é”™è¯¯
func (s *WaitAndQueryService) WaitForAggregationResult(
	ctx context.Context,
	height uint64,
	aggregatorID peer.ID,
) error {
	s.logger.Infof("ğŸ“¡ å¼€å§‹ç­‰å¾…èšåˆç»“æœ: height=%d, aggregator=%s", height, aggregatorID)

	// 1. è®¡ç®—ç­‰å¾…è¶…æ—¶æ—¶é—´ï¼ˆåŸºäºé…ç½®ï¼‰
	waitTimeout := s.calculateWaitTimeout()
	s.logger.Infof("â±ï¸  ç­‰å¾…è¶…æ—¶æ—¶é—´: %s", waitTimeout)

	// 2. V2 ä¼˜åŒ–ï¼šè®¢é˜… ConsensusResultBroadcast å¹¿æ’­æ¶ˆæ¯ï¼ˆé€šè¿‡ channel æ¥æ”¶ï¼‰
	resultChan := make(chan *protocol.ConsensusResultBroadcast, 10) // å¢åŠ ç¼“å†²é¿å…é˜»å¡
	unsubscribe, err := s.subscribeToConsensusResult(ctx, height, resultChan)
	if err != nil {
		s.logger.Warnf("âš ï¸ è®¢é˜…å…±è¯†ç»“æœå¹¿æ’­å¤±è´¥: %vï¼Œå°†ä»…ä¾èµ–ä¸»åŠ¨æŸ¥è¯¢", err)
		// è®¢é˜…å¤±è´¥ä¸è‡´å‘½ï¼Œç»§ç»­ä¾èµ–ä¸»åŠ¨æŸ¥è¯¢
	} else {
		defer unsubscribe()
		s.logger.Debugf("âœ… Gossip è®¢é˜…æˆåŠŸ")
	}

	// 3. V2 ä¼˜åŒ–ï¼šä¼˜å…ˆå¤„ç† Gossip å¹¿æ’­ï¼Œå…¶æ¬¡æ‰æ˜¯è¶…æ—¶æŸ¥è¯¢
	timer := time.NewTimer(waitTimeout)
	defer timer.Stop()

	for {
		select {
		case <-ctx.Done():
			return ctx.Err()

		case broadcast, ok := <-resultChan:
			if !ok {
				s.logger.Warnf("âš ï¸ å¹¿æ’­ channel å·²å…³é—­")
				// Channel å…³é—­ï¼Œç»§ç»­ç­‰å¾…è¶…æ—¶æˆ–æ‰§è¡ŒæŸ¥è¯¢
				goto QueryStatus
			}
			if broadcast != nil && broadcast.FinalBlock != nil {
				s.logger.Infof("âœ… é€šè¿‡ Gossip å¹¿æ’­æ”¶åˆ°æœ€ç»ˆåŒºå—: height=%d", height)
				// æˆåŠŸæ”¶åˆ°å¹¿æ’­ï¼Œå¤„ç†å®Œæˆ
				return nil
			}

		case <-timer.C:
			// ç­‰å¾…è¶…æ—¶ï¼Œä¸»åŠ¨æŸ¥è¯¢
			s.logger.Warnf("â° ç­‰å¾…å¹¿æ’­è¶…æ—¶: height=%d, timeout=%s", height, waitTimeout)
			goto QueryStatus
		}
	}

QueryStatus:
	// 4. è¶…æ—¶åä¸»åŠ¨æŸ¥è¯¢èšåˆå™¨çŠ¶æ€
	if err := s.queryAggregatorStatus(ctx, height, aggregatorID); err != nil {
		s.logger.Errorf("âŒ ä¸»åŠ¨æŸ¥è¯¢èšåˆå™¨çŠ¶æ€å¤±è´¥: %v", err)
		return fmt.Errorf("query aggregator status failed: %v", err)
	}

	// æŸ¥è¯¢æˆåŠŸåï¼Œå†ç­‰å¾…ä¸€å°æ®µæ—¶é—´æ¥æ”¶å¯èƒ½çš„å¹¿æ’­
	s.logger.Debugf("ğŸ” æŸ¥è¯¢å®Œæˆï¼Œå†ç­‰å¾…5ç§’æ¥æ”¶å¯èƒ½çš„å¹¿æ’­...")
	finalTimer := time.NewTimer(5 * time.Second)
	defer finalTimer.Stop()

	select {
	case broadcast, ok := <-resultChan:
		if ok && broadcast != nil && broadcast.FinalBlock != nil {
			s.logger.Infof("âœ… æŸ¥è¯¢åé€šè¿‡å¹¿æ’­æ”¶åˆ°æœ€ç»ˆåŒºå—: height=%d", height)
			return nil
		}
	case <-finalTimer.C:
		s.logger.Debugf("æŸ¥è¯¢å®Œæˆåæœªæ”¶åˆ°å¹¿æ’­ï¼Œä½†æŸ¥è¯¢å·²æˆåŠŸ")
		return nil
	case <-ctx.Done():
		return ctx.Err()
	}

	return nil
}

// calculateWaitTimeout è®¡ç®—ç­‰å¾…è¶…æ—¶æ—¶é—´
//
// V2 æ–°å¢ï¼šæ ¹æ®é…ç½®åŠ¨æ€è®¡ç®—
// waitTimeout = CollectionWindowDuration + DistributionTimeout + NetworkBuffer
func (s *WaitAndQueryService) calculateWaitTimeout() time.Duration {
	const defaultNetworkBuffer = 5 * time.Second

	collectionWindow := 10 * time.Second    // é»˜è®¤å€¼
	distributionTimeout := 30 * time.Second // é»˜è®¤å€¼

	if s.config != nil {
		if s.config.Aggregator.CollectionWindowDuration > 0 {
			collectionWindow = s.config.Aggregator.CollectionWindowDuration
		}
		if s.config.Aggregator.DistributionTimeout > 0 {
			distributionTimeout = s.config.Aggregator.DistributionTimeout
		}
	}

	waitTimeout := collectionWindow + distributionTimeout + defaultNetworkBuffer
	return waitTimeout
}

// subscribeToConsensusResult è®¢é˜…å…±è¯†ç»“æœå¹¿æ’­
//
// V2 æ–°å¢ï¼šè®¢é˜… Gossip å¹¿æ’­æ¶ˆæ¯
func (s *WaitAndQueryService) subscribeToConsensusResult(
	ctx context.Context,
	height uint64,
	resultChan chan<- *protocol.ConsensusResultBroadcast,
) (func(), error) {
	// V2 ç®€åŒ–å®ç°ï¼šæš‚ä¸å®ç° Gossip è®¢é˜…ï¼Œä¿ç•™æ¥å£ä¾›åç»­å®Œå–„
	// åŸå› ï¼šéœ€è¦è¿›ä¸€æ­¥è®¾è®¡è®¢é˜…æœºåˆ¶ä¸ç°æœ‰ç³»ç»Ÿçš„é›†æˆæ–¹å¼
	s.logger.Debugf("ğŸ“¡ Gossip è®¢é˜…åŠŸèƒ½å¾…åç»­å®Œå–„ï¼ˆå½“å‰ä»…ä¾èµ–ä¸»åŠ¨æŸ¥è¯¢ï¼‰")

	// è¿”å›ç©ºçš„ unsubscribe å‡½æ•°ï¼Œé¿å…è°ƒç”¨æ–¹å‡ºé”™
	return func() {}, nil

	// TODO: å®Œå–„ Gossip è®¢é˜…å®ç°
	// å‚è€ƒ: internal/core/consensus/integration/network/subscribe_handlers.go
	// éœ€è¦ç¡®è®¤å¦‚ä½•é€šè¿‡ networkService.Subscribe() è·å– channel å½¢å¼çš„è®¢é˜…
}

// queryAggregatorStatus ä¸»åŠ¨æŸ¥è¯¢èšåˆå™¨çŠ¶æ€
//
// V2 æ–°å¢ï¼šç­‰å¾…è¶…æ—¶åä¸»åŠ¨æŸ¥è¯¢
func (s *WaitAndQueryService) queryAggregatorStatus(
	ctx context.Context,
	height uint64,
	aggregatorID peer.ID,
) error {
	s.logger.Infof("ğŸ” ä¸»åŠ¨æŸ¥è¯¢èšåˆå™¨çŠ¶æ€: height=%d, aggregator=%s", height, aggregatorID)

	// 1. è·å–æŸ¥è¯¢é…ç½®
	queryRetryInterval := 15 * time.Second
	maxQueryAttempts := uint32(3)
	queryTotalTimeout := 60 * time.Second

	if s.config != nil {
		if s.config.Miner.QueryRetryInterval > 0 {
			queryRetryInterval = s.config.Miner.QueryRetryInterval
		}
		if s.config.Miner.MaxQueryAttempts > 0 {
			maxQueryAttempts = s.config.Miner.MaxQueryAttempts
		}
		if s.config.Miner.QueryTotalTimeout > 0 {
			queryTotalTimeout = s.config.Miner.QueryTotalTimeout
		}
	}

	// 2. åˆ›å»ºæŸ¥è¯¢è¶…æ—¶ context
	queryCtx, cancel := context.WithTimeout(ctx, queryTotalTimeout)
	defer cancel()

	// 3. å¾ªç¯æŸ¥è¯¢ï¼Œç›´åˆ°æˆåŠŸæˆ–è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°
	for attempt := uint32(0); attempt < maxQueryAttempts; attempt++ {
		select {
		case <-queryCtx.Done():
			return fmt.Errorf("æŸ¥è¯¢æ€»è¶…æ—¶: %v", queryCtx.Err())
		default:
		}

		s.logger.Infof("ğŸ” æŸ¥è¯¢èšåˆå™¨çŠ¶æ€ï¼ˆå°è¯• %d/%dï¼‰: aggregator=%s", attempt+1, maxQueryAttempts, aggregatorID)

		// 4. æ„å»ºæŸ¥è¯¢è¯·æ±‚
		query := &protocol.AggregatorStatusQuery{
			Base: &protocol.BaseMessage{
				MessageId:     s.generateMessageID(),
				SenderId:      []byte(s.p2pService.Host().ID()),
				TimestampUnix: time.Now().Unix(),
			},
			Height: height,
		}

		reqBytes, err := proto.Marshal(query)
		if err != nil {
			s.logger.Errorf("âŒ åºåˆ—åŒ–æŸ¥è¯¢è¯·æ±‚å¤±è´¥: %v", err)
			continue
		}

		// 5. å‘é€æŸ¥è¯¢è¯·æ±‚
		respBytes, err := s.networkService.Call(queryCtx, aggregatorID, protocols.ProtocolAggregatorStatus, reqBytes, nil)
		if err != nil {
			// èšåˆå™¨ç¦»çº¿æˆ–ç½‘ç»œé”™è¯¯ï¼Œè§¦å‘é‡é€‰
			s.logger.Warnf("âš ï¸ æŸ¥è¯¢å¤±è´¥ï¼ˆèšåˆå™¨å¯èƒ½ç¦»çº¿ï¼‰: %v", err)
			// TODO: è§¦å‘é‡é€‰æœºåˆ¶
			return fmt.Errorf("aggregator offline or network error: %v", err)
		}

		// 6. ååºåˆ—åŒ–å“åº”
		var response protocol.AggregatorStatusResponse
		if err := proto.Unmarshal(respBytes, &response); err != nil {
			s.logger.Errorf("âŒ ååºåˆ—åŒ–å“åº”å¤±è´¥: %v", err)
			continue
		}

		// 7. å¤„ç†å“åº”çŠ¶æ€
		switch response.State {
		case protocol.AggregatorStatusResponse_AGGREGATOR_STATE_COMPLETED:
			// èšåˆå·²å®Œæˆï¼Œå¤„ç†æœ€ç»ˆåŒºå—
			s.logger.Infof("âœ… èšåˆå·²å®Œæˆ: height=%d", height)
			if response.FinalBlock != nil {
				// TODO: å¤„ç†æœ€ç»ˆåŒºå—
				s.logger.Infof("âœ… æ”¶åˆ°æœ€ç»ˆåŒºå—: height=%d", height)
				return nil
			}
			return fmt.Errorf("èšåˆå·²å®Œæˆä½†æœªè¿”å›æœ€ç»ˆåŒºå—")

		case protocol.AggregatorStatusResponse_AGGREGATOR_STATE_NOT_AGGREGATOR:
			// æŸ¥è¯¢äº†é”™è¯¯çš„èšåˆå™¨ï¼Œè§¦å‘é‡é€‰
			s.logger.Warnf("âš ï¸  æŸ¥è¯¢äº†é”™è¯¯çš„èšåˆå™¨: height=%d", height)
			// TODO: è§¦å‘é‡é€‰æœºåˆ¶
			return fmt.Errorf("queried wrong aggregator")

		case protocol.AggregatorStatusResponse_AGGREGATOR_STATE_COLLECTING,
			protocol.AggregatorStatusResponse_AGGREGATOR_STATE_EVALUATING,
			protocol.AggregatorStatusResponse_AGGREGATOR_STATE_DISTRIBUTING:
			// èšåˆå™¨æ­£åœ¨å¤„ç†ï¼Œç»§ç»­ç­‰å¾…
			s.logger.Infof("ğŸ”„ èšåˆå™¨æ­£åœ¨å¤„ç†: height=%d, state=%v, candidate_count=%d",
				height, response.State, response.CandidateCount)

			// å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
			if attempt < maxQueryAttempts-1 {
				s.logger.Infof("â³ ç­‰å¾… %s åé‡è¯•æŸ¥è¯¢", queryRetryInterval)
				select {
				case <-queryCtx.Done():
					return queryCtx.Err()
				case <-time.After(queryRetryInterval):
					continue
				}
			}
			// æœ€åä¸€æ¬¡å°è¯•ä¹Ÿæœªå®Œæˆï¼Œè¿”å›è¶…æ—¶
			return fmt.Errorf("èšåˆå™¨ä»åœ¨å¤„ç†ï¼ŒæŸ¥è¯¢å°è¯•å·²ç”¨å°½")

		default:
			// æœªçŸ¥çŠ¶æ€æˆ–é”™è¯¯
			s.logger.Warnf("âš ï¸  èšåˆå™¨è¿”å›æœªçŸ¥çŠ¶æ€: state=%v", response.State)
			return fmt.Errorf("unknown aggregator state: %v", response.State)
		}
	}

	return fmt.Errorf("æŸ¥è¯¢å°è¯•å·²ç”¨å°½: max_attempts=%d", maxQueryAttempts)
}

// generateMessageID ç”Ÿæˆæ¶ˆæ¯ID
func (s *WaitAndQueryService) generateMessageID() string {
	return fmt.Sprintf("query_%d_%s", time.Now().UnixNano(), s.p2pService.Host().ID().String())
}
