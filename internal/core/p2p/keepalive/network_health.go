// network_health.go - ç½‘ç»œå¥åº·æ£€æŸ¥æœåŠ¡
// ğŸ†• HIGH-003 ä¿®å¤ï¼šæä¾›å…¨é¢çš„ç½‘ç»œå¥åº·ç›‘æ§å’Œè‡ªåŠ¨ä¿®å¤åŠŸèƒ½
package keepalive

import (
	"context"
	"sync"
	"sync/atomic"
	"time"

	"github.com/libp2p/go-libp2p/core/host"
	libnetwork "github.com/libp2p/go-libp2p/core/network"
	"github.com/libp2p/go-libp2p/core/peer"
	p2pcfg "github.com/weisyn/v1/internal/config/p2p"
	"github.com/weisyn/v1/pkg/constants/events"
	"github.com/weisyn/v1/pkg/interfaces/infrastructure/event"
	"github.com/weisyn/v1/pkg/interfaces/infrastructure/log"
	"github.com/weisyn/v1/pkg/types"
)

// NetworkHealthStatus ç½‘ç»œå¥åº·çŠ¶æ€
type NetworkHealthStatus string

const (
	NetworkHealthStatusHealthy   NetworkHealthStatus = "healthy"
	NetworkHealthStatusDegraded  NetworkHealthStatus = "degraded"
	NetworkHealthStatusUnhealthy NetworkHealthStatus = "unhealthy"
)

// NetworkHealthStats ç½‘ç»œå¥åº·ç»Ÿè®¡
type NetworkHealthStats struct {
	Status             NetworkHealthStatus
	TotalConnections   int
	ActiveConnections  int
	IdleConnections    int
	TotalTimeouts      uint64
	RecentTimeouts     uint64 // æœ€è¿‘ä¸€ä¸ªå‘¨æœŸå†…çš„è¶…æ—¶æ•°
	TimeoutRatio       float64
	AvgLatencyMs       float64
	LastCheckAt        time.Time
	ConsecutiveFailures int
	ConsecutiveSuccesses int
}

// NetworkHealthChecker ç½‘ç»œå¥åº·æ£€æŸ¥å™¨
type NetworkHealthChecker struct {
	host     host.Host
	logger   log.Logger
	eventBus event.EventBus
	config   p2pcfg.NetworkHealthConfig

	// çŠ¶æ€
	stats     NetworkHealthStats
	statsMu   sync.RWMutex
	
	// è¶…æ—¶è®¡æ•°å™¨
	totalTimeouts  uint64
	periodTimeouts uint64

	// åŠ¨æ€è¶…æ—¶ç®¡ç†
	currentTimeout   time.Duration
	timeoutConfig    p2pcfg.NetworkTimeoutConfig
	timeoutMu        sync.RWMutex

	// ä¿®å¤çŠ¶æ€
	healingAttempts  int
	lastHealingAt    time.Time
	healingMu        sync.Mutex

	// è¿è¡Œæ§åˆ¶
	ctx       context.Context
	cancel    context.CancelFunc
	wg        sync.WaitGroup
	running   bool
	runningMu sync.RWMutex
}

// NewNetworkHealthChecker åˆ›å»ºç½‘ç»œå¥åº·æ£€æŸ¥å™¨
func NewNetworkHealthChecker(
	host host.Host,
	logger log.Logger,
	eventBus event.EventBus,
	healthConfig p2pcfg.NetworkHealthConfig,
	timeoutConfig p2pcfg.NetworkTimeoutConfig,
) *NetworkHealthChecker {
	ctx, cancel := context.WithCancel(context.Background())

	// è®¾ç½®é»˜è®¤å€¼
	if healthConfig.CheckInterval <= 0 {
		healthConfig.CheckInterval = 30 * time.Second
	}
	if healthConfig.UnhealthyThreshold <= 0 {
		healthConfig.UnhealthyThreshold = 3
	}
	if healthConfig.HealthyThreshold <= 0 {
		healthConfig.HealthyThreshold = 2
	}
	if healthConfig.TimeoutRatioThreshold <= 0 {
		healthConfig.TimeoutRatioThreshold = 0.3
	}
	if healthConfig.HealingCooldown <= 0 {
		healthConfig.HealingCooldown = time.Minute
	}
	if healthConfig.MaxHealingAttempts <= 0 {
		healthConfig.MaxHealingAttempts = 5
	}

	return &NetworkHealthChecker{
		host:           host,
		logger:         logger,
		eventBus:       eventBus,
		config:         healthConfig,
		timeoutConfig:  timeoutConfig,
		currentTimeout: timeoutConfig.DialTimeout,
		stats: NetworkHealthStats{
			Status: NetworkHealthStatusHealthy,
		},
		ctx:    ctx,
		cancel: cancel,
	}
}

// Start å¯åŠ¨å¥åº·æ£€æŸ¥
func (nhc *NetworkHealthChecker) Start() error {
	nhc.runningMu.Lock()
	defer nhc.runningMu.Unlock()

	if nhc.running {
		return nil
	}

	nhc.running = true

	nhc.wg.Add(1)
	go nhc.checkLoop()

	if nhc.logger != nil {
		nhc.logger.Info("ğŸ¥ ç½‘ç»œå¥åº·æ£€æŸ¥å™¨å·²å¯åŠ¨")
	}

	return nil
}

// Stop åœæ­¢å¥åº·æ£€æŸ¥
func (nhc *NetworkHealthChecker) Stop() {
	nhc.runningMu.Lock()
	if !nhc.running {
		nhc.runningMu.Unlock()
		return
	}
	nhc.running = false
	nhc.runningMu.Unlock()

	nhc.cancel()
	nhc.wg.Wait()

	if nhc.logger != nil {
		nhc.logger.Info("ğŸ¥ ç½‘ç»œå¥åº·æ£€æŸ¥å™¨å·²åœæ­¢")
	}
}

// checkLoop å¥åº·æ£€æŸ¥å¾ªç¯
func (nhc *NetworkHealthChecker) checkLoop() {
	defer nhc.wg.Done()

	ticker := time.NewTicker(nhc.config.CheckInterval)
	defer ticker.Stop()

	for {
		select {
		case <-nhc.ctx.Done():
			return
		case <-ticker.C:
			nhc.performHealthCheck()
		}
	}
}

// performHealthCheck æ‰§è¡Œå¥åº·æ£€æŸ¥
func (nhc *NetworkHealthChecker) performHealthCheck() {
	nhc.statsMu.Lock()
	defer nhc.statsMu.Unlock()

	// æ”¶é›†è¿æ¥ç»Ÿè®¡
	network := nhc.host.Network()
	conns := network.Conns()
	totalConns := len(conns)

	activeConns := 0
	idleConns := 0
	var totalLatency time.Duration
	latencyCount := 0

	for _, conn := range conns {
		stat := conn.Stat()
		if stat.NumStreams > 0 {
			activeConns++
		} else {
			idleConns++
		}
		// ç®€å•çš„å»¶è¿Ÿä¼°ç®—ï¼ˆä½¿ç”¨è¿æ¥å»ºç«‹æ—¶é—´ï¼‰
		if !stat.Opened.IsZero() {
			latencyCount++
			totalLatency += time.Since(stat.Opened)
		}
	}

	// è·å–è¶…æ—¶ç»Ÿè®¡
	periodTimeouts := atomic.SwapUint64(&nhc.periodTimeouts, 0)
	totalTimeouts := atomic.LoadUint64(&nhc.totalTimeouts)

	// è®¡ç®—è¶…æ—¶æ¯”ä¾‹
	var timeoutRatio float64
	if totalConns > 0 {
		timeoutRatio = float64(periodTimeouts) / float64(totalConns+int(periodTimeouts))
	}

	// è®¡ç®—å¹³å‡å»¶è¿Ÿ
	var avgLatencyMs float64
	if latencyCount > 0 {
		avgLatencyMs = float64(totalLatency.Milliseconds()) / float64(latencyCount)
	}

	// æ›´æ–°ç»Ÿè®¡
	nhc.stats.TotalConnections = totalConns
	nhc.stats.ActiveConnections = activeConns
	nhc.stats.IdleConnections = idleConns
	nhc.stats.TotalTimeouts = totalTimeouts
	nhc.stats.RecentTimeouts = periodTimeouts
	nhc.stats.TimeoutRatio = timeoutRatio
	nhc.stats.AvgLatencyMs = avgLatencyMs
	nhc.stats.LastCheckAt = time.Now()

	// åˆ¤æ–­å¥åº·çŠ¶æ€
	oldStatus := nhc.stats.Status
	if timeoutRatio >= nhc.config.TimeoutRatioThreshold {
		nhc.stats.ConsecutiveFailures++
		nhc.stats.ConsecutiveSuccesses = 0
		if nhc.stats.ConsecutiveFailures >= nhc.config.UnhealthyThreshold {
			nhc.stats.Status = NetworkHealthStatusUnhealthy
		} else {
			nhc.stats.Status = NetworkHealthStatusDegraded
		}
	} else if totalConns < 3 {
		nhc.stats.Status = NetworkHealthStatusDegraded
		nhc.stats.ConsecutiveFailures++
		nhc.stats.ConsecutiveSuccesses = 0
	} else {
		nhc.stats.ConsecutiveSuccesses++
		nhc.stats.ConsecutiveFailures = 0
		if nhc.stats.ConsecutiveSuccesses >= nhc.config.HealthyThreshold {
			nhc.stats.Status = NetworkHealthStatusHealthy
		}
	}

	// çŠ¶æ€å˜åŒ–æ—¶è®°å½•æ—¥å¿—
	if oldStatus != nhc.stats.Status {
		if nhc.logger != nil {
			nhc.logger.Infof("ğŸ¥ ç½‘ç»œå¥åº·çŠ¶æ€å˜åŒ–: %s -> %s (conns=%d, timeout_ratio=%.2f%%)",
				oldStatus, nhc.stats.Status, totalConns, timeoutRatio*100)
		}

		// å‘å¸ƒçŠ¶æ€å˜åŒ–äº‹ä»¶
		if nhc.eventBus != nil {
			nhc.publishHealthEvent()
		}
	}

	// è§¦å‘è‡ªåŠ¨ä¿®å¤
	if nhc.config.EnableAutoHealing && nhc.stats.Status == NetworkHealthStatusUnhealthy {
		nhc.tryAutoHealing()
	}

	// åŠ¨æ€è°ƒæ•´è¶…æ—¶
	if nhc.timeoutConfig.EnableDynamicTimeout {
		nhc.adjustDynamicTimeout(timeoutRatio)
	}

	if nhc.logger != nil {
		nhc.logger.Debugf("ğŸ¥ å¥åº·æ£€æŸ¥å®Œæˆ: status=%s conns=%d active=%d idle=%d timeouts=%d ratio=%.2f%%",
			nhc.stats.Status, totalConns, activeConns, idleConns, periodTimeouts, timeoutRatio*100)
	}
}

// adjustDynamicTimeout åŠ¨æ€è°ƒæ•´è¶…æ—¶æ—¶é—´
func (nhc *NetworkHealthChecker) adjustDynamicTimeout(timeoutRatio float64) {
	nhc.timeoutMu.Lock()
	defer nhc.timeoutMu.Unlock()

	oldTimeout := nhc.currentTimeout

	if timeoutRatio >= nhc.config.TimeoutRatioThreshold {
		// è¶…æ—¶æ¯”ä¾‹é«˜ï¼Œå¢åŠ è¶…æ—¶æ—¶é—´
		newTimeout := time.Duration(float64(nhc.currentTimeout) * nhc.timeoutConfig.TimeoutIncreaseFactor)
		if newTimeout > nhc.timeoutConfig.MaxTimeout {
			newTimeout = nhc.timeoutConfig.MaxTimeout
		}
		nhc.currentTimeout = newTimeout
	} else if timeoutRatio < nhc.config.TimeoutRatioThreshold/2 {
		// è¶…æ—¶æ¯”ä¾‹ä½ï¼Œå‡å°‘è¶…æ—¶æ—¶é—´
		newTimeout := time.Duration(float64(nhc.currentTimeout) * nhc.timeoutConfig.TimeoutDecreaseFactor)
		if newTimeout < nhc.timeoutConfig.MinTimeout {
			newTimeout = nhc.timeoutConfig.MinTimeout
		}
		nhc.currentTimeout = newTimeout
	}

	if oldTimeout != nhc.currentTimeout && nhc.logger != nil {
		nhc.logger.Infof("ğŸ• åŠ¨æ€è¶…æ—¶è°ƒæ•´: %s -> %s (ratio=%.2f%%)",
			oldTimeout, nhc.currentTimeout, timeoutRatio*100)
	}
}

// tryAutoHealing å°è¯•è‡ªåŠ¨ä¿®å¤
func (nhc *NetworkHealthChecker) tryAutoHealing() {
	nhc.healingMu.Lock()
	defer nhc.healingMu.Unlock()

	// æ£€æŸ¥å†·å´æ—¶é—´
	if time.Since(nhc.lastHealingAt) < nhc.config.HealingCooldown {
		return
	}

	// æ£€æŸ¥æœ€å¤§å°è¯•æ¬¡æ•°
	if nhc.healingAttempts >= nhc.config.MaxHealingAttempts {
		if nhc.logger != nil {
			nhc.logger.Warnf("ğŸ¥ è‡ªåŠ¨ä¿®å¤å·²è¾¾æœ€å¤§å°è¯•æ¬¡æ•°: %d", nhc.config.MaxHealingAttempts)
		}
		return
	}

	nhc.healingAttempts++
	nhc.lastHealingAt = time.Now()

	if nhc.logger != nil {
		nhc.logger.Infof("ğŸ¥ å¼€å§‹è‡ªåŠ¨ä¿®å¤ç½‘ç»œ (å°è¯• %d/%d)",
			nhc.healingAttempts, nhc.config.MaxHealingAttempts)
	}

	// è§¦å‘å‘ç°åŠ é€Ÿ
	if nhc.eventBus != nil {
		resetData := &types.DiscoveryResetEventData{
			Reason:    "network_unhealthy",
			Trigger:   "network_health_checker",
			Timestamp: time.Now().Unix(),
		}
		nhc.eventBus.Publish(events.EventTypeDiscoveryIntervalReset, resetData)
	}

	// æ¸…ç†ç©ºé—²è¿æ¥
	nhc.cleanupIdleConnections()
}

// cleanupIdleConnections æ¸…ç†ç©ºé—²è¿æ¥
func (nhc *NetworkHealthChecker) cleanupIdleConnections() {
	if !nhc.config.ConnectionCheckEnabled {
		return
	}

	network := nhc.host.Network()
	conns := network.Conns()

	idleCount := 0
	closedCount := 0

	for _, conn := range conns {
		stat := conn.Stat()
		// æ£€æŸ¥æ˜¯å¦ç©ºé—²ä¸”è¶…æ—¶
		if stat.NumStreams == 0 {
			idleDuration := time.Since(stat.Opened)
			if idleDuration > nhc.config.IdleConnectionTimeout {
				if err := conn.Close(); err == nil {
					closedCount++
				}
			} else {
				idleCount++
			}
		}
	}

	if closedCount > 0 && nhc.logger != nil {
		nhc.logger.Infof("ğŸ§¹ æ¸…ç†ç©ºé—²è¿æ¥: closed=%d remaining_idle=%d", closedCount, idleCount)
	}
}

// publishHealthEvent å‘å¸ƒå¥åº·äº‹ä»¶
func (nhc *NetworkHealthChecker) publishHealthEvent() {
	if nhc.eventBus == nil {
		return
	}

	// å¯ä»¥å®šä¹‰ä¸€ä¸ªæ–°çš„äº‹ä»¶ç±»å‹ï¼Œè¿™é‡Œæš‚æ—¶ä½¿ç”¨æ—¥å¿—è®°å½•
	if nhc.logger != nil {
		nhc.logger.Infof("ğŸ“¢ ç½‘ç»œå¥åº·äº‹ä»¶: status=%s conns=%d timeouts=%d ratio=%.2f%%",
			nhc.stats.Status, nhc.stats.TotalConnections,
			nhc.stats.RecentTimeouts, nhc.stats.TimeoutRatio*100)
	}
}

// RecordTimeout è®°å½•è¶…æ—¶äº‹ä»¶
func (nhc *NetworkHealthChecker) RecordTimeout() {
	atomic.AddUint64(&nhc.totalTimeouts, 1)
	atomic.AddUint64(&nhc.periodTimeouts, 1)
}

// GetCurrentTimeout è·å–å½“å‰åŠ¨æ€è¶…æ—¶æ—¶é—´
func (nhc *NetworkHealthChecker) GetCurrentTimeout() time.Duration {
	nhc.timeoutMu.RLock()
	defer nhc.timeoutMu.RUnlock()
	return nhc.currentTimeout
}

// GetStats è·å–å¥åº·ç»Ÿè®¡
func (nhc *NetworkHealthChecker) GetStats() NetworkHealthStats {
	nhc.statsMu.RLock()
	defer nhc.statsMu.RUnlock()
	return nhc.stats
}

// IsHealthy æ£€æŸ¥ç½‘ç»œæ˜¯å¦å¥åº·
func (nhc *NetworkHealthChecker) IsHealthy() bool {
	nhc.statsMu.RLock()
	defer nhc.statsMu.RUnlock()
	return nhc.stats.Status == NetworkHealthStatusHealthy
}

// ResetHealingAttempts é‡ç½®ä¿®å¤å°è¯•è®¡æ•°
func (nhc *NetworkHealthChecker) ResetHealingAttempts() {
	nhc.healingMu.Lock()
	defer nhc.healingMu.Unlock()
	nhc.healingAttempts = 0
}

// ConnectionHealthChecker è¿æ¥å¥åº·æ£€æŸ¥å™¨ï¼ˆç”¨äºå•ä¸ªè¿æ¥ï¼‰
type ConnectionHealthChecker struct {
	timeout    time.Duration
	maxRetries int
	backoff    *RetryBackoff
}

// RetryBackoff é‡è¯•é€€é¿ç­–ç•¥
type RetryBackoff struct {
	base    time.Duration
	max     time.Duration
	factor  float64
	current time.Duration
	mu      sync.Mutex
}

// NewRetryBackoff åˆ›å»ºé‡è¯•é€€é¿
func NewRetryBackoff(base, max time.Duration, factor float64) *RetryBackoff {
	return &RetryBackoff{
		base:    base,
		max:     max,
		factor:  factor,
		current: base,
	}
}

// Next è·å–ä¸‹ä¸€ä¸ªé€€é¿æ—¶é—´
func (rb *RetryBackoff) Next() time.Duration {
	rb.mu.Lock()
	defer rb.mu.Unlock()

	current := rb.current
	rb.current = time.Duration(float64(rb.current) * rb.factor)
	if rb.current > rb.max {
		rb.current = rb.max
	}
	return current
}

// Reset é‡ç½®é€€é¿
func (rb *RetryBackoff) Reset() {
	rb.mu.Lock()
	defer rb.mu.Unlock()
	rb.current = rb.base
}

// NewConnectionHealthChecker åˆ›å»ºè¿æ¥å¥åº·æ£€æŸ¥å™¨
func NewConnectionHealthChecker(config p2pcfg.NetworkTimeoutConfig) *ConnectionHealthChecker {
	return &ConnectionHealthChecker{
		timeout:    config.DialTimeout,
		maxRetries: config.MaxRetries,
		backoff:    NewRetryBackoff(config.RetryBackoffBase, config.RetryBackoffMax, config.RetryBackoffFactor),
	}
}

// CheckConnection æ£€æŸ¥è¿æ¥å¥åº·çŠ¶æ€
func (chc *ConnectionHealthChecker) CheckConnection(ctx context.Context, host host.Host, peerID peer.ID) error {
	// æ£€æŸ¥è¿æ¥çŠ¶æ€
	connectedness := host.Network().Connectedness(peerID)
	if connectedness == libnetwork.Connected {
		return nil
	}

	// å°è¯•é‡è¿
	chc.backoff.Reset()
	var lastErr error

	for i := 0; i < chc.maxRetries; i++ {
		// ä½¿ç”¨åŠ¨æ€è¶…æ—¶
		dialCtx, cancel := context.WithTimeout(ctx, chc.timeout)

		addrs := host.Peerstore().Addrs(peerID)
		if len(addrs) > 0 {
			addrInfo := peer.AddrInfo{ID: peerID, Addrs: addrs}
			lastErr = host.Connect(dialCtx, addrInfo)
			cancel()

			if lastErr == nil {
				return nil
			}
		} else {
			cancel()
			lastErr = ErrNoAddresses
		}

		// ç­‰å¾…é€€é¿æ—¶é—´
		backoffDuration := chc.backoff.Next()
		select {
		case <-ctx.Done():
			return ctx.Err()
		case <-time.After(backoffDuration):
		}
	}

	return lastErr
}

// ErrNoAddresses æ— åœ°å€é”™è¯¯
var ErrNoAddresses = &NoAddressesError{}

// NoAddressesError æ— åœ°å€é”™è¯¯ç±»å‹
type NoAddressesError struct{}

func (e *NoAddressesError) Error() string {
	return "no addresses available for peer"
}

