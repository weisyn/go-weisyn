package keepalive

import (
	"context"
	"fmt"
	"sync"
	"time"

	"github.com/libp2p/go-libp2p/core/host"
	libnetwork "github.com/libp2p/go-libp2p/core/network"
	"github.com/libp2p/go-libp2p/core/peer"
	"github.com/weisyn/v1/internal/core/p2p/discovery"
	"github.com/weisyn/v1/pkg/constants/events"
	"github.com/weisyn/v1/pkg/interfaces/infrastructure/event"
	"github.com/weisyn/v1/pkg/interfaces/infrastructure/log"
	"github.com/weisyn/v1/pkg/interfaces/p2p"
	"github.com/weisyn/v1/pkg/types"
)

// ä¸ºé¿å…å¾ªç¯å¯¼å…¥ï¼Œå®šä¹‰æ‰€éœ€çš„æ¥å£åˆ«å
type RendezvousRouting = p2p.Routing

// KeyPeerMonitor å…³é”®peerç›‘æ§å™¨
// è´Ÿè´£å‘¨æœŸæ€§æ¢æµ‹å…³é”®peeré›†åˆï¼Œå¤±è´¥æ—¶è§¦å‘è‡ªæ„ˆ
type KeyPeerMonitor struct {
	host          host.Host
	routing       RendezvousRouting
	addrManager   *discovery.AddrManager
	keyPeerSet    *KeyPeerSet
	logger        log.Logger
	eventBus      event.EventBus
	
	// æ¢æµ‹çŠ¶æ€
	lastProbeAt   map[peer.ID]time.Time
	probeFailures map[peer.ID]int
	stateMu       sync.RWMutex
	
	// é…ç½®
	probeInterval      time.Duration  // æ¢æµ‹å‘¨æœŸï¼ˆé»˜è®¤60sï¼‰
	perPeerMinInterval time.Duration  // å•ä¸ªpeeræœ€å°æ¢æµ‹é—´éš”ï¼ˆé»˜è®¤30sï¼‰
	probeTimeout       time.Duration  // æ¢æµ‹è¶…æ—¶ï¼ˆé»˜è®¤5sï¼‰
	failThreshold      int            // å¤±è´¥é˜ˆå€¼ï¼ˆé»˜è®¤3ï¼‰
	maxConcurrent      int            // æœ€å¤§å¹¶å‘æ¢æµ‹æ•°ï¼ˆé»˜è®¤5ï¼‰
	
	probeSem      chan struct{}      // å¹¶å‘æ§åˆ¶ä¿¡å·é‡
	
	// è¿è¡Œæ§åˆ¶
	ctx        context.Context
	cancel     context.CancelFunc
	wg         sync.WaitGroup
	running    bool
	runningMu  sync.RWMutex
}

// NewKeyPeerMonitor åˆ›å»ºKeyPeerMonitor
func NewKeyPeerMonitor(
	host host.Host,
	routing RendezvousRouting,
	addrManager *discovery.AddrManager,
	keyPeerSet *KeyPeerSet,
	logger log.Logger,
	eventBus event.EventBus,
	probeInterval time.Duration,
	perPeerMinInterval time.Duration,
	probeTimeout time.Duration,
	failThreshold int,
	maxConcurrent int,
) *KeyPeerMonitor {
	// è®¾ç½®é»˜è®¤å€¼
	if probeInterval <= 0 {
		probeInterval = 60 * time.Second
	}
	if perPeerMinInterval <= 0 {
		perPeerMinInterval = 30 * time.Second
	}
	if probeTimeout <= 0 {
		probeTimeout = 5 * time.Second
	}
	if failThreshold <= 0 {
		failThreshold = 3
	}
	if maxConcurrent <= 0 {
		maxConcurrent = 5
	}
	
	ctx, cancel := context.WithCancel(context.Background())
	
	return &KeyPeerMonitor{
		host:               host,
		routing:            routing,
		addrManager:        addrManager,
		keyPeerSet:         keyPeerSet,
		logger:             logger,
		eventBus:           eventBus,
		lastProbeAt:        make(map[peer.ID]time.Time),
		probeFailures:      make(map[peer.ID]int),
		probeInterval:      probeInterval,
		perPeerMinInterval: perPeerMinInterval,
		probeTimeout:       probeTimeout,
		failThreshold:      failThreshold,
		maxConcurrent:      maxConcurrent,
		probeSem:           make(chan struct{}, maxConcurrent),
		ctx:                ctx,
		cancel:             cancel,
	}
}

// Start å¯åŠ¨ç›‘æ§å™¨
func (kpm *KeyPeerMonitor) Start() error {
	kpm.runningMu.Lock()
	defer kpm.runningMu.Unlock()
	
	if kpm.running {
		return fmt.Errorf("monitor already running")
	}
	
	kpm.running = true
	kpm.wg.Add(1)
	go kpm.probeLoop()
	
	if kpm.logger != nil {
		kpm.logger.Infof("âœ… KeyPeerMonitorå·²å¯åŠ¨: interval=%s per_peer_min=%s timeout=%s threshold=%d concurrent=%d",
			kpm.probeInterval, kpm.perPeerMinInterval, kpm.probeTimeout, kpm.failThreshold, kpm.maxConcurrent)
	}
	
	return nil
}

// Stop åœæ­¢ç›‘æ§å™¨
func (kpm *KeyPeerMonitor) Stop() error {
	kpm.runningMu.Lock()
	defer kpm.runningMu.Unlock()
	
	if !kpm.running {
		return nil
	}
	
	kpm.cancel()
	kpm.wg.Wait()
	kpm.running = false
	
	if kpm.logger != nil {
		kpm.logger.Info("KeyPeerMonitorå·²åœæ­¢")
	}
	
	return nil
}

// probeLoop æ¢æµ‹å¾ªç¯
func (kpm *KeyPeerMonitor) probeLoop() {
	defer kpm.wg.Done()
	
	ticker := time.NewTicker(kpm.probeInterval)
	defer ticker.Stop()
	
	for {
		select {
		case <-kpm.ctx.Done():
			return
		case <-ticker.C:
			kpm.runProbeRound()
		}
	}
}

// runProbeRound æ‰§è¡Œä¸€è½®æ¢æµ‹
func (kpm *KeyPeerMonitor) runProbeRound() {
	// å…è®¸åœ¨â€œæœªæ³¨å…¥çœŸå®hostâ€çš„æµ‹è¯•/é™çº§æ¨¡å¼ä¸‹è¿è¡Œï¼šç›´æ¥è·³è¿‡æ¢æµ‹ï¼Œé¿å…ç©ºæŒ‡é’ˆå´©æºƒ
	if kpm == nil || kpm.host == nil || kpm.keyPeerSet == nil {
		if kpm != nil && kpm.logger != nil {
			kpm.logger.Debug("KeyPeerMonitoræœªå°±ç»ªï¼ˆhost/keyPeerSetä¸ºç©ºï¼‰ï¼Œè·³è¿‡æœ¬è½®æ¢æµ‹")
		}
		return
	}

	// æ¸…ç†KeyPeerSetä¸­è¿‡æœŸçš„recentlyUsefulè®°å½•
	kpm.keyPeerSet.Cleanup()
	
	// è·å–æ‰€æœ‰å…³é”®peer
	keyPeers := kpm.keyPeerSet.GetAllKeyPeers()
	if len(keyPeers) == 0 {
		if kpm.logger != nil {
			kpm.logger.Debug("KeyPeerSetä¸ºç©ºï¼Œè·³è¿‡æœ¬è½®æ¢æµ‹")
		}
		return
	}
	
	if kpm.logger != nil {
		kpm.logger.Debugf("å¼€å§‹KeyPeeræ¢æµ‹è½®æ¬¡: key_peers=%d", len(keyPeers))
	}
	
	now := time.Now()
	probeCount := 0
	skippedCount := 0
	
	for _, p := range keyPeers {
		// æ£€æŸ¥æ˜¯å¦æ»¡è¶³per-peeræœ€å°é—´éš”
		kpm.stateMu.RLock()
		lastProbe, exists := kpm.lastProbeAt[p]
		kpm.stateMu.RUnlock()
		
		if exists && now.Sub(lastProbe) < kpm.perPeerMinInterval {
			skippedCount++
			continue
		}
		
		// æ£€æŸ¥è¿æ¥çŠ¶æ€
		connectedness := kpm.host.Network().Connectedness(p)
		if connectedness == libnetwork.Connected {
			// å·²è¿æ¥ï¼Œé‡ç½®å¤±è´¥è®¡æ•°
			kpm.stateMu.Lock()
			kpm.probeFailures[p] = 0
			kpm.lastProbeAt[p] = now
			kpm.stateMu.Unlock()
			continue
		}
		
		// éœ€è¦æ¢æµ‹
		probeCount++
		kpm.wg.Add(1)
		go func(peerID peer.ID) {
			defer kpm.wg.Done()
			
			// è·å–ä¿¡å·é‡
			select {
			case kpm.probeSem <- struct{}{}:
				defer func() { <-kpm.probeSem }()
			case <-kpm.ctx.Done():
				return
			}
			
			kpm.probePeer(peerID)
		}(p)
	}
	
	if kpm.logger != nil {
		kpm.logger.Debugf("KeyPeeræ¢æµ‹è½®æ¬¡å®Œæˆ: probed=%d skipped=%d total=%d", probeCount, skippedCount, len(keyPeers))
	}
}

// probePeer æ¢æµ‹å•ä¸ªpeer
func (kpm *KeyPeerMonitor) probePeer(p peer.ID) {
	if kpm == nil || kpm.host == nil {
		// æµ‹è¯•/é™çº§æ¨¡å¼ï¼šæ— çœŸå®hostæ—¶ä¸æ¢æµ‹
		return
	}

	if kpm.logger != nil {
		kpm.logger.Debugf("æ¢æµ‹peer: %s", p)
	}
	
	ctx, cancel := context.WithTimeout(kpm.ctx, kpm.probeTimeout)
	defer cancel()
	
	// è·å–peerçš„åœ°å€ä¿¡æ¯
	addrs := kpm.host.Peerstore().Addrs(p)
	if len(addrs) == 0 {
		if kpm.logger != nil {
			kpm.logger.Debugf("peer %s æ— åœ°å€ï¼Œè·³è¿‡æ¢æµ‹", p)
		}
		return
	}
	
	// å°è¯•è¿æ¥
	addrInfo := peer.AddrInfo{ID: p, Addrs: addrs}
	err := kpm.host.Connect(ctx, addrInfo)
	
	kpm.stateMu.Lock()
	kpm.lastProbeAt[p] = time.Now()
	
	if err != nil {
		// æ¢æµ‹å¤±è´¥
		kpm.probeFailures[p]++
		failCount := kpm.probeFailures[p]
		kpm.stateMu.Unlock()
		
		if kpm.logger != nil {
			kpm.logger.Warnf("æ¢æµ‹peerå¤±è´¥: %s, å¤±è´¥æ¬¡æ•°=%d/%d, é”™è¯¯: %v", p, failCount, kpm.failThreshold, err)
		}
		
		// è¾¾åˆ°å¤±è´¥é˜ˆå€¼ï¼Œè§¦å‘è‡ªæ„ˆ
		if failCount >= kpm.failThreshold {
			kpm.repairPeer(p)
		}
	} else {
		// æ¢æµ‹æˆåŠŸ
		kpm.probeFailures[p] = 0
		kpm.stateMu.Unlock()
		
		if kpm.logger != nil {
			kpm.logger.Debugf("æ¢æµ‹peeræˆåŠŸ: %s", p)
		}
	}
}

// repairPeer ä¿®å¤peerè¿æ¥
func (kpm *KeyPeerMonitor) repairPeer(p peer.ID) {
	if kpm.logger != nil {
		kpm.logger.Infof("ğŸ”§ å¼€å§‹ä¿®å¤peerè¿æ¥: %s", p)
	}
	
	// 1. å¿«é€Ÿé‡è¿ï¼ˆä½¿ç”¨å½“å‰åœ°å€ï¼‰
	ctx, cancel := context.WithTimeout(kpm.ctx, kpm.probeTimeout)
	addrs := kpm.host.Peerstore().Addrs(p)
	if len(addrs) > 0 {
		addrInfo := peer.AddrInfo{ID: p, Addrs: addrs}
		err := kpm.host.Connect(ctx, addrInfo)
		cancel()
		
		if err == nil {
			// é‡è¿æˆåŠŸ
			kpm.stateMu.Lock()
			kpm.probeFailures[p] = 0
			kpm.stateMu.Unlock()
			
			if kpm.logger != nil {
				kpm.logger.Infof("âœ… å¿«é€Ÿé‡è¿æˆåŠŸ: %s", p)
			}
			return
		}
		
		if kpm.logger != nil {
			kpm.logger.Warnf("å¿«é€Ÿé‡è¿å¤±è´¥: %s, é”™è¯¯: %v", p, err)
		}
	} else {
		cancel()
	}
	
	// 2. DHTè¡¥åœ°å€
	if kpm.routing != nil {
		ctx, cancel = context.WithTimeout(kpm.ctx, 30*time.Second)
		newAddrInfo, err := kpm.routing.FindPeer(ctx, p)
		cancel()
		
		if err != nil {
			if kpm.logger != nil {
				kpm.logger.Warnf("DHT FindPeerå¤±è´¥: %s, é”™è¯¯: %v", p, err)
			}
		} else if len(newAddrInfo.Addrs) > 0 {
			if kpm.logger != nil {
				kpm.logger.Infof("é€šè¿‡DHTæ‰¾åˆ°æ–°åœ°å€: %s, addrs=%d", p, len(newAddrInfo.Addrs))
			}
			
			// 3. ä½¿ç”¨æ–°åœ°å€äºŒæ¬¡é‡è¿
			ctx, cancel = context.WithTimeout(kpm.ctx, kpm.probeTimeout)
			err = kpm.host.Connect(ctx, newAddrInfo)
			cancel()
			
			if err == nil {
				kpm.stateMu.Lock()
				kpm.probeFailures[p] = 0
				kpm.stateMu.Unlock()
				
				if kpm.logger != nil {
					kpm.logger.Infof("âœ… ä½¿ç”¨æ–°åœ°å€é‡è¿æˆåŠŸ: %s", p)
				}
				return
			}
			
			if kpm.logger != nil {
				kpm.logger.Warnf("ä½¿ç”¨æ–°åœ°å€é‡è¿å¤±è´¥: %s, é”™è¯¯: %v", p, err)
			}
		}
	}
	
	// 4. å‘å¸ƒDiscoveryé—´éš”é‡ç½®äº‹ä»¶
	if kpm.eventBus != nil {
		resetData := &types.DiscoveryResetEventData{
			Reason:    "peer_disconnected",
			Trigger:   "keypeer_monitor",
			PeerID:    p.String(),
			Timestamp: time.Now().Unix(),
		}
		kpm.eventBus.Publish(events.EventTypeDiscoveryIntervalReset, resetData)
		
		if kpm.logger != nil {
			kpm.logger.Infof("ğŸ”„ å…³é”®peerä¿®å¤å¤±è´¥ï¼Œå·²è§¦å‘Discoveryé—´éš”é‡ç½®: %s", p)
		}
	}
}

