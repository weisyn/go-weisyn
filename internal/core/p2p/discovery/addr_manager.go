package discovery

import (
	"context"
	"strings"
	"sync"
	"time"

	libevent "github.com/libp2p/go-libp2p/core/event"
	lphost "github.com/libp2p/go-libp2p/core/host"
	libnetwork "github.com/libp2p/go-libp2p/core/network"
	libpeer "github.com/libp2p/go-libp2p/core/peer"
	"github.com/libp2p/go-libp2p/core/peerstore"
	ma "github.com/multiformats/go-multiaddr"
	manet "github.com/multiformats/go-multiaddr/net"
	"sort"

	"github.com/weisyn/v1/internal/core/p2p/interfaces"
	logiface "github.com/weisyn/v1/pkg/interfaces/infrastructure/log"
	metricsiface "github.com/weisyn/v1/pkg/interfaces/infrastructure/metrics"
)

// AddrTTL åœ°å€ç”Ÿå‘½å‘¨æœŸé…ç½®
type AddrTTL struct {
	DHT       time.Duration // DHTå‘ç°çš„åœ°å€
	Connected time.Duration // è¿æ¥æˆåŠŸçš„åœ°å€
	Bootstrap time.Duration // BootstrapèŠ‚ç‚¹åœ°å€
	Failed    time.Duration // è¿æ¥å¤±è´¥çš„åœ°å€
}

// DefaultAddrTTL é»˜è®¤åœ°å€TTLé…ç½®
var DefaultAddrTTL = AddrTTL{
	// ğŸ†• P0-009: DHT åœ°å€ TTL è¿‡çŸ­ä¼šå¯¼è‡´åœ°å€åœ¨çŸ­æ—¶é—´å†…è¿‡æœŸï¼ŒDHT å†æ¬¡å‘ç°æ—¶å‡ºç° addrs=0ã€‚
	// ç»Ÿä¸€é»˜è®¤å€¼ä¸º 2hï¼ˆä¸ discovery/service.go çš„å†…ç½®é…ç½®ä¿æŒä¸€è‡´ï¼‰ã€‚
	DHT:       2 * time.Hour,
	Connected: 24 * time.Hour,
	Bootstrap: peerstore.PermanentAddrTTL,
	Failed:    5 * time.Minute,
}

// PeerRediscoveryInfo peeré‡å‘ç°ä¿¡æ¯
type PeerRediscoveryInfo struct {
	PeerID        libpeer.ID
	LastAttemptAt time.Time
	FailCount     int
	Priority      int // 0=normal, 1=high (from recent connections)
}

// AddrManager åœ°å€ç®¡ç†å™¨
//
// è´Ÿè´£ä¸»åŠ¨ç®¡ç†peeråœ°å€çš„ç”Ÿå‘½å‘¨æœŸï¼Œè§£å†³libp2p Peerstoreåœ°å€24å°æ—¶è‡ªåŠ¨è¿‡æœŸé—®é¢˜ã€‚
// æ ¸å¿ƒåŠŸèƒ½ï¼š
// - åˆ†çº§TTLç®¡ç†ï¼šä¸åŒæ¥æºçš„åœ°å€ä½¿ç”¨ä¸åŒçš„ç”Ÿå‘½å‘¨æœŸ
// - ä¸»åŠ¨åˆ·æ–°ï¼šå®šæœŸæ£€æŸ¥å¹¶åˆ·æ–°å³å°†è¿‡æœŸçš„åœ°å€
// - äº‹ä»¶é©±åŠ¨ï¼šæ ¹æ®è¿æ¥çŠ¶æ€å‡çº§/é™çº§åœ°å€TTL
// - æ•…éšœè‡ªæ„ˆï¼šåœ°å€å¤±æ•ˆæ—¶è‡ªåŠ¨è§¦å‘é‡æ–°å‘ç°
// - ä¸»åŠ¨é‡å‘ç°ï¼šæ— åœ°å€peeråŠ å…¥é˜Ÿåˆ—ï¼Œå‘¨æœŸé‡è¯•
type AddrManager struct {
	host      lphost.Host
	peerstore peerstore.Peerstore
	routing   interfaces.RendezvousRouting // ç”¨äºDHTæŸ¥è¯¢
	ttl       AddrTTL
	logger    logiface.Logger

	// åœ°å€åˆ·æ–°çŠ¶æ€
	mu             sync.RWMutex
	lastRefreshAt  map[libpeer.ID]time.Time // è®°å½•æ¯ä¸ªpeerçš„æœ€ååˆ·æ–°æ—¶é—´
	lastSeenAt     map[libpeer.ID]time.Time // è®°å½•æ¯ä¸ªpeerçš„æœ€åâ€œçœ‹è§â€æ—¶é—´ï¼ˆç”¨äºæ·˜æ±°/æœ‰ç•ŒåŒ–ï¼‰
	lastConnectedAt map[libpeer.ID]time.Time // è®°å½•æ¯ä¸ªpeerçš„æœ€è¿‘è¿æ¥æ—¶é—´ï¼ˆç”¨äºåˆ·æ–°ç­–ç•¥ç²¾ç»†åŒ–ï¼‰
	pendingLookups map[libpeer.ID]bool      // æ­£åœ¨æŸ¥è¯¢çš„peerï¼Œé˜²æ­¢é‡å¤æŸ¥è¯¢
	refreshCursor  int                      // refreshAllPeers çš„åˆ†ç‰‡éå†æ¸¸æ ‡ï¼ˆé¿å…æ¯æ¬¡å…¨é‡æ‰«æï¼‰

	// ğŸ†• é‡å‘ç°é˜Ÿåˆ—
	rediscoveryQueue map[libpeer.ID]*PeerRediscoveryInfo
	rediscoveryMu    sync.RWMutex

	// é…ç½®å‚æ•°
	maxConcurrentLookups int           // æœ€å¤§å¹¶å‘æŸ¥è¯¢æ•°
	lookupTimeout        time.Duration // æŸ¥è¯¢è¶…æ—¶æ—¶é—´
	refreshInterval      time.Duration // åˆ·æ–°å‘¨æœŸ
	refreshThreshold     time.Duration // åˆ·æ–°é˜ˆå€¼
	maxTrackedPeers      int           // æœ€å¤§è·Ÿè¸ª peer æ•°ï¼ˆè¶…é™åˆ™æ·˜æ±°ï¼‰
	refreshBudget        int           // æ¯æ¬¡ refresh å‘¨æœŸæœ€å¤šå¤„ç†çš„ peer æ•°ï¼ˆé¿å…å…¨é‡éå†å¼•å‘èµ„æºé£æš´ï¼‰
	maxAddrsPerPeer      int           // æ¯ä¸ª peer æœ€å¤šä¿ç•™çš„åœ°å€æ•°é‡ï¼ˆæ§åˆ¶ peerstore å ç”¨ï¼‰
	maxPendingLookups    int           // pendingLookups ä¸Šé™ï¼ˆé¿å… map æ— ç•Œå¢é•¿ï¼‰
	maxRediscoveryQueue  int           // rediscoveryQueue ä¸Šé™ï¼ˆé¿å…é˜Ÿåˆ—æ— ç•Œå¢é•¿ï¼‰

	// ğŸ†• é‡å‘ç°é…ç½®
	rediscoveryInterval    time.Duration // é‡å‘ç°æ‰«æé—´éš”ï¼ˆé»˜è®¤30sï¼‰
	rediscoveryMaxRetries  int           // æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤10ï¼‰
	rediscoveryBackoffBase time.Duration // é€€é¿åŸºç¡€æ—¶é—´ï¼ˆé»˜è®¤1mï¼‰

	// æŒä¹…åŒ–é…ç½®
	enablePersistence bool
	persistenceBackend string // "badger" | "json"
	badgerDir          string
	namespacePrefix    string
	pruneInterval      time.Duration
	recordTTL          time.Duration

	// lookup å¹¶å‘é™æµï¼ˆé¿å… DHT é£æš´ï¼‰
	lookupSem chan struct{}
	// rediscovery å¹¶å‘é™æµï¼ˆé¿å… goroutine é£æš´ï¼‰
	rediscoverySem chan struct{}

	ctx    context.Context
	cancel context.CancelFunc

	// æŒä¹…åŒ–å­˜å‚¨ï¼ˆBadger/JSONï¼‰
	store AddrStore

	// bootstrap peer é›†åˆï¼ˆç”¨äºæ·˜æ±°ä¿æŠ¤ï¼‰
	bootstrapPeers map[libpeer.ID]struct{}
}

// AddrManagerConfig åœ°å€ç®¡ç†å™¨é…ç½®
type AddrManagerConfig struct {
	TTL                  AddrTTL
	MaxConcurrentLookups int
	LookupTimeout        time.Duration
	RefreshInterval      time.Duration
	RefreshThreshold     time.Duration
	// === æœ‰ç•ŒåŒ–ï¼ˆé˜² OOMï¼‰===
	MaxTrackedPeers     int // æœ€å¤§å¯è·Ÿè¸ª peer æ•°ï¼ˆè¶…é™æ·˜æ±°ï¼Œbootstrap/connected/recent ä¼˜å…ˆä¿ç•™ï¼‰
	RefreshBudget       int // æ¯æ¬¡ refresh å‘¨æœŸæœ€å¤šå¤„ç†çš„ peer æ•°
	MaxAddrsPerPeer     int // æ¯ä¸ª peer æœ€å¤šä¿ç•™åœ°å€æ•°é‡
	MaxPendingLookups   int // pendingLookups map ä¸Šé™
	MaxRediscoveryQueue int // rediscoveryQueue ä¸Šé™
	// ğŸ†• é‡å‘ç°é…ç½®
	RediscoveryInterval    time.Duration // é‡å‘ç°æ‰«æé—´éš”ï¼ˆé»˜è®¤30sï¼‰
	RediscoveryMaxRetries  int           // æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤10ï¼‰
	RediscoveryBackoffBase time.Duration // é€€é¿åŸºç¡€æ—¶é—´ï¼ˆé»˜è®¤1mï¼‰
	// æŒä¹…åŒ–
	EnablePersistence  bool
	PersistenceBackend string        // "badger" | "json"
	BadgerDir          string        // ä¾‹å¦‚ data/p2p/<hostID>/badger
	NamespacePrefix    string        // ä¾‹å¦‚ peer_addrs/v1/
	PruneInterval      time.Duration // ä¾‹å¦‚ 1h
	RecordTTL          time.Duration // ä¾‹å¦‚ 7d
}

// NewAddrManager åˆ›å»ºåœ°å€ç®¡ç†å™¨
func NewAddrManager(h lphost.Host, routing interfaces.RendezvousRouting, cfg AddrManagerConfig, logger logiface.Logger) *AddrManager {
	ctx, cancel := context.WithCancel(context.Background())

	am := &AddrManager{
		host:                   h,
		peerstore:              h.Peerstore(),
		routing:                routing,
		ttl:                    cfg.TTL,
		logger:                 logger,
		lastRefreshAt:          make(map[libpeer.ID]time.Time),
		lastSeenAt:             make(map[libpeer.ID]time.Time),
		lastConnectedAt:        make(map[libpeer.ID]time.Time),
		pendingLookups:         make(map[libpeer.ID]bool),
		rediscoveryQueue:       make(map[libpeer.ID]*PeerRediscoveryInfo),
		maxConcurrentLookups:   cfg.MaxConcurrentLookups,
		lookupTimeout:          cfg.LookupTimeout,
		refreshInterval:        cfg.RefreshInterval,
		refreshThreshold:       cfg.RefreshThreshold,
		maxTrackedPeers:        cfg.MaxTrackedPeers,
		refreshBudget:          cfg.RefreshBudget,
		maxAddrsPerPeer:        cfg.MaxAddrsPerPeer,
		maxPendingLookups:      cfg.MaxPendingLookups,
		maxRediscoveryQueue:    cfg.MaxRediscoveryQueue,
		rediscoveryInterval:    cfg.RediscoveryInterval,
		rediscoveryMaxRetries:  cfg.RediscoveryMaxRetries,
		rediscoveryBackoffBase: cfg.RediscoveryBackoffBase,
		enablePersistence:      cfg.EnablePersistence,
		persistenceBackend:     cfg.PersistenceBackend,
		badgerDir:              cfg.BadgerDir,
		namespacePrefix:        cfg.NamespacePrefix,
		pruneInterval:          cfg.PruneInterval,
		recordTTL:              cfg.RecordTTL,
		ctx:                    ctx,
		cancel:                 cancel,
		bootstrapPeers:         make(map[libpeer.ID]struct{}),
	}
	
	// è®¾ç½®é‡å‘ç°é»˜è®¤å€¼
	if am.rediscoveryInterval == 0 {
		am.rediscoveryInterval = 30 * time.Second
	}
	if am.rediscoveryMaxRetries == 0 {
		am.rediscoveryMaxRetries = 10
	}
	if am.rediscoveryBackoffBase == 0 {
		am.rediscoveryBackoffBase = 1 * time.Minute
	}

	// åˆå§‹åŒ– lookup semaphore
	if am.maxConcurrentLookups <= 0 {
		am.maxConcurrentLookups = 10
	}
	am.lookupSem = make(chan struct{}, am.maxConcurrentLookups)

	// ğŸ†• åˆå§‹åŒ– rediscovery semaphoreï¼ˆç‹¬ç«‹çš„å¹¶å‘é™åˆ¶ï¼Œé»˜è®¤5ï¼Œé¿å… goroutine é£æš´ï¼‰
	rediscoveryMaxConcurrent := 5
	if cfg.MaxConcurrentLookups > 0 && cfg.MaxConcurrentLookups < 5 {
		rediscoveryMaxConcurrent = cfg.MaxConcurrentLookups
	}
	am.rediscoverySem = make(chan struct{}, rediscoveryMaxConcurrent)

	// ğŸ†• P1 ä¿®å¤ï¼šä¼˜åŒ–æœ‰ç•ŒåŒ–é»˜è®¤å€¼ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
	// æ ¹æ®é˜¿é‡Œäº‘èŠ‚ç‚¹åˆ†ææŠ¥å‘Šï¼ˆ20,087 å¯¹è±¡ / 41.1MBï¼‰ï¼Œå°†é»˜è®¤å€¼è°ƒæ•´ä¸ºæ›´ä¿å®ˆçš„å€¼
	// å‚è€ƒï¼šæœ¬åœ°ç¨³å®šèŠ‚ç‚¹çº¦ 6,000 å¯¹è±¡ / 11.8MB
	if am.maxTrackedPeers <= 0 {
		am.maxTrackedPeers = 5000 // åŸ 20000 â†’ 5000ï¼Œå‡å°‘ 75%
	}
	if am.refreshBudget <= 0 {
		am.refreshBudget = 500 // åŸ 1000 â†’ 500ï¼Œå‡å°‘æ¯æ¬¡ refresh çš„ peer æ•°
	}
	if am.maxAddrsPerPeer <= 0 {
		am.maxAddrsPerPeer = 10 // åŸ 8 â†’ 10ï¼Œæ¯ä¸ª peer æœ€å¤š 10 ä¸ªåœ°å€
	}
	if am.maxPendingLookups <= 0 {
		am.maxPendingLookups = 5000 // åŸ 20000 â†’ 5000ï¼Œå‡å°‘ 75%
	}
	// ğŸ†• ä¼˜åŒ–ï¼šå°†é˜Ÿåˆ—å¤§å°ä»10000é™ä½åˆ°50ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
	if am.maxRediscoveryQueue <= 0 {
		am.maxRediscoveryQueue = 50
	}

	// åˆå§‹åŒ–æŒä¹…åŒ– storeï¼ˆä¸“ç”¨ BadgerDB / JSONï¼‰
	if am.enablePersistence {
		switch strings.TrimSpace(strings.ToLower(am.persistenceBackend)) {
		case "", "badger":
			s, err := newBadgerAddrStore(badgerAddrStoreConfig{
				Dir:             am.badgerDir,
				NamespacePrefix: am.namespacePrefix,
			}, logger)
			if err != nil {
				if logger != nil {
					logger.Errorf("addr_manager badger store init failed: %v", err)
				}
			} else {
				am.store = s
				am.loadPersistedRecords()
			}
		case "json":
			// TODO: å¦‚ç¡®éœ€ JSON åç«¯ï¼Œå¯å®ç° json store é€‚é…å™¨
			if logger != nil {
				logger.Warnf("addr_manager persistence backend=json not implemented, skipping persistence")
			}
		default:
			if logger != nil {
				logger.Warnf("addr_manager unknown persistence backend=%s, skipping persistence", am.persistenceBackend)
			}
		}
	}

	return am
}

// ModuleName å®ç° MemoryReporterï¼ˆç”¨äº MemoryDoctor é‡‡æ ·ï¼‰
func (am *AddrManager) ModuleName() string {
	return "p2p.addr_manager"
}

// CollectMemoryStats å®ç° MemoryReporterï¼ˆç”¨äº MemoryDoctor é‡‡æ ·ï¼‰
func (am *AddrManager) CollectMemoryStats() metricsiface.ModuleMemoryStats {
	// ä¼°ç®—ï¼špeerstore è§„æ¨¡ä¸åœ°å€æ•°é‡æ˜¯ä¸»è¦é©±åŠ¨ã€‚
	//
	// æ³¨æ„ï¼š
	// - è¿™é‡Œä¸ä½¿ç”¨â€œæ¯ peer å›ºå®š X KBâ€çš„æ‹è„‘è¢‹å¸¸æ•°ï¼ˆä¼šè¯¯å¯¼åˆ†æï¼‰ï¼Œè€Œæ˜¯å¯¹ peerstore åšå°æ ·æœ¬é‡‡æ ·ï¼Œ
	//   ç”¨çœŸå®çš„ peerID / addr å­—ç¬¦ä¸²é•¿åº¦å¾—åˆ°æ¯ peer çš„å¹³å‡å ç”¨ï¼Œå†æŒ‰æ€» peer æ•°æ”¾å¤§ã€‚
	// - ä¼°ç®—åªç”¨äºè¶‹åŠ¿è§‚å¯Ÿï¼Œä¸è¿½æ±‚ç»å¯¹ç²¾ç¡®ã€‚
	peerCount := 0
	if am != nil && am.peerstore != nil {
		peerCount = len(am.peerstore.Peers())
	}
	pending := 0
	if am != nil {
		am.mu.RLock()
		pending = len(am.pendingLookups)
		am.mu.RUnlock()
	}
	rediscovery := 0
	if am != nil {
		rediscovery = am.GetRediscoveryQueueSize()
	}

	approx := int64(0)
	if am != nil && am.peerstore != nil && peerCount > 0 {
		peers := am.peerstore.Peers()
		// é‡‡æ ·ä¸Šé™ï¼šé¿å… MemoryDoctor é‡‡æ ·æ—¶å¯¹è¶…å¤§ peerstore é€ æˆæ˜æ˜¾å¼€é”€
		sampleN := peerCount
		if sampleN > 50 {
			sampleN = 50
		}
		var totalBytes int64
		for i := 0; i < sampleN; i++ {
			pid := peers[i]
			// peerID å­—ç¬¦ä¸²é•¿åº¦ï¼ˆè¿‘ä¼¼è¡¨ç¤ºå…¶åœ¨å†…å­˜ä¸­çš„ payloadï¼‰
			totalBytes += int64(len(pid))
			// åœ°å€å­—ç¬¦ä¸²é•¿åº¦ï¼ˆaddr payloadï¼‰
			for _, a := range am.peerstore.Addrs(pid) {
				if a == nil {
					continue
				}
				totalBytes += int64(len(a.String()))
			}
		}
		avgBytesPerPeer := float64(totalBytes) / float64(sampleN)
		approx = int64(avgBytesPerPeer * float64(peerCount))
	}
	return metricsiface.ModuleMemoryStats{
		Module:      "p2p.addr_manager",
		Layer:       "L2-Infrastructure",
		Objects:     int64(peerCount),
		ApproxBytes: approx,
		CacheItems:  int64(pending),
		QueueLength: int64(rediscovery),
	}
}

// capAddrs é™åˆ¶å•ä¸ª peer çš„åœ°å€æ•°é‡ï¼Œé¿å… peerstore å ç”¨æ— ç•Œå¢é•¿
func (am *AddrManager) capAddrs(addrs []ma.Multiaddr) []ma.Multiaddr {
	if am == nil || am.maxAddrsPerPeer <= 0 || len(addrs) <= am.maxAddrsPerPeer {
		return addrs
	}
	// å»é‡å¹¶æŒ‰â€œå¯æ‹¨å·ä¼˜å…ˆâ€ç®€å•æ’åºï¼špublic/relay ä¼˜å…ˆï¼Œå…¶æ¬¡ privateï¼Œæœ€åå…¶ä»–
	type bucket struct {
		pub   []ma.Multiaddr
		priv  []ma.Multiaddr
		other []ma.Multiaddr
	}
	seen := make(map[string]struct{}, len(addrs))
	var b bucket
	for _, a := range addrs {
		if a == nil {
			continue
		}
		s := a.String()
		if _, ok := seen[s]; ok {
			continue
		}
		seen[s] = struct{}{}
		isRelay := false
		for _, p := range a.Protocols() {
			if p.Name == "p2p-circuit" {
				isRelay = true
				break
			}
		}
		if isRelay {
			b.pub = append(b.pub, a)
			continue
		}
		if ip, err := manet.ToIP(a); err == nil && ip != nil {
			if ip.IsPrivate() {
				b.priv = append(b.priv, a)
			} else {
				b.pub = append(b.pub, a)
			}
			continue
		}
		b.other = append(b.other, a)
	}
	out := make([]ma.Multiaddr, 0, am.maxAddrsPerPeer)
	appendUpTo := func(src []ma.Multiaddr) {
		for _, a := range src {
			if len(out) >= am.maxAddrsPerPeer {
				return
			}
			out = append(out, a)
		}
	}
	appendUpTo(b.pub)
	appendUpTo(b.priv)
	appendUpTo(b.other)
	return out
}

func (am *AddrManager) markSeenLocked(id libpeer.ID, now time.Time) {
	am.lastSeenAt[id] = now
	am.lastRefreshAt[id] = now
}

// enforceBounds åœ¨ refresh å‘¨æœŸå†…åšè½»é‡æœ‰ç•ŒåŒ–ï¼šè¶…é™æ·˜æ±° + å…³é”® map æ¸…ç†
func (am *AddrManager) enforceBounds() {
	if am == nil || am.peerstore == nil {
		return
	}
	if am.maxTrackedPeers <= 0 {
		return
	}
	peers := am.peerstore.Peers()
	if len(peers) <= am.maxTrackedPeers {
		return
	}

	type cand struct {
		id        libpeer.ID
		seenAt    time.Time
		connected bool
	}
	now := time.Now()
	cands := make([]cand, 0, len(peers))
	for _, p := range peers {
		if p == "" || p == am.host.ID() {
			continue
		}
		if am.isBootstrapPeer(p) {
			continue
		}
		connected := false
		if am.host != nil && am.host.Network().Connectedness(p) == libnetwork.Connected {
			connected = true
		}
		if connected {
			continue
		}
		am.mu.RLock()
		seenAt := am.lastSeenAt[p]
		lastConn := am.lastConnectedAt[p]
		am.mu.RUnlock()
		// è¿‘æœŸè¿æ¥è¿‡çš„ä¼˜å…ˆä¿ç•™
		if !lastConn.IsZero() && now.Sub(lastConn) < am.ttl.Connected {
			continue
		}
		cands = append(cands, cand{id: p, seenAt: seenAt, connected: connected})
	}

	needEvict := len(peers) - am.maxTrackedPeers
	if needEvict <= 0 || len(cands) == 0 {
		return
	}

	// seenAt è¶Šæ—©è¶Šå…ˆæ·˜æ±°ï¼›æ²¡æœ‰ seenAt çš„è®¤ä¸ºæœ€è€
	sort.Slice(cands, func(i, j int) bool {
		a := cands[i].seenAt
		b := cands[j].seenAt
		if a.IsZero() && !b.IsZero() {
			return true
		}
		if !a.IsZero() && b.IsZero() {
			return false
		}
		return a.Before(b)
	})

	if needEvict > len(cands) {
		needEvict = len(cands)
	}

	ctx, cancel := context.WithTimeout(context.Background(), 3*time.Second)
	defer cancel()

	evicted := 0
	for i := 0; i < needEvict; i++ {
		id := cands[i].id
		// æ¸…ç† peerstoreï¼ˆå°½å¯èƒ½é‡Šæ”¾åœ°å€ä¸å…ƒæ•°æ®ï¼‰
		am.peerstore.ClearAddrs(id)
		am.peerstore.RemovePeer(id)

		am.mu.Lock()
		delete(am.lastRefreshAt, id)
		delete(am.lastSeenAt, id)
		delete(am.lastConnectedAt, id)
		delete(am.pendingLookups, id)
		am.mu.Unlock()

		am.rediscoveryMu.Lock()
		delete(am.rediscoveryQueue, id)
		am.rediscoveryMu.Unlock()

		if am.store != nil {
			_ = am.store.Delete(ctx, id.String())
		}
		evicted++
	}

	if evicted > 0 && am.logger != nil {
		am.logger.Warnf("addr_manager bounded_peerstore evicted=%d current_peers=%d max=%d", evicted, len(peers)-evicted, am.maxTrackedPeers)
	}
}

func (am *AddrManager) isBootstrapPeer(id libpeer.ID) bool {
	if am == nil {
		return false
	}
	am.mu.RLock()
	_, ok := am.bootstrapPeers[id]
	am.mu.RUnlock()
	return ok
}

// Start å¯åŠ¨åœ°å€ç®¡ç†å™¨
func (am *AddrManager) Start() {
	if am.logger != nil {
		am.logger.Infof("addr_manager starting")
	}

	// å¯åŠ¨ä¸»åŠ¨åˆ·æ–°goroutine
	go am.refreshLoop()

	// ç›‘å¬è¿æ¥äº‹ä»¶
	go am.handleConnectionEvents()

	// å¯åŠ¨ prune å¾ªç¯ï¼ˆall_discovered åœºæ™¯å¿…é¡»ï¼‰
	if am.store != nil && am.pruneInterval > 0 && am.recordTTL > 0 {
		go am.pruneLoop()
	}

	// ğŸ†• å¯åŠ¨é‡å‘ç°å¾ªç¯
	go am.rediscoveryLoop()

	if am.logger != nil {
		am.logger.Infof("addr_manager started with rediscovery enabled")
	}
}

// Stop åœæ­¢åœ°å€ç®¡ç†å™¨
func (am *AddrManager) Stop() {
	if am.logger != nil {
		am.logger.Infof("addr_manager stopping")
	}

	am.cancel()

	// å…³é—­æŒä¹…åŒ– store
	if am.store != nil {
		_ = am.store.Close()
		am.store = nil
	}

	if am.logger != nil {
		am.logger.Infof("addr_manager stopped")
	}
}

// AddDHTAddr æ·»åŠ DHTå‘ç°çš„åœ°å€
func (am *AddrManager) AddDHTAddr(id libpeer.ID, addrs []ma.Multiaddr) {
	if len(addrs) == 0 {
		return
	}

	addrs = am.capAddrs(addrs)
	am.peerstore.AddAddrs(id, addrs, am.ttl.DHT)

	now := time.Now()
	am.mu.Lock()
	am.markSeenLocked(id, now)
	am.mu.Unlock()

	if am.logger != nil {
		am.logger.Debugf("addr_manager add_dht_addr peer=%s addrs=%d ttl=%s",
			id.String(), len(addrs), am.ttl.DHT)
	}

	am.upsertPeerRecord(id.String(), func(r *PeerAddrRecord) {
		r.LastSeenAt = now
		r.Addrs = mergeStringAddrs(r.Addrs, addrs)
	})
}

// AddConnectedAddr æ·»åŠ è¿æ¥æˆåŠŸçš„åœ°å€ï¼ˆå‡çº§TTLï¼‰
func (am *AddrManager) AddConnectedAddr(id libpeer.ID, addrs []ma.Multiaddr) {
	if len(addrs) == 0 {
		return
	}

	addrs = am.capAddrs(addrs)
	// è¿æ¥æˆåŠŸï¼Œå‡çº§TTLåˆ°24å°æ—¶
	am.peerstore.AddAddrs(id, addrs, am.ttl.Connected)

	now := time.Now()
	am.mu.Lock()
	am.markSeenLocked(id, now)
	am.lastConnectedAt[id] = now
	am.mu.Unlock()

	if am.logger != nil {
		am.logger.Debugf("addr_manager add_connected_addr peer=%s addrs=%d ttl=%s",
			id.String(), len(addrs), am.ttl.Connected)
	}

	am.upsertPeerRecord(id.String(), func(r *PeerAddrRecord) {
		r.LastSeenAt = now
		r.LastConnectedAt = now
		r.SuccessCount++
		r.Addrs = mergeStringAddrs(r.Addrs, addrs)
	})
}

// AddBootstrapAddr æ·»åŠ BootstrapèŠ‚ç‚¹åœ°å€ï¼ˆæ°¸ä¹…ä¿å­˜ï¼‰
func (am *AddrManager) AddBootstrapAddr(id libpeer.ID, addrs []ma.Multiaddr) {
	if len(addrs) == 0 {
		return
	}

	addrs = am.capAddrs(addrs)
	// BootstrapèŠ‚ç‚¹ä½¿ç”¨æ°¸ä¹…TTL
	am.peerstore.AddAddrs(id, addrs, am.ttl.Bootstrap)

	if am.logger != nil {
		am.logger.Debugf("addr_manager add_bootstrap_addr peer=%s addrs=%d",
			id.String(), len(addrs))
	}

	now := time.Now()
	am.mu.Lock()
	am.bootstrapPeers[id] = struct{}{}
	am.markSeenLocked(id, now)
	am.mu.Unlock()
	am.upsertPeerRecord(id.String(), func(r *PeerAddrRecord) {
		r.IsBootstrap = true
		r.LastSeenAt = now
		r.Addrs = mergeStringAddrs(r.Addrs, addrs)
	})
}

// MarkAddrFailed æ ‡è®°åœ°å€è¿æ¥å¤±è´¥ï¼ˆé™çº§TTLï¼‰
func (am *AddrManager) MarkAddrFailed(id libpeer.ID) {
	// è·å–ç°æœ‰åœ°å€
	addrs := am.peerstore.Addrs(id)
	if len(addrs) == 0 {
		return
	}

	// é™ä½TTLåˆ°5åˆ†é’Ÿ
	am.peerstore.AddAddrs(id, addrs, am.ttl.Failed)

	if am.logger != nil {
		am.logger.Debugf("addr_manager mark_failed peer=%s ttl=%s",
			id.String(), am.ttl.Failed)
	}

	now := time.Now()
	am.mu.Lock()
	am.lastSeenAt[id] = now
	am.mu.Unlock()
	am.upsertPeerRecord(id.String(), func(r *PeerAddrRecord) {
		r.LastSeenAt = now
		r.LastFailedAt = now
		r.FailCount++
	})
}

// GetAddrs è·å–peeråœ°å€ï¼ˆå¦‚æœæ— åœ°å€ï¼Œè§¦å‘é‡å‘ç°ï¼‰
func (am *AddrManager) GetAddrs(id libpeer.ID) []ma.Multiaddr {
	addrs := am.peerstore.Addrs(id)

	if len(addrs) == 0 {
		// ğŸ†• ä¼˜åŒ–ï¼šæ— åœ°å€æ—¶ï¼Œé™¤äº†è§¦å‘æŸ¥è¯¢ï¼Œè¿˜åŠ å…¥é‡å‘ç°é˜Ÿåˆ—
		am.triggerAddrLookup(id)
		am.TriggerRediscovery(id, false) // normal priority
	}

	return addrs
}

// triggerAddrLookup è§¦å‘åœ°å€æŸ¥è¯¢ï¼ˆå¼‚æ­¥ï¼Œé˜²é‡å¤ï¼‰
func (am *AddrManager) triggerAddrLookup(id libpeer.ID) {
	am.mu.Lock()
	// pendingLookups æœ‰ç•ŒåŒ–ï¼šé˜²æ­¢æç«¯æƒ…å†µä¸‹ map æ— ç•Œå¢é•¿ï¼ˆæ¯”å¦‚ refresh é£æš´ï¼‰
	if am.maxPendingLookups > 0 && len(am.pendingLookups) >= am.maxPendingLookups {
		am.mu.Unlock()
		if am.logger != nil {
			am.logger.Warnf("addr_manager pending_lookups_full drop peer=%s size=%d max=%d",
				id.String(), len(am.pendingLookups), am.maxPendingLookups)
		}
		return
	}

	// æ£€æŸ¥æ˜¯å¦å·²åœ¨æŸ¥è¯¢ä¸­
	if am.pendingLookups[id] {
		am.mu.Unlock()
		return
	}

	am.pendingLookups[id] = true
	am.mu.Unlock()

	if am.logger != nil {
		am.logger.Warnf("addr_manager trigger_lookup peer=%s", id.String())
	}

	// å¹¶å‘é™æµï¼šé¿å… refresh/all_discovered åœºæ™¯æŠŠ DHT æ‰“çˆ†
	select {
	case am.lookupSem <- struct{}{}:
		// acquired
	default:
		if am.logger != nil {
			am.logger.Warnf("addr_manager lookup_throttled peer=%s max_concurrent=%d", id.String(), am.maxConcurrentLookups)
		}
		am.mu.Lock()
		delete(am.pendingLookups, id)
		am.mu.Unlock()
		return
	}

	// å¼‚æ­¥æŸ¥è¯¢
	go func() {
		defer func() {
			am.mu.Lock()
			delete(am.pendingLookups, id)
			am.mu.Unlock()
			<-am.lookupSem
		}()

		// æ£€æŸ¥æ˜¯å¦æœ‰routingå¯ç”¨
		if am.routing == nil {
			if am.logger != nil {
				am.logger.Warnf("addr_manager lookup_skipped peer=%s reason=no_routing", id.String())
			}
			return
		}

		ctx, cancel := context.WithTimeout(am.ctx, am.lookupTimeout)
		defer cancel()

		// é€šè¿‡DHTæŸ¥è¯¢peeråœ°å€
		if am.logger != nil {
			am.logger.Debugf("addr_manager lookup_start peer=%s", id.String())
		}

	info, err := am.routing.FindPeer(ctx, id)
	if err != nil {
		// ğŸ†• P0-009: å®¹é”™â€”â€”å³ä½¿ FindPeer å¤±è´¥ï¼Œåªè¦å½“å‰ peerstore ä»æœ‰åœ°å€ï¼Œä¹Ÿç»™äºˆâ€œå®½é™æœŸâ€ç»­æœŸï¼Œ
		// é¿å…åœ°å€æŒ‰åŸ TTL ç›´æ¥è¿‡æœŸï¼Œå¯¼è‡´åç»­å‡ºç° addrs=0 -> æ— æ³•é‡è¿ -> ç½‘ç»œå­¤å²›ã€‚
		if existing := am.peerstore.Addrs(id); len(existing) > 0 {
			graceTTL := 30 * time.Minute
			existing = am.capAddrs(existing)
			am.peerstore.AddAddrs(id, existing, graceTTL)
			if am.logger != nil {
				am.logger.Debugf("addr_manager lookup_failed_grace_extended peer=%s grace_ttl=%s addrs=%d",
					id.String(), graceTTL, len(existing))
			}
		}

		if am.logger != nil {
			// "routing: not found" æ˜¯æ­£å¸¸çš„ P2P ç½‘ç»œè¡Œä¸ºï¼ˆèŠ‚ç‚¹ç¦»çº¿/æœªå¹¿æ’­ï¼‰ï¼Œé™çº§ä¸º DEBUG
			// å…¶ä»–é”™è¯¯ï¼ˆå¦‚ç½‘ç»œæ•…éšœã€è¶…æ—¶ç­‰ï¼‰ä»è®°å½•ä¸º WARN
			if err.Error() == "routing: not found" {
				am.logger.Debugf("addr_manager lookup_not_in_dht peer=%s", id.String())
			} else {
				am.logger.Warnf("addr_manager lookup_failed peer=%s err=%v", id.String(), err)
			}
		}
		return
	}

		if len(info.Addrs) > 0 {
			am.AddDHTAddr(info.ID, info.Addrs)
			if am.logger != nil {
				am.logger.Infof("addr_manager lookup_success peer=%s addrs=%d", id.String(), len(info.Addrs))
			}
		} else {
			if am.logger != nil {
				am.logger.Warnf("addr_manager lookup_no_addrs peer=%s", id.String())
			}
		}
	}()
}

// loadPersistedRecords å¯åŠ¨æ—¶åŠ è½½æŒä¹…åŒ–è®°å½•å¹¶å›å¡«åˆ° peerstore
func (am *AddrManager) loadPersistedRecords() {
	if am.store == nil {
		return
	}
	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()

	recs, err := am.store.LoadAll(ctx)
	if err != nil {
		if am.logger != nil {
			am.logger.Warnf("addr_manager load_persisted_records failed: %v", err)
		}
		return
	}

	now := time.Now()
	loaded := 0
	for _, r := range recs {
		if r == nil || strings.TrimSpace(r.PeerID) == "" {
			continue
		}
		// TTL è¿‡æœŸè®°å½•è·³è¿‡ï¼ˆpruneLoop ä¼šæ¸…ç†ï¼‰
		if !r.IsBootstrap && am.recordTTL > 0 && !r.LastSeenAt.IsZero() && now.Sub(r.LastSeenAt) > am.recordTTL {
			continue
		}
		id, err := libpeer.Decode(r.PeerID)
		if err != nil || id == "" {
			continue
		}
		addrs := make([]ma.Multiaddr, 0, len(r.Addrs))
		for _, s := range r.Addrs {
			a, err := ma.NewMultiaddr(s)
			if err != nil {
				continue
			}
			addrs = append(addrs, a)
		}
		if len(addrs) == 0 {
			continue
		}

		// å›å¡«
		addrs = am.capAddrs(addrs)
		if r.IsBootstrap {
			am.peerstore.AddAddrs(id, addrs, am.ttl.Bootstrap)
			am.mu.Lock()
			am.bootstrapPeers[id] = struct{}{}
			am.mu.Unlock()
		} else if !r.LastConnectedAt.IsZero() {
			am.peerstore.AddAddrs(id, addrs, am.ttl.Connected)
			am.mu.Lock()
			am.lastConnectedAt[id] = r.LastConnectedAt
			am.mu.Unlock()
		} else {
			am.peerstore.AddAddrs(id, addrs, am.ttl.DHT)
		}
		am.mu.Lock()
		if !r.LastSeenAt.IsZero() {
			am.lastRefreshAt[id] = r.LastSeenAt
			am.lastSeenAt[id] = r.LastSeenAt
		} else {
			am.lastRefreshAt[id] = now
			am.lastSeenAt[id] = now
		}
		am.mu.Unlock()
		loaded++
	}

	if am.logger != nil {
		am.logger.Infof("addr_manager loaded_persisted_records count=%d", loaded)
	}
}

func (am *AddrManager) upsertPeerRecord(peerID string, mutate func(r *PeerAddrRecord)) {
	if am.store == nil || strings.TrimSpace(peerID) == "" {
		return
	}
	ctx, cancel := context.WithTimeout(context.Background(), 2*time.Second)
	defer cancel()

	rec, ok, err := am.store.Get(ctx, peerID)
	if err != nil {
		if am.logger != nil {
			am.logger.Warnf("addr_manager store_get failed peer=%s err=%v", peerID, err)
		}
		return
	}
	if !ok || rec == nil {
		rec = &PeerAddrRecord{Version: PeerAddrRecordVersion, PeerID: peerID}
	}
	if rec.Version == 0 {
		rec.Version = PeerAddrRecordVersion
	}
	mutate(rec)
	_ = am.store.Upsert(ctx, rec)
}

func mergeStringAddrs(existing []string, addrs []ma.Multiaddr) []string {
	if len(addrs) == 0 {
		return existing
	}
	seen := make(map[string]struct{}, len(existing)+len(addrs))
	out := make([]string, 0, len(existing)+len(addrs))
	for _, s := range existing {
		if strings.TrimSpace(s) == "" {
			continue
		}
		if _, ok := seen[s]; ok {
			continue
		}
		seen[s] = struct{}{}
		out = append(out, s)
	}
	for _, a := range addrs {
		if a == nil {
			continue
		}
		s := a.String()
		if _, ok := seen[s]; ok {
			continue
		}
		seen[s] = struct{}{}
		out = append(out, s)
	}
	return out
}

// pruneLoop å®šæœŸæ¸…ç†è¿‡æœŸ/åŠ£è´¨è®°å½•ï¼ˆall_discovered åœºæ™¯å¿…é¡»ï¼‰
func (am *AddrManager) pruneLoop() {
	ticker := time.NewTicker(am.pruneInterval)
	defer ticker.Stop()

	if am.logger != nil {
		am.logger.Infof("addr_manager prune_loop started interval=%s ttl=%s", am.pruneInterval, am.recordTTL)
	}

	for {
		select {
		case <-am.ctx.Done():
			if am.logger != nil {
				am.logger.Infof("addr_manager prune_loop stopped")
			}
			return
		case <-ticker.C:
			am.pruneOnce()
		}
	}
}

// MaxAddrManagerMemoryBytes åœ°å€ç®¡ç†å™¨æœ€å¤§å†…å­˜å ç”¨ï¼ˆ15MBï¼‰
// è¶…è¿‡æ­¤å€¼ä¼šè§¦å‘å¼ºåˆ¶æ·˜æ±°
const MaxAddrManagerMemoryBytes = 15 * 1024 * 1024

func (am *AddrManager) pruneOnce() {
	if am.store == nil || am.recordTTL <= 0 {
		return
	}
	ctx, cancel := context.WithTimeout(context.Background(), 60*time.Second)
	defer cancel()

	recs, err := am.store.LoadAll(ctx)
	if err != nil {
		if am.logger != nil {
			am.logger.Warnf("addr_manager prune_load_all failed: %v", err)
		}
		return
	}

	now := time.Now()
	deleted := 0

	// ğŸ†• P1 ä¿®å¤ï¼šé˜¶æ®µ1 - æ¸…ç†è¿‡æœŸå’Œå¤±è´¥è®°å½•
	for _, r := range recs {
		if r == nil || r.IsBootstrap {
			continue
		}
		expired := !r.LastSeenAt.IsZero() && now.Sub(r.LastSeenAt) > am.recordTTL
		tooManyFails := r.FailCount >= 50 && (r.LastConnectedAt.IsZero() || now.Sub(r.LastConnectedAt) > 24*time.Hour)
		if expired || tooManyFails {
			_ = am.store.Delete(ctx, r.PeerID)
			deleted++
		}
	}

	// ğŸ†• P1 ä¿®å¤ï¼šé˜¶æ®µ2 - å†…å­˜ä¸Šé™æ£€æŸ¥å’Œ LRU æ·˜æ±°
	// é‡æ–°åŠ è½½å‰©ä½™è®°å½•
	recs, err = am.store.LoadAll(ctx)
	if err == nil {
		am.pruneByMemoryLimit(ctx, recs, now)
	}

	if deleted > 0 && am.logger != nil {
		am.logger.Infof("addr_manager prune_done deleted=%d total=%d", deleted, len(recs))
	}
}

// pruneByMemoryLimit æ ¹æ®å†…å­˜ä¸Šé™æ·˜æ±°è®°å½•
// å½“è®°å½•æ•°è¶…è¿‡ maxTrackedPeers æˆ–ä¼°ç®—å†…å­˜è¶…è¿‡ MaxAddrManagerMemoryBytes æ—¶è§¦å‘ LRU æ·˜æ±°
func (am *AddrManager) pruneByMemoryLimit(ctx context.Context, recs []*PeerAddrRecord, now time.Time) {
	// maxTrackedPeers æœªé…ç½®æ—¶ä½¿ç”¨é»˜è®¤å€¼ï¼Œé¿å…æµ‹è¯•/éæ ‡å‡†æ„é€ å¯¼è‡´â€œæ‰€æœ‰è®°å½•è¢«è¯¯åˆ â€
	effectiveMaxTrackedPeers := am.maxTrackedPeers
	if effectiveMaxTrackedPeers <= 0 {
		effectiveMaxTrackedPeers = 5000
	}

	// ç»Ÿè®¡é bootstrap è®°å½•æ•°é‡
	nonBootstrapCount := 0
	for _, r := range recs {
		if r != nil && !r.IsBootstrap {
			nonBootstrapCount++
		}
	}

	// ä¼°ç®—å½“å‰å†…å­˜å ç”¨ï¼ˆæ¯ä¸ªè®°å½•çº¦ 2KBï¼‰
	estimatedMemory := int64(nonBootstrapCount) * 2 * 1024

	// åˆ¤æ–­æ˜¯å¦éœ€è¦æ·˜æ±°
	needPrune := false
	var pruneReason string

	if nonBootstrapCount > effectiveMaxTrackedPeers {
		needPrune = true
		pruneReason = "records_exceed_max"
	} else if estimatedMemory > MaxAddrManagerMemoryBytes {
		needPrune = true
		pruneReason = "memory_exceed_limit"
	}

	if !needPrune {
		return
	}

	// è®¡ç®—éœ€è¦æ·˜æ±°çš„æ•°é‡
	// ç›®æ ‡ï¼šé™åˆ° maxTrackedPeers çš„ 80% æˆ–å†…å­˜ä¸Šé™çš„ 80%
	targetCount := int(float64(effectiveMaxTrackedPeers) * 0.8)
	targetMemory := int64(float64(MaxAddrManagerMemoryBytes) * 0.8)
	targetByMemory := int(targetMemory / (2 * 1024))

	if targetByMemory < targetCount {
		targetCount = targetByMemory
	}

	needEvict := nonBootstrapCount - targetCount
	if needEvict <= 0 {
		return
	}

	// ğŸ†• LRU æ·˜æ±°ï¼šæŒ‰ LastSeenAt æ’åºï¼Œæ·˜æ±°æœ€ä¹…æœªè§çš„
	type candidate struct {
		peerID   string
		lastSeen time.Time
	}

	candidates := make([]candidate, 0, nonBootstrapCount)
	for _, r := range recs {
		if r == nil || r.IsBootstrap {
			continue
		}
		// è·³è¿‡è¿‘æœŸè¿æ¥è¿‡çš„
		if !r.LastConnectedAt.IsZero() && now.Sub(r.LastConnectedAt) < 24*time.Hour {
			continue
		}
		candidates = append(candidates, candidate{
			peerID:   r.PeerID,
			lastSeen: r.LastSeenAt,
		})
	}

	// æŒ‰ LastSeenAt å‡åºæ’åºï¼ˆæœ€è€çš„åœ¨å‰é¢ï¼‰
	sort.Slice(candidates, func(i, j int) bool {
		if candidates[i].lastSeen.IsZero() && !candidates[j].lastSeen.IsZero() {
			return true
		}
		if !candidates[i].lastSeen.IsZero() && candidates[j].lastSeen.IsZero() {
			return false
		}
		return candidates[i].lastSeen.Before(candidates[j].lastSeen)
	})

	// æ‰§è¡Œæ·˜æ±°
	evicted := 0
	for i := 0; i < needEvict && i < len(candidates); i++ {
		_ = am.store.Delete(ctx, candidates[i].peerID)
		evicted++
	}

	if evicted > 0 && am.logger != nil {
		am.logger.Warnf("addr_manager prune_by_memory_limit reason=%s evicted=%d target_count=%d estimated_memory_mb=%.1f",
			pruneReason, evicted, targetCount, float64(estimatedMemory)/(1024*1024))
	}
}

// handleConnectionEvents å¤„ç†è¿æ¥äº‹ä»¶ï¼ˆåœ¨addr_refresh.goä¸­å®ç°ï¼‰
func (am *AddrManager) handleConnectionEvents() {
	// è®¢é˜…libp2pè¿æ¥äº‹ä»¶
	sub, err := am.host.EventBus().Subscribe(new(libevent.EvtPeerConnectednessChanged))
	if err != nil {
		if am.logger != nil {
			am.logger.Errorf("addr_manager subscribe_events failed: %v", err)
		}
		return
	}
	defer sub.Close()

	if am.logger != nil {
		am.logger.Infof("addr_manager event_handler started")
	}

	for {
		select {
		case <-am.ctx.Done():
			if am.logger != nil {
				am.logger.Infof("addr_manager event_handler stopped")
			}
			return

		case e := <-sub.Out():
			evt, ok := e.(libevent.EvtPeerConnectednessChanged)
			if !ok {
				continue
			}
			am.handleConnectednessChange(evt)
		}
	}
}

// handleConnectednessChange å¤„ç†è¿æ¥çŠ¶æ€å˜åŒ–
func (am *AddrManager) handleConnectednessChange(evt libevent.EvtPeerConnectednessChanged) {
	switch evt.Connectedness {
	case libnetwork.Connected:
		// è¿æ¥æˆåŠŸï¼Œå‡çº§åœ°å€TTL
		addrs := am.peerstore.Addrs(evt.Peer)
		if len(addrs) > 0 {
			am.AddConnectedAddr(evt.Peer, addrs)
		}

	case libnetwork.NotConnected:
		// è¿æ¥æ–­å¼€ï¼Œä¿æŒç°æœ‰TTLï¼ˆä¸é™çº§ï¼Œå…è®¸é‡è¿ï¼‰
		if am.logger != nil {
			am.logger.Debugf("addr_manager peer_disconnected peer=%s", evt.Peer.String())
		}

	case libnetwork.CannotConnect:
		// æ— æ³•è¿æ¥ï¼Œé™çº§TTL
		am.MarkAddrFailed(evt.Peer)
	}
}

