//go:build !android && !ios && cgo
// +build !android,!ios,cgo

// Package onnx provides metrics collection functionality for ONNX inference engine.
package onnx

import (
	"sync"
	"sync/atomic"
	"time"
)

// InferenceMetrics Êé®ÁêÜÁõëÊéßÊåáÊ†á
//
// üéØ **Ê†∏ÂøÉËÅåË¥£**Ôºö
// - ÁªüËÆ°Êé®ÁêÜÊ¨°Êï∞„ÄÅÂª∂Ëøü„ÄÅÈîôËØØÁéá
// - ÁõëÊéßÁºìÂ≠òÂëΩ‰∏≠Áéá
// - Êèê‰æõÊÄßËÉΩÂàÜÊûêÊï∞ÊçÆ
type InferenceMetrics struct {
	// ÂéüÂ≠êÊìç‰ΩúÁªüËÆ°
	totalInferences  atomic.Int64   // ÊÄªÊé®ÁêÜÊ¨°Êï∞
	totalLatencyMs   atomic.Int64   // ÊÄªÂª∂ËøüÔºàÊØ´ÁßíÔºâ
	errorCount       atomic.Int64   // ÈîôËØØÊ¨°Êï∞
	cacheHits        atomic.Int64   // ÁºìÂ≠òÂëΩ‰∏≠Ê¨°Êï∞
	cacheMisses      atomic.Int64   // ÁºìÂ≠òÊú™ÂëΩ‰∏≠Ê¨°Êï∞

	// ÂÆûÊó∂ÁªüËÆ°ÔºàÈúÄË¶ÅÈîÅ‰øùÊä§Ôºâ
	lastInferenceTime time.Time
	mu                sync.RWMutex
}

// NewInferenceMetrics ÂàõÂª∫Êé®ÁêÜÁõëÊéß
func NewInferenceMetrics() *InferenceMetrics {
	return &InferenceMetrics{}
}

// RecordInference ËÆ∞ÂΩïÊé®ÁêÜ
//
// ÂèÇÊï∞Ôºö
//   - duration: Êé®ÁêÜËÄóÊó∂
//   - err: Êé®ÁêÜÈîôËØØÔºànilË°®Á§∫ÊàêÂäüÔºâ
func (im *InferenceMetrics) RecordInference(duration time.Duration, err error) {
	im.totalInferences.Add(1)
	im.totalLatencyMs.Add(duration.Milliseconds())

	if err != nil {
		im.errorCount.Add(1)
	}

	im.mu.Lock()
	im.lastInferenceTime = time.Now()
	im.mu.Unlock()
}

// RecordCacheHit ËÆ∞ÂΩïÁºìÂ≠òÂëΩ‰∏≠/Êú™ÂëΩ‰∏≠
func (im *InferenceMetrics) RecordCacheHit(hit bool) {
	if hit {
		im.cacheHits.Add(1)
	} else {
		im.cacheMisses.Add(1)
	}
}

// Stats Ëé∑ÂèñÁªüËÆ°‰ø°ÊÅØ
//
// ËøîÂõûÔºö
//   - map[string]interface{}: ÁªüËÆ°Êï∞ÊçÆ
func (im *InferenceMetrics) Stats() map[string]interface{} {
	total := im.totalInferences.Load()
	avgLatency := int64(0)
	if total > 0 {
		avgLatency = im.totalLatencyMs.Load() / total
	}

	cacheTotal := im.cacheHits.Load() + im.cacheMisses.Load()
	cacheHitRate := 0.0
	if cacheTotal > 0 {
		cacheHitRate = float64(im.cacheHits.Load()) / float64(cacheTotal)
	}

	errorRate := 0.0
	if total > 0 {
		errorRate = float64(im.errorCount.Load()) / float64(total)
	}

	im.mu.RLock()
	lastInferenceTime := im.lastInferenceTime
	im.mu.RUnlock()

	return map[string]interface{}{
		"total_inferences":   total,
		"average_latency_ms": avgLatency,
		"error_count":        im.errorCount.Load(),
		"error_rate":         errorRate,
		"cache_hits":         im.cacheHits.Load(),
		"cache_misses":       im.cacheMisses.Load(),
		"cache_hit_rate":     cacheHitRate,
		"last_inference_time": lastInferenceTime,
	}
}

// Reset ÈáçÁΩÆÁªüËÆ°‰ø°ÊÅØ
func (im *InferenceMetrics) Reset() {
	im.totalInferences.Store(0)
	im.totalLatencyMs.Store(0)
	im.errorCount.Store(0)
	im.cacheHits.Store(0)
	im.cacheMisses.Store(0)

	im.mu.Lock()
	im.lastInferenceTime = time.Time{}
	im.mu.Unlock()
}

