#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# This script creates example_multitype.onnx to use in testing.
# The "network" doesn't actually do much other than cast around some types and
# perform basic arithmetic.  It takes two inputs:
#  - "InputA": A 1x1 8-bit unsigned int tensor
#  - "InputB": A 2x2 64-bit float tensor
#
# It produces 2 outputs:
#  - "OutputA": A 2x2 16-bit signed int tensor, equal to (InputB * InputA) - 512
#  - "OutputB": A 1x1 64-bit int tensor, equal to InputA multiplied by 1234

"""
å¤šæ•°æ®ç±»å‹ ONNX æ¨¡å‹ç”Ÿæˆè„šæœ¬ï¼ˆä¸­æ–‡æ³¨é‡Šè¡¥å……ï¼‰

æœ¬è„šæœ¬ç”¨äºç”Ÿæˆ example_multitype.onnx æ¨¡å‹æ–‡ä»¶ï¼Œç”¨äºæµ‹è¯• WES å¹³å°å¯¹ä¸åŒæ•°æ®ç±»å‹çš„æ”¯æŒèƒ½åŠ›ã€‚

æ¨¡å‹è®¾è®¡ç›®çš„ï¼š
- æµ‹è¯• WES å¹³å°å¯¹å¤šç§æ•°æ®ç±»å‹çš„æ”¯æŒï¼ˆuint8, float64, int16, int64ï¼‰
- éªŒè¯ç±»å‹è½¬æ¢å’Œå…¼å®¹æ€§å¤„ç†
- æµ‹è¯•æ··åˆç±»å‹è¾“å…¥è¾“å‡ºçš„å¤„ç†èƒ½åŠ›

æ¨¡å‹è¾“å…¥ï¼š
- "InputA": [1, 1, 1] å½¢çŠ¶çš„ uint8 å¼ é‡ï¼ˆ8ä½æ— ç¬¦å·æ•´æ•°ï¼ŒèŒƒå›´ 0-255ï¼‰
- "InputB": [1, 2, 2] å½¢çŠ¶çš„ float64 å¼ é‡ï¼ˆ64ä½åŒç²¾åº¦æµ®ç‚¹æ•°ï¼‰

æ¨¡å‹è¾“å‡ºï¼š
- "OutputA": [1, 2, 2] å½¢çŠ¶çš„ int16 å¼ é‡ï¼ˆ16ä½æœ‰ç¬¦å·æ•´æ•°ï¼‰
  è®¡ç®—å…¬å¼ï¼šOutputA = (InputB * InputA[0][0][0]) - 512
- "OutputB": [1, 1, 1] å½¢çŠ¶çš„ int64 å¼ é‡ï¼ˆ64ä½æœ‰ç¬¦å·æ•´æ•°ï¼‰
  è®¡ç®—å…¬å¼ï¼šOutputB = InputA * 1234

WES å¹³å°æµ‹è¯•åœºæ™¯ï¼š
- âœ… uint8 æ•°æ®ç±»å‹æ”¯æŒ
- âœ… float64 æ•°æ®ç±»å‹æ”¯æŒ
- âœ… int16 æ•°æ®ç±»å‹æ”¯æŒ
- âœ… int64 æ•°æ®ç±»å‹æ”¯æŒ
- âœ… ç±»å‹è½¬æ¢å¤„ç†ï¼ˆuint8 â†’ int64, float64 â†’ int16ï¼‰
- âœ… æ··åˆç±»å‹è¾“å…¥è¾“å‡º

ä½¿ç”¨æ–¹æ³•ï¼š
    python generate_network_different_types.py

ä¾èµ–è¦æ±‚ï¼š
    pip install torch onnx
"""
import torch


class DifferentTypesModel(torch.nn.Module):
    """å¤šæ•°æ®ç±»å‹æµ‹è¯•æ¨¡å‹ã€‚
    
    è¯¥æ¨¡å‹è®¾è®¡ç”¨äºæµ‹è¯• ONNX Runtime å’Œ WES å¹³å°å¯¹ä¸åŒæ•°æ®ç±»å‹çš„æ”¯æŒã€‚
    æ¨¡å‹æœ¬èº«ä¸æ‰§è¡Œå¤æ‚çš„è®¡ç®—ï¼Œä¸»è¦ç›®çš„æ˜¯éªŒè¯ç±»å‹è½¬æ¢å’Œå…¼å®¹æ€§ã€‚
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æ¨¡å‹ã€‚
        
        æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªç®€å•çš„æµ‹è¯•æ¨¡å‹ï¼Œä¸éœ€è¦ä»»ä½•å¯è®­ç»ƒå‚æ•°ã€‚
        æ‰€æœ‰è®¡ç®—éƒ½åœ¨ forward æ–¹æ³•ä¸­å®Œæˆã€‚
        """
        super().__init__()

    def forward(self, input_a, input_b):
        """å‰å‘ä¼ æ’­ï¼šæ‰§è¡Œç±»å‹è½¬æ¢å’ŒåŸºæœ¬ç®—æœ¯è¿ç®—ã€‚
        
        Args:
            input_a: uint8 ç±»å‹çš„è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ [1, 1, 1]
            input_b: float64 ç±»å‹çš„è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ [1, 2, 2]
        
        Returns:
            output_a: int16 ç±»å‹çš„è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ [1, 2, 2]
            output_b: int64 ç±»å‹çš„è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ [1, 1, 1]
        
        è®¡ç®—æµç¨‹ï¼š
        1. OutputA è®¡ç®—ï¼š
           - å°† InputA çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼ˆæ ‡é‡ï¼‰ä¸ InputB çš„æ¯ä¸ªå…ƒç´ ç›¸ä¹˜
           - å‡å»å¸¸æ•° 512ï¼ˆç”¨äºæµ‹è¯•è´Ÿæ•°å¤„ç†ï¼‰
           - è½¬æ¢ä¸º int16 ç±»å‹ï¼ˆæµ‹è¯• float64 â†’ int16 çš„ç±»å‹è½¬æ¢ï¼‰
        
        2. OutputB è®¡ç®—ï¼š
           - å°† InputA è½¬æ¢ä¸º int64 ç±»å‹ï¼ˆæµ‹è¯• uint8 â†’ int64 çš„ç±»å‹è½¬æ¢ï¼‰
           - ä¹˜ä»¥å¸¸æ•° 1234ï¼ˆç”¨äºæµ‹è¯•æ•´æ•°è¿ç®—ï¼‰
        """
        # OutputA è®¡ç®—ï¼šæµ‹è¯• float64 åˆ° int16 çš„ç±»å‹è½¬æ¢
        # OutputA calculation: test float64 to int16 type conversion
        # å°† InputA çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼ˆæ ‡é‡ï¼‰å¹¿æ’­åˆ° InputB çš„æ¯ä¸ªå…ƒç´ 
        # Broadcast InputA[0][0][0] (scalar) to each element of InputB
        output_a = input_b * input_a[0][0][0]
        # å‡å»å¸¸æ•° 512ï¼Œç”¨äºæµ‹è¯•è´Ÿæ•°å¤„ç†èƒ½åŠ›
        # Subtract 512 to test negative number handling
        output_a -= 512
        # è½¬æ¢ä¸º int16 ç±»å‹ï¼šè¿™æ˜¯å…³é”®çš„ç±»å‹è½¬æ¢æµ‹è¯•ç‚¹
        # WES å¹³å°éœ€è¦æ­£ç¡®å¤„ç† float64 â†’ int16 çš„è½¬æ¢
        # Convert to int16: key type conversion test point
        # WES platform needs to correctly handle float64 â†’ int16 conversion
        output_a = output_a.type(torch.int16)
        
        # OutputB è®¡ç®—ï¼šæµ‹è¯• uint8 åˆ° int64 çš„ç±»å‹è½¬æ¢
        # OutputB calculation: test uint8 to int64 type conversion
        # å°† InputA è½¬æ¢ä¸º int64 ç±»å‹ï¼šæµ‹è¯• uint8 â†’ int64 çš„ç±»å‹è½¬æ¢
        # Convert InputA to int64: test uint8 â†’ int64 type conversion
        output_b = input_a.type(torch.int64)
        # ä¹˜ä»¥å¸¸æ•° 1234ï¼Œç”¨äºæµ‹è¯•æ•´æ•°è¿ç®—å’Œæº¢å‡ºå¤„ç†
        # Multiply by 1234 to test integer operations and overflow handling
        output_b *= 1234
        
        return output_a, output_b


def fake_inputs():
    """ç”Ÿæˆæµ‹è¯•ç”¨çš„å‡è¾“å…¥æ•°æ®ã€‚
    
    Returns:
        input_a: uint8 ç±»å‹çš„éšæœºå¼ é‡ï¼Œå½¢çŠ¶ [1, 1, 1]ï¼Œå€¼èŒƒå›´ [0, 255]
        input_b: float64 ç±»å‹çš„éšæœºå¼ é‡ï¼Œå½¢çŠ¶ [1, 2, 2]ï¼Œå€¼èŒƒå›´ [0.0, 1.0]
    
    æ³¨æ„ï¼š
        - InputA ä½¿ç”¨ uint8 ç±»å‹ï¼Œæ¨¡æ‹Ÿå›¾åƒåƒç´ å€¼ç­‰åœºæ™¯
        - InputB ä½¿ç”¨ float64 ç±»å‹ï¼Œæ¨¡æ‹Ÿé«˜ç²¾åº¦è®¡ç®—åœºæ™¯
        - è¿™äº›æ•°æ®ç±»å‹çš„é€‰æ‹©æ˜¯ä¸ºäº†æµ‹è¯• WES å¹³å°çš„ç±»å‹å…¼å®¹æ€§
    """
    # ç”Ÿæˆ uint8 ç±»å‹çš„è¾“å…¥ï¼šæ¨¡æ‹Ÿå›¾åƒåƒç´ å€¼ï¼ˆ0-255ï¼‰
    # Generate uint8 input: simulate image pixel values (0-255)
    input_a = torch.rand((1, 1, 1)) * 255.0
    input_a = input_a.type(torch.uint8)
    
    # ç”Ÿæˆ float64 ç±»å‹çš„è¾“å…¥ï¼šæ¨¡æ‹Ÿé«˜ç²¾åº¦è®¡ç®—åœºæ™¯
    # Generate float64 input: simulate high-precision computation scenarios
    input_b = torch.rand((1, 2, 2), dtype=torch.float64)
    
    return input_a, input_b


def main():
    """ä¸»å‡½æ•°ï¼šåˆ›å»ºæ¨¡å‹ã€ç”Ÿæˆæµ‹è¯•æ•°æ®ã€å¯¼å‡º ONNX æ¨¡å‹ã€‚
    
    æµç¨‹ï¼š
    1. åˆ›å»ºæ¨¡å‹å®ä¾‹å¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    2. ç”Ÿæˆæµ‹è¯•è¾“å…¥æ•°æ®
    3. è¿è¡Œæ¨¡å‹æ¨ç†ï¼ˆç”¨äºéªŒè¯ï¼‰
    4. å¯¼å‡ºä¸º ONNX æ ¼å¼
    """
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    # Create model instance
    model = DifferentTypesModel()
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼šç¦ç”¨ dropoutã€batch normalization ç­‰è®­ç»ƒæ—¶çš„è¡Œä¸º
    # Set to evaluation mode: disable dropout, batch normalization, etc.
    model.eval()
    
    # ç”Ÿæˆæµ‹è¯•è¾“å…¥æ•°æ®
    # Generate test input data
    input_a, input_b = fake_inputs()
    
    # è¿è¡Œæ¨¡å‹æ¨ç†ï¼ŒéªŒè¯æ¨¡å‹å·¥ä½œæ­£å¸¸
    # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ torch.no_grad() å¯ä»¥èŠ‚çœå†…å­˜ï¼Œä½†ä¸ºäº†æ¼”ç¤ºæ¸…æ™°ï¼Œæˆ‘ä»¬ç›´æ¥è¿è¡Œ
    # Run model inference to verify the model works correctly
    # Note: Using torch.no_grad() can save memory, but we run directly for clarity
    output_a, output_b = model(input_a, input_b)
    print(f"Example inputs: A = {input_a!s}, B = {input_b!s}")
    print(f"Produced outputs: A = {output_a!s}, B = {output_b!s}")

    # å¯¼å‡º ONNX æ¨¡å‹
    # Export ONNX model
    out_name = "example_multitype.onnx"
    print(f"Saving model as {out_name}")
    
    # å®šä¹‰è¾“å…¥è¾“å‡ºåç§°ï¼šè¿™äº›åç§°å°†åœ¨ WES å¹³å°ä¸­ç”¨äºè¯†åˆ«è¾“å…¥è¾“å‡º
    # Define input/output names: these names will be used in WES platform to identify inputs/outputs
    input_names = ["InputA", "InputB"]
    output_names = ["OutputA", "OutputB"]
    
    # å¯¼å‡ºä¸º ONNX æ ¼å¼
    # torch.onnx.export å‚æ•°è¯´æ˜ï¼š
    # - model: è¦å¯¼å‡ºçš„ PyTorch æ¨¡å‹
    # - (input_a, input_b): ç¤ºä¾‹è¾“å…¥ï¼ˆç”¨äºç¡®å®šæ¨¡å‹è¾“å…¥å½¢çŠ¶å’Œç±»å‹ï¼‰
    # - out_name: è¾“å‡ºæ–‡ä»¶å
    # - input_names: è¾“å…¥å¼ é‡çš„åç§°ï¼ˆåœ¨ ONNX æ¨¡å‹ä¸­ï¼‰
    # - output_names: è¾“å‡ºå¼ é‡çš„åç§°ï¼ˆåœ¨ ONNX æ¨¡å‹ä¸­ï¼‰
    # Export to ONNX format
    # torch.onnx.export parameters:
    # - model: PyTorch model to export
    # - (input_a, input_b): Example inputs (used to determine model input shapes and types)
    # - out_name: Output filename
    # - input_names: Input tensor names (in ONNX model)
    # - output_names: Output tensor names (in ONNX model)
    torch.onnx.export(
        model, 
        (input_a, input_b), 
        out_name,
        input_names=input_names, 
        output_names=output_names
    )
    print(f"{out_name} saved OK.")
    
    print("\nâœ… æ¨¡å‹ç”Ÿæˆå®Œæˆï¼")
    print("ğŸ“ è¯¥æ¨¡å‹å¯ç”¨äºæµ‹è¯• WES å¹³å°å¯¹ä¸åŒæ•°æ®ç±»å‹çš„æ”¯æŒèƒ½åŠ›ã€‚")
    print("ğŸ“– è¯¦ç»†ä½¿ç”¨è¯´æ˜è¯·å‚è€ƒ README.md")

if __name__ == "__main__":
    main()

